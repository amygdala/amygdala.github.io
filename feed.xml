<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Amy on GCP</title>
    <description></description>
    <link>http://amygdala.github.io/</link>
    <atom:link href="http://amygdala.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 28 Oct 2017 18:31:54 -0700</pubDate>
    <lastBuildDate>Sat, 28 Oct 2017 18:31:54 -0700</lastBuildDate>
    <generator>Jekyll v3.3.1</generator>
    
      <item>
        <title>Using Cloud Dataflow pipeline templates from App Engine</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This post describes how to use &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt;
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/overview&quot;&gt;job templates&lt;/a&gt;
to easily launch &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Dataflow&lt;/a&gt; pipelines from a &lt;a href=&quot;https://cloud.google.com/appengine/&quot;&gt;Google App Engine (GAE)&lt;/a&gt; app,
in order to support &lt;a href=&quot;https://en.wikipedia.org/wiki/MapReduce&quot;&gt;MapReduce&lt;/a&gt;
jobs and many other data processing and analysis tasks.&lt;/p&gt;

&lt;p&gt;This post builds on a &lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;previous post&lt;/a&gt;, which
used a &lt;a href=&quot;https://cloud.google.com/appengine/docs/flexible/&quot;&gt;GAE Flexible&lt;/a&gt;
&lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/python/an-overview-of-app-engine#services_the_building_blocks_of_app_engine&quot;&gt;service&lt;/a&gt; 
to periodically launch a Python Dataflow pipeline.  The use of GAE Flex was necessary at the time, because we needed
to install the &lt;a href=&quot;https://cloud.google.com/sdk/&quot;&gt;&lt;code&gt;gcloud&lt;/code&gt; sdk&lt;/a&gt; in the instance container(s) in order to launch the pipelines.&lt;/p&gt;

&lt;p&gt;Since then, Cloud Dataflow &lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/overview&quot;&gt;templates&lt;/a&gt; have come into the
picture for the Python SDK. Dataflow templates allow you to stage your pipelines on
&lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; and execute them from a variety of environments.
This has a number of benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;With templates, you don’t have to recompile your code every time you execute a pipeline.&lt;/li&gt;
  &lt;li&gt;This means that you don’t need to launch your pipeline from a development environment or worry about dependencies.&lt;/li&gt;
  &lt;li&gt;It’s much easier for non-technical users to launch pipelines using templates.  You can launch via 
the &lt;a href=&quot;https://console.cloud.google.com&quot;&gt;Google Cloud Platform Console&lt;/a&gt;, the &lt;code&gt;gcloud&lt;/code&gt; command-line interface, or the REST API.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we’ll show how to use the Dataflow job template
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/reference/rest/#collection-v1b3projectslocationstemplates&quot;&gt;REST API&lt;/a&gt;
to periodically launch a Dataflow templated job from GAE.  Because we’re now simply calling an
API, and no longer relying on the &lt;code&gt;gcloud&lt;/code&gt; sdk to launch from App Engine, we can build a simpler &lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/&quot;&gt;App Engine
Standard&lt;/a&gt; app.&lt;/p&gt;

&lt;p&gt;With templates, you can use
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#modifying-your-code-to-use-runtime-parameters&quot;&gt;runtime parameters&lt;/a&gt;
to customize the execution.  We’ll use that feature in this example too.&lt;/p&gt;

&lt;p&gt;The pipeline used in this example is nearly the same as that described in the
&lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;earlier post&lt;/a&gt;; it analyzes data
stored in &lt;a href=&quot;https://cloud.google.com/datastore/&quot;&gt;Cloud Datastore&lt;/a&gt; — in
this case, stored tweets fetched periodically from Twitter.
The pipeline does several sorts of analysis on the tweet data; for example, it identifies important word co-occurrences in the tweets, based on a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;tf*idf&lt;/a&gt; metric.&lt;/p&gt;

&lt;figure&gt;
&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq2.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq2.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;i&gt;Detecting important word co-occurrences in tweets&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;defining-a-parameterized-dataflow-pipeline-and-creating-a-template&quot;&gt;Defining a parameterized Dataflow pipeline and creating a template&lt;/h2&gt;

&lt;p&gt;The first step in building our app is creating a Dataflow template. We do this by building a pipeline and then
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#creating-and-staging-templates&quot;&gt;deploying&lt;/a&gt;
it with the &lt;code&gt;--template_location&lt;/code&gt; flag, which causes the template to be compiled and stored at the given
&lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Google Cloud Storage (GCS)&lt;/a&gt; location.&lt;/p&gt;

&lt;p&gt;You can see the pipeline definition &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/job_template_launch/dfpipe/pipe.py&quot;&gt;here&lt;/a&gt;.
It reads recent tweets from the past N days from Cloud Datastore, then splits into three processing branches.
It finds the most popular words in terms of the percentage of tweets they were found in, calculates the most
popular URLs in terms of their count, and then derives relevant word co-occurrences using an approximation to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt; &lt;em&gt;tf*idf&lt;/em&gt;&lt;/a&gt;
ranking metric.  It writes the results to three BigQuery tables. (It would be equally straightforward to write results
to Datastore instead/as well).&lt;/p&gt;

&lt;figure&gt;
&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/df_template_pipe.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/df_template_pipe.png&quot; width=&quot;600&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;i&gt;The dataflow pipeline graph.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The &lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;previous post&lt;/a&gt; in this series
goes into a bit more detail about what some of the pipeline steps do, and how the pipeline accesses the Datastore.&lt;/p&gt;

&lt;p&gt;As part of our new template-ready pipeline definition, we’ll specify that the pipeline takes a
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#modifying-your-code-to-use-runtime-parameters&quot;&gt;runtime argument&lt;/a&gt;, 
named &lt;code&gt;timestamp&lt;/code&gt;. This value is used to filter out tweets N days older than the timestamp, so that the pipeline analyzes
only recent activity.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class UserOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
      parser.add_value_provider_argument('--timestamp', type=str)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, that argument can be accessed at runtime from a template-generated pipeline, as in this snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;  user_options = pipeline_options.view_as(UserOptions)
  ...
  wc_records = top_percents | 'format' &amp;gt;&amp;gt; beam.FlatMap(
      lambda x: [{'word': xx[0], 'percent': xx[1], 
                  'ts': user_options.timestamp.get()} for xx in x])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The example includes a template creation utility script called &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/job_template_launch/create_template.py&quot;&gt;&lt;code&gt;create_template.py&lt;/code&gt;&lt;/a&gt;, which
sets some pipeline options, including the &lt;code&gt;--template_location&lt;/code&gt; flag, defines the pipeline (via &lt;code&gt;pipe.process_datastore_tweets()&lt;/code&gt;), and calls &lt;code&gt;run()&lt;/code&gt; on it. The core of this script is shown below.
Note that the &lt;code&gt;pipeline_options&lt;/code&gt; dict doesn’t include &lt;code&gt;timestamp&lt;/code&gt;; we’ll define that
at runtime, not compile time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import dfpipe.pipe as pipe
...
pipeline_options = {
    'project': PROJECT,
    'staging_location': 'gs://' + BUCKET + '/staging',
    'runner': 'DataflowRunner',
    'setup_file': './setup.py',
    'job_name': PROJECT + '-twcount',
    'temp_location': 'gs://' + BUCKET + '/temp',
    'template_location': 'gs://' + BUCKET + '/templates/' + PROJECT + '-twproc_tmpl'
}
pipeline_options = PipelineOptions.from_dictionary(pipeline_options)
pipe.process_datastore_tweets(PROJECT, DATASET, pipeline_options)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because we used the &lt;code&gt;--template_location&lt;/code&gt; flag, a template for that pipeline
is compiled and saved to the indicated GCS location (rather than triggering a run of the pipeline).&lt;/p&gt;

&lt;p&gt;Now that the template is created, we can use it to launch Dataflow pipeline jobs from our GAE app.&lt;/p&gt;

&lt;h3 id=&quot;a-note-on-input-sources-and-template-runtime-arguments&quot;&gt;A note on input sources and template runtime arguments&lt;/h3&gt;

&lt;p&gt;As you can see from &lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#pipeline-io-and-runtime-parameters&quot;&gt;this table&lt;/a&gt;
in the documentation, the Dataflow Python SDK does not yet support the use of runtime parameters with Datastore input.&lt;/p&gt;

&lt;p&gt;For pipeline analysis, we want to consider only Datastore data from the last N days.
But, because of the above constraint, we can’t access the runtime &lt;code&gt;timestamp&lt;/code&gt; parameter when we’re constructing the
Datastore reader query. (If you try, you will see a compile-time error). Similarly, if you try the approach taken by
the non-template version of the pipeline &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/dfpipe/pipe.py&quot;&gt;here&lt;/a&gt;,
which uses &lt;code&gt;datetime.datetime.now()&lt;/code&gt; to construct its Datastore query, you’ll find that you’re always using the same
compile-time static timestamp each time you run the template.&lt;/p&gt;

&lt;p&gt;To work around this for the template version of this pipeline, we will include a filter step, that &lt;em&gt;can&lt;/em&gt; access runtime parameters, and which filters out all but the last N days of tweets post-query.
You can see this step as &lt;code&gt;FilterDate&lt;/code&gt; in the Dataflow pipeline graph figure above.&lt;/p&gt;

&lt;h3 id=&quot;launching-a-dataflow-templated-job-from-the-cloud-console&quot;&gt;Launching a Dataflow templated job from the Cloud Console&lt;/h3&gt;

&lt;p&gt;Before we actually deploy the GAE app, let’s check that we can launch a properly running Dataflow templated job
from our newly generated template. We can do that by launching a job based on that template from &lt;a href=&quot;https://console.cloud.google.com&quot;&gt;Cloud
Console&lt;/a&gt;.  (You could also do this via the &lt;code&gt;gcloud&lt;/code&gt; command-line tool). Note that the
pipeline won’t do anything interesting unless you already have Tweet data in the Datastore— which would be the case if
you tried the &lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;earlier example&lt;/a&gt; in this
series— but you can still confirm that it launches and runs successfully.&lt;/p&gt;

&lt;p&gt;Go to the &lt;a href=&quot;https://console.cloud.google.com/dataflow&quot;&gt;Dataflow pane&lt;/a&gt; of Cloud Console, and click “Create Job From
Template”.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/job_templates1.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/job_templates1.png&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;&lt;i&gt;Creating a Dataflow job from a template.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Select “Custom Template”, then browse to your new template’s location in GCS. This info was output when you ran
&lt;code&gt;create_template.py&lt;/code&gt;. (The pulldown menu also includes some predefined templates that you may want to explore later).&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/job_templates2.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/job_templates2.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;&lt;i&gt;Select &quot;Custom Template&quot;, and specify the path to the template file.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Finally, set your pipeline’s defined runtime parameter(s). In this case, we have one: &lt;code&gt;timestamp&lt;/code&gt;. The pipeline is
expecting a value in a format like this:&lt;br /&gt;
&lt;code&gt;2017-10-22 10:18:13.491543&lt;/code&gt; (you can generate such a string in the Python interpreter via &lt;code&gt;str(datetime.datetime.now())&lt;/code&gt;).&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/job_templates3.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/job_templates3.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;&lt;i&gt;Set your pipeline's runtime parameter(s) before running the job.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;While we don’t show it here, &lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#metadata&quot;&gt;you can extend your templates with additional
metadata&lt;/a&gt; so that custom parameters may be
validated when the template is executed.&lt;/p&gt;

&lt;p&gt;Once you click ‘Run Job’, you should be able to see your job running in Cloud Console.&lt;/p&gt;

&lt;h2 id=&quot;using-an-app-engine-app-to-periodically-launch-dataflow-jobs-and-fetch-tweets&quot;&gt;Using an App Engine app to periodically launch Dataflow jobs (and fetch Tweets)&lt;/h2&gt;

&lt;p&gt;Now that we’ve checked that we can successfully launch a Dataflow job using our template, we’ll define an App
Engine app handler to launch such jobs via the Dataflow job template
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/reference/rest/#collection-v1b3projectslocationstemplates&quot;&gt;REST API&lt;/a&gt;, and
run that handler periodically via a GAE cron.
We’ll use another handler of the same app to periodically fetch tweets and store them in the Datastore.&lt;/p&gt;

&lt;p&gt;You can see the GAE app script &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/job_template_launch/main.py&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
The &lt;code&gt;FetchTweets&lt;/code&gt; handler fetches tweets and stores them in the Datastore.
See the &lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;previous post&lt;/a&gt; in this series for a bit more info on that.  However, this part of the app is just
for example purposes; in your own apps, you probably already have some other means of collecting and storing data in Datastore.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;LaunchJob&lt;/code&gt; handler is the new piece of the puzzle: using the Dataflow REST API, it sets the &lt;code&gt;timestamp&lt;/code&gt; runtime parameter, and launches a Dataflow job using the template.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from googleapiclient.discovery import build
from oauth2client.client import GoogleCredentials
...
    credentials = GoogleCredentials.get_application_default()
    service = build('dataflow', 'v1b3', credentials=credentials)

    BODY = {
            &quot;jobName&quot;: &quot;{jobname}&quot;.format(jobname=JOBNAME),
            &quot;gcsPath&quot;: &quot;gs://{bucket}/templates/{template}&quot;.format(
                bucket=BUCKET, template=TEMPLATE),
            &quot;parameters&quot;: {&quot;timestamp&quot;: str(datetime.datetime.utcnow())},
             &quot;environment&quot;: {
                &quot;tempLocation&quot;: &quot;gs://{bucket}/temp&quot;.format(bucket=BUCKET),
                &quot;zone&quot;: &quot;us-central1-f&quot;
             }
        }

    dfrequest = service.projects().templates().create(
        projectId=PROJECT, body=BODY)
    dfresponse = dfrequest.execute()
    logging.info(dfresponse)
    self.response.write('Done')
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;launching-the-dataflow-pipeline-periodically-using-a-gae-cron&quot;&gt;Launching the Dataflow pipeline periodically using a GAE cron&lt;/h2&gt;

&lt;p&gt;For our GAE app, we want to launch a Dataflow templated job every few hours, where each job analyzes the tweets from the past few days, providing a ‘moving window’ of analysis. 
So, it makes sense to set things using a &lt;a href=&quot;https://cloud.google.com/appengine/docs/flexible/python/scheduling-jobs-with-cron-yaml&quot;&gt;cron.yaml&lt;/a&gt; file like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;cron:
- description: fetch tweets
  url: /timeline
  schedule: every 17 minutes
  target: default
- description: launch dataflow pipeline
  url: /launchtemplatejob
  schedule: every 5 hours
  target: default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A GAE app makes it easy to run such a cron, but note that now that we’re using templates, it becomes easier to to support this functionality in other ways too.  E.g., it would also be straightforward to use the &lt;code&gt;gcloud&lt;/code&gt; CLI to launch the template job, and set up a local cron job.&lt;/p&gt;

&lt;h2 id=&quot;a-look-at-the-example-results-in-bigquery&quot;&gt;A look at the example results in BigQuery&lt;/h2&gt;

&lt;p&gt;Once our example app is up and running, it periodically runs a Dataflow job that writes the results of its analysis to
BigQuery.  (It would also be straightforward to write results to the Datastore if that makes more sense for your
workflow – or to write to multiple sources).&lt;/p&gt;

&lt;p&gt;With BigQuery, it is easy to run some fun queries on the data. 
For example, we can find recent word co-occurrences that are ‘interesting’ by our metric:&lt;/p&gt;

&lt;figure&gt;
  &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/xScreenshot_2017-10-28_14_28_35%20copy%202.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/xScreenshot_2017-10-28_14_28_35%20copy%202.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;&lt;i&gt;&quot;Interesting&quot; word co-occurrences&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Or we can look for &lt;em&gt;emergent&lt;/em&gt; word pairs, that have become ‘interesting’ in the last day or so (compare April and Oct
2017 results):&lt;/p&gt;

&lt;figure&gt;
  &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/temp_queries.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/temp_queries.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;&lt;i&gt;Emergent (new) interesting word co-occurrences can reflect current news&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We can contrast the ‘interesting’ word pairs with the words that are simply the most popular within a given period (you
can see that most of these words are common, but not particularly newsworthy):&lt;/p&gt;

&lt;figure&gt;
  &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/pScreenshot_2017-10-28_14_27_05%202.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/pScreenshot_2017-10-28_14_27_05%202.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;&lt;i&gt;Popular, but not necessarily interesting words&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Or, find the most frequently tweeted URLs from the past few weeks (some URLs are truncated in the output):&lt;/p&gt;

&lt;figure&gt;
  &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/bq_popurls2b.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/bq_popurls2b.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;&lt;i&gt;The most frequently tweeted URLs from the past few weeks (filtering out some of the shortlinks)&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;summary-and-whats-next&quot;&gt;Summary… and what’s next?&lt;/h2&gt;

&lt;p&gt;In this post, we’ve looked at how you can programmatically launch Dataflow pipelines — that read from Datastore — using Cloud Dataflow &lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/overview&quot;&gt;job templates&lt;/a&gt;, and call the Dataflow
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/reference/rest/#collection-v1b3projectslocationstemplates&quot;&gt;REST API&lt;/a&gt; from an App Engine app.
See the example app’s &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/job_template_launch/README.md&quot;&gt;README&lt;/a&gt; for more detail on how to configure and run the app yourself.&lt;/p&gt;

&lt;p&gt;Dataflow’s expressive programming model makes it easy to build and support a wide range of scalable processing and
analytics tasks. With templates, it becomes much easier to launch pipeline jobs — you don’t have to recompile every time
you execute, or worry about your environment and dependencies. And it’s more straightforward for less technical users to
launch template-based pipelines.&lt;/p&gt;

&lt;p&gt;We hope you find the example app useful as a starting point towards defining new pipeline templates and running
your own analytics — via App Engine apps or otherwise. We look forward to hearing more about what you build!&lt;/p&gt;
</description>
        <pubDate>Tue, 24 Oct 2017 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/dataflow/app_engine/2017/10/24/gae_dataflow.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/dataflow/app_engine/2017/10/24/gae_dataflow.html</guid>
        
        <category>dataflow</category>
        
        <category>gae</category>
        
        
        <category>Dataflow</category>
        
        <category>App_Engine</category>
        
      </item>
    
      <item>
        <title>Running Cloud Dataflow jobs from an App Engine app</title>
        <description>&lt;p&gt;This post looks at how you can launch &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt; pipelines from your &lt;a href=&quot;https://cloud.google.com/appengine/&quot;&gt;App Engine&lt;/a&gt; app, in order to support &lt;a href=&quot;https://en.wikipedia.org/wiki/MapReduce&quot;&gt;MapReduce&lt;/a&gt; jobs and other data processing and analysis tasks.&lt;/p&gt;

&lt;p&gt;Until recently, if you wanted to run MapReduce jobs from a Python App Engine app, you would use &lt;a href=&quot;https://github.com/GoogleCloudPlatform/appengine-mapreduce&quot;&gt;this MR library&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, &lt;a href=&quot;https://beam.apache.org/&quot;&gt;Apache Beam&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt; have entered the picture.  Apache Beam is a unified model for building data processing pipelines that handle bounded and unbounded data, as well as a collection of SDKs for building these pipelines. Google Cloud Dataflow is a managed service for executing parallelized data processing pipelines written using Apache Beam.&lt;/p&gt;

&lt;p&gt;Dataflow allows a wide range of data processing patterns, including ETL, batch computation, and continuous computation.
The Beam model &lt;a href=&quot;https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount.py&quot;&gt;supports&lt;/a&gt; and subsumes MapReduce.  So, you can map your MR jobs to equivalent Beam pipelines, and Beam’s programming model makes it straightforward to extend and modify your existing MR logic.&lt;/p&gt;

&lt;p&gt;The Beam Python SDK makes it easy to launch Dataflow pipeline jobs from a Python App Engine app. The SDK 
includes a &lt;a href=&quot;https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py#L868&quot;&gt;Cloud Datastore &lt;em&gt;source&lt;/em&gt; and &lt;em&gt;sink&lt;/em&gt;&lt;/a&gt;.  This makes it easy to write Dataflow pipelines that support the functionality of any existing MR jobs, as well as support additional analytics.&lt;/p&gt;

&lt;p&gt;In this blog post, we’ll look at an &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/tree/master/sdk_launch&quot;&gt;example app&lt;/a&gt; that shows how to periodically launch a Python Dataflow pipeline from GAE, to analyze data stored in &lt;a href=&quot;https://cloud.google.com/datastore/&quot;&gt;Cloud Datastore&lt;/a&gt;; in this case, stored tweets from Twitter.  The pipeline does several sorts of analysis on the data; for example, it identifies ‘interesting’ word co-occurrences (bigrams) in the tweets, as in this snippet below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq2.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq2.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The example is a GAE app with two &lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/python/an-overview-of-app-engine#services_the_building_blocks_of_app_engine&quot;&gt;services (previously, ‘modules’)&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a &lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/&quot;&gt;GAE Standard&lt;/a&gt; service that periodically pulls in timeline tweets from Twitter and stores them in Datastore; and&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a &lt;a href=&quot;https://cloud.google.com/appengine/docs/flexible/&quot;&gt;GAE Flexible&lt;/a&gt; service that periodically launches the Python Dataflow pipeline to analyze the tweet data in the Datastore.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Standard service– the one that gathers the tweets– is just for example purposes; in your own apps, you probably already have other means of collecting and storing data in Datastore.&lt;/p&gt;

&lt;h2 id=&quot;building-a-service-to-define-and-launch-a-dataflow-pipeline-from-app-engine&quot;&gt;Building a service to define and launch a Dataflow pipeline from App Engine&lt;/h2&gt;

&lt;p&gt;We’ll use a Flex custom runtime based on the &lt;code&gt;gcr.io/google_appengine/python&lt;/code&gt; image for the service that launches the dataflow pipeline, as we’ll install the &lt;code&gt;gcloud&lt;/code&gt; sdk in the instance container(s).  So, the example includes a &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/Dockerfile&quot;&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&lt;/a&gt; used to deploy the service.  As the last command in the &lt;code&gt;Dockerfile&lt;/code&gt;, we’ll start up a Gunicorn server to serve a Flask app script (&lt;code&gt;main_df.py&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;The Python code for this service consists of the small Flask app script (&lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/main_df.py&quot;&gt;&lt;code&gt;main_df.py&lt;/code&gt;&lt;/a&gt;), which accesses a module (&lt;code&gt;dfpipe&lt;/code&gt;) that does most of the heavy lifting in terms of defining and launching the example pipeline (in &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/dfpipe/pipe.py&quot;&gt;&lt;code&gt;dfpipe/pipe.py&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;setting-the-pipeline-options&quot;&gt;Setting the pipeline options&lt;/h3&gt;

&lt;p&gt;As part of the process of launching a Dataflow pipeline, various options may be set.
In order to make the &lt;code&gt;dfpipe&lt;/code&gt; module available to the Dataflow &lt;em&gt;workers&lt;/em&gt;, the pipeline options include a &lt;code&gt;setup_file&lt;/code&gt; flag.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;  pipeline_options = {
      'project': PROJECT,
      'staging_location': 'gs://' + BUCKET + '/staging',
      'runner': 'DataflowRunner',
      'setup_file': './setup.py',
      'job_name': PROJECT + '-twcount',
      'max_num_workers': 10,
      'temp_location': 'gs://' + BUCKET + '/temp'
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This points to a &lt;code&gt;setup.py&lt;/code&gt; file which specifies to package the &lt;code&gt;dfpipe&lt;/code&gt; module using &lt;code&gt;setuptools&lt;/code&gt;. If our pipeline also had dependencies on third-party libs, we could include those in setup.py as well. 
The indicated code is  gathered in a package that is built as a source distribution, staged in the staging area for the workflow being run, and then installed in the workers when they start running.&lt;/p&gt;

&lt;h2 id=&quot;a-look-at-the-dataflow-pipeline&quot;&gt;A look at the Dataflow pipeline&lt;/h2&gt;

&lt;p&gt;Now let’s take a quick look at &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/dfpipe/pipe.py&quot;&gt;&lt;code&gt;dfpipe/pipe.py&lt;/code&gt;&lt;/a&gt;, to see what the Python Dataflow pipeline does.&lt;/p&gt;

&lt;p&gt;It reads recent tweets from the past N days from Cloud Datastore, then
essentially splits into three processing branches. It finds the top N most popular words in terms of
the percentage of tweets they were found in, calculates the top N most popular URLs in terms of
their count, and then derives relevant word co-occurrences (bigrams) using an approximation to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt; &lt;em&gt;tf*idf&lt;/em&gt;&lt;/a&gt;
ranking metric.  It writes the results to three BigQuery tables. (It would be equally straightforward to write results to Datastore instead/as well).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_df_graph.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_df_graph.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;using-datastore-as-a-pipeline-source&quot;&gt;Using Datastore as a pipeline &lt;em&gt;source&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;This pipeline reads from Datastore, grabbing the tweets that the other GAE Standard service is periodically grabbing and writing to the Datastore.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/main.py&quot;&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt;, the app script for the GAE standard service, you can see the Tweet entity schema:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from google.appengine.ext import ndb
class Tweet(ndb.Model):
  user = ndb.StringProperty()
  text = ndb.StringProperty()
  created_at = ndb.DateTimeProperty()
  tid = ndb.IntegerProperty()
  urls = ndb.StringProperty(repeated=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/dfpipe/pipe.py&quot;&gt;&lt;code&gt;dfpipe/pipe.py&lt;/code&gt;&lt;/a&gt;, we can use the &lt;a href=&quot;https://cloud.google.com/datastore/docs/reference/rpc/google.datastore.v1&quot;&gt;&lt;code&gt;google.cloud.proto.datastore&lt;/code&gt;&lt;/a&gt; API to define a query for Tweet entities more recent than a given date— in this case, four days ago— by creating a property filter on the &lt;code&gt;created_at&lt;/code&gt; field.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from google.cloud.proto.datastore.v1 import query_pb2
def make_query(kind):
  &quot;&quot;&quot;Creates a Cloud Datastore query to retrieve all Tweet entities with a
  'created_at' date &amp;gt; N days ago.
  &quot;&quot;&quot;
  days = 4
  now = datetime.datetime.now()
  earlier = now - datetime.timedelta(days=days)

  query = query_pb2.Query()
  query.kind.add().name = kind
  datastore_helper.set_property_filter(query.filter, 'created_at',
                                       PropertyFilter.GREATER_THAN,
                                       earlier)
  return query
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we use that query to define an input source for the pipeline:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; p = beam.Pipeline(options=pipeline_options)
  # Create a query to read entities from datastore.
  query = make_query('Tweet')

  # Read entities from Cloud Datastore into a PCollection.
  lines = (p
      | 'read from datastore' &amp;gt;&amp;gt; ReadFromDatastore(project, query, None))
  ... 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use &lt;code&gt;properties.get()&lt;/code&gt; on an element of the resulting collection to extract the value of a given field of the entity, in this case the ‘text’ field:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class WordExtractingDoFn(beam.DoFn):
  &quot;&quot;&quot;Parse each tweet text into words, removing some 'stopwords'.&quot;&quot;&quot;

  def process(self, element):
    content_value = element.properties.get('text', None)
    text_line = ''
    if content_value:
      text_line = content_value.string_value
    words = set([x.lower() for x in re.findall(r'[A-Za-z\']+', text_line)])
    stopwords = [...]
    return list(words - set(stopwords))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, this snippet from the pipeline shows how  &lt;code&gt;WordExtractingDoFn&lt;/code&gt; can be used as part of the Datastore input processing:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;  # Count the occurrences of each word.
  percents = (lines
      | 'split' &amp;gt;&amp;gt; (beam.ParDo(WordExtractingDoFn())
                    .with_output_types(unicode))
      | 'pair_with_one' &amp;gt;&amp;gt; beam.Map(lambda x: (x, 1))
      | 'group' &amp;gt;&amp;gt; beam.GroupByKey()
      | 'count' &amp;gt;&amp;gt; beam.Map(lambda (word, ones): (word, sum(ones)))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;launching-the-dataflow-pipeline-periodically-using-a-cron-job&quot;&gt;Launching the Dataflow pipeline periodically using a cron job&lt;/h2&gt;

&lt;p&gt;In the example app, we want to launch a pipeline job every few hours, where each job analyzes the tweets from the past few days, providing a ‘moving window’ of analysis.
So, it makes sense to just set things up as an app &lt;a href=&quot;https://cloud.google.com/appengine/docs/flexible/python/scheduling-jobs-with-cron-yaml&quot;&gt;cron&lt;/a&gt; job, which looks like this (&lt;code&gt;backend&lt;/code&gt; is the name of the app service that handles this request, and the &lt;code&gt;url&lt;/code&gt; is the handler that launches the job):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;cron:
- description: launch dataflow pipeline
  url: /launchpipeline
  schedule: every 5 hours
  target: backend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A pipeline job could of course be triggered by other means as well– e.g. as part of handling a client request to the app, or perhaps via a &lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/python/taskqueue/push/&quot;&gt;Task Queue task&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-look-at-the-example-results-in-bigquery&quot;&gt;A look at the example results in BigQuery&lt;/h2&gt;

&lt;p&gt;Once our example app is up and running, it periodically runs a Dataflow job that writes the results of its analysis to BigQuery.  (It would be just as easy to write results to the Datastore if that makes more sense for your workflow – or to write to multiple sources).&lt;/p&gt;

&lt;p&gt;With BigQuery, it is easy to run some fun queries on the data. 
For example, we can find recent word co-occurrences that are ‘interesting’ by our metric:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq3.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq3.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Or look for emerging word pairs, that have become ‘interesting’ in the last day or so (as of early April 2017):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq4.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq4.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can contrast the ‘interesting’ word pairs with the words that are simply the most popular within a given period (you can see that most of these words are common, but not particularly newsworthy):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_wc1.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_wc1.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Or, find the most often-tweeted URLs from the past few days (some URLs are truncated in the output):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_urls1.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_urls1.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-and-whats-next&quot;&gt;Summary… and what’s next?&lt;/h2&gt;

&lt;p&gt;In this post, we’ve looked at how you can programmatically launch Dataflow pipelines — that read from Datastore — directly from your App Engine app.
See the example app’s &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/README.md&quot;&gt;README&lt;/a&gt; for more detail on how to configure and run the app yourself.&lt;/p&gt;

&lt;p&gt;Dataflow’s expressive programming model makes it easy to build and support a wide range of scalable processing and analytics tasks.
We hope you find the example app useful as a starting point towards defining new pipelines and running your own analytics from your App Engine apps.
We look forward to hearing more about what you build!&lt;/p&gt;

</description>
        <pubDate>Fri, 14 Apr 2017 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html</guid>
        
        <category>dataflow</category>
        
        <category>gae</category>
        
        
        <category>Dataflow</category>
        
        <category>App_Engine</category>
        
      </item>
    
      <item>
        <title>Learning and using your own image classifications</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transfer-learning-building-your-own-image-classifier&quot;&gt;Transfer learning: building your own image classifier&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#an-easy-way-to-use-your-trained-image-classifier&quot;&gt;An easy way to use your trained image classifier&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#an-example-app-would-you-hug-that&quot;&gt;An example app: “would you hug that?”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#exporting-your-trained-model-to-cloud-ml&quot;&gt;Exporting your trained model to Cloud ML&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#using-the-hugs-classifier-for-prediction-with-the-cloud-ml-api&quot;&gt;Using the “hugs classifier” for prediction with the Cloud ML API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bonus-hedgehogs-vs-dandelions&quot;&gt;Bonus: Hedgehogs vs Dandelions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-next&quot;&gt;What Next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/vision/&quot;&gt;Google Vision API&lt;/a&gt; is great for identifying labels, or categories, for a given
image. However, sometimes you want to further classify your own images, for more specialized categories that the Google
Vision API hasn’t been trained on.  E.g., maybe you’re a birdwatcher and want to recognize rare species of birds that the Vision API doesn’t do well at discriminating.  Maybe you’re a cell biologist, and want to try to automatically classify your slides.&lt;/p&gt;

&lt;p&gt;It turns out that it can be pretty straightforward to build your own neural net model to do this, via &lt;em&gt;transfer
learning&lt;/em&gt; –  bootstrapping an existing image classification model to reduce the effort needed to learn something new.&lt;/p&gt;

&lt;p&gt;In this post, we’ll take a look at an example that does that.&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning-building-your-own-image-classifier&quot;&gt;Transfer learning: building your own image classifier&lt;/h2&gt;

&lt;p&gt;One such deep neural net model is the &lt;a href=&quot;http://arxiv.org/abs/1512.00567&quot;&gt;Inception&lt;/a&gt; architecture, built using &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;, a machine learning framework open-sourced by Google.
Google has also open-sourced the &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/inception&quot;&gt;Inception v3&lt;/a&gt; model, trained to classify images against 1000 different &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt; categories.  We can use its penultimate “bottleneck” layer to train a new top layer that can recognize other classes of images: your own classes.
We’ll see that our new top layer does not need to be very complex, and that we typically don’t need much data or much training of this new model, to get good results for our new image classifications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image-classification-3-1.png&quot; alt=&quot;Transfer learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There have been some great posts showing how you can &lt;a href=&quot;https://petewarden.com/2016/02/28/tensorflow-for-poets/&quot;&gt;train this new ‘top layer’ model with TensorFlow&lt;/a&gt;, and how to do this training on &lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/12/how-to-train-and-classify-images-using-google-cloud-machine-learning-and-cloud-dataflow&quot;&gt;Google Cloud Machine Learning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;an-easy-way-to-use-your-trained-image-classifier&quot;&gt;An easy way to use your trained image classifier&lt;/h2&gt;

&lt;p&gt;In this post, we’ll focus more on the next step: After you’ve trained your model, and can classify your own images, you’ll want to be able to use the model for inference (prediction). That is, given a new image, you’d like to ask: which of your categories does it fall into?&lt;/p&gt;

&lt;p&gt;You’ll often need to have these prediction capabilities scale up. Perhaps you’ve built a mobile app to (say) recognize bird species, by using your trained ML model for online species prediction. If the app becomes wildly popular, you don’t want it to stop working because it’s getting too much traffic.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/ml/&quot;&gt;Google Cloud Machine Learning&lt;/a&gt; (Cloud ML) is an easy way to do this: in addition to supporting distributed model training, Cloud ML lets you scalably serve your online predictions after the model is trained.&lt;/p&gt;

&lt;p&gt;One way to access the Cloud ML online prediction service is to use the &lt;a href=&quot;https://developers.google.com/discovery/libraries&quot;&gt;Google API Client Libraries&lt;/a&gt; to access the &lt;a href=&quot;https://cloud.google.com/ml/reference/rest/&quot;&gt;Cloud ML API&lt;/a&gt;.
That means that it is quite straightforward to build an app that classifies images according to your categories, and scales up without your needing to do anything.&lt;/p&gt;

&lt;h2 id=&quot;an-example-app-would-you-hug-that&quot;&gt;An example app: “would you hug that?”&lt;/h2&gt;

&lt;p&gt;Let’s look at an example with a … kind of goofy data set.  The code used to train this model and make this app is
&lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/tree/master/workshop_sections/transfer_learning/cloudml&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(Note: This
post is not really a tutorial – the
&lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/README.md&quot;&gt;README&lt;/a&gt; in the repo walks you through the process of building the example in more detail.  If you want to give it a try, make sure you’ve done all the
&lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/INSTALL.md&quot;&gt;necessary setup&lt;/a&gt; first).&lt;/p&gt;

&lt;p&gt;Suppose we have a set of training images that have been labeled according to two categories: “things you might want to hug”, and “things you would not want to hug”. The ‘huggable’ dataset includes images of things like puppies and kittens.  The non-huggable dataset includes images of things with sharp edges, etc.
Now, we want to build a web app that we can upload images to, and have the app tell us whether or not the object is something “huggable”.&lt;/p&gt;

&lt;p&gt;We want our web app to work like this: If you upload an image to the app:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/yarn_octopus.png&quot; width=&quot;500&quot; alt=&quot;A yarn octopus&quot; /&gt;&lt;/p&gt;

&lt;p&gt;…the app uses the trained model to get the predicted category of the image (“hugs”? “no hugs”?).  Then the app will use the response to display the results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/yarn_octopus_score.png&quot; width=&quot;600&quot; alt=&quot;The yarn octopus is scored as huggable&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thanks to &lt;a href=&quot;https://twitter.com/juliaferraioli&quot;&gt;Julia Ferraioli&lt;/a&gt; for the idea, the “hugs/no-hugs” dataset and an earlier version of the prediction web app.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We’ll train our “hugs/no-hugs” model on Cloud ML, and use the Cloud ML API to make it easy to build the prediction web app.&lt;/p&gt;

&lt;p&gt;To do this, we first need to do some &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/hugs_preproc.sh&quot;&gt;image preprocessing&lt;/a&gt;, to extract the ‘bottleneck layer’ information (the &lt;em&gt;embeds&lt;/em&gt;) from the Inception v3 model, for each image in our dataset. These embeds will form the input data for the new ‘top layer’ model that we will train.
We will use &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt; (&lt;a href=&quot;https://beam.apache.org/&quot;&gt;Apache Beam&lt;/a&gt;)
to do this preprocessing – that is, the Beam pipeline uses the Inception v3 model to generate the inputs. We’ll save those preprocessing results in the cloud, in &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; (GCS), as &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/python_io/&quot;&gt;TFRecords&lt;/a&gt;, for easy consumption by the training process.&lt;/p&gt;

&lt;p&gt;Then, we’re ready to &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/hugs_train.sh&quot;&gt;train the model&lt;/a&gt;. Follow the code link to see the specifics.
(If you follow the blog post links above, you’ll also find other examples that train using a different “flowers” dataset.)&lt;/p&gt;

&lt;p&gt;Once the model is trained, we can use it to classify new images.  If we deploy the trained model to Cloud ML, we can make prediction requests using the Cloud ML API scalably, without any other setup.  This is great, since it makes our web app very easy to write.&lt;/p&gt;

&lt;h2 id=&quot;exporting-your-trained-model-to-cloud-ml&quot;&gt;Exporting your trained model to Cloud ML&lt;/h2&gt;

&lt;p&gt;To be able to &lt;a href=&quot;https://cloud.google.com/ml/docs/how-tos/getting-predictions&quot;&gt;use a trained model for prediction&lt;/a&gt;, you will need to &lt;a href=&quot;https://cloud.google.com/ml/docs/how-tos/preparing-models#adding_input_and_output_collections_to_the_graph&quot;&gt;add input and output collections&lt;/a&gt; to your model graph.  This gives Cloud ML the necessary ‘hooks’ into your deployed model graph for running predictions and returning the results.  See &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/trainer/model.py#L343&quot;&gt;this model prediction graph definition&lt;/a&gt; for an example. You’ll also need to make sure you export (save) your model as described &lt;a href=&quot;https://cloud.google.com/ml/docs/how-tos/preparing-models#exporting_saving_the_final_model&quot;&gt;here&lt;/a&gt;, including exporting the &lt;a href=&quot;https://www.tensorflow.org/versions/r0.11/how_tos/meta_graph/&quot;&gt;MetaGraph&lt;/a&gt;.  See &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/trainer/model.py#L365&quot;&gt;this method&lt;/a&gt; for example export code.&lt;/p&gt;

&lt;p&gt;Once you’ve done that, you can deploy a trained model to Cloud ML like this.  First, “create the model” in Cloud ML – this does not define the actual model, but creates a name to associate with the model once you upload it. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;gcloud beta ml models create hugs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/model.sh&quot;&gt;deploy a &lt;em&gt;version&lt;/em&gt; of that model&lt;/a&gt; by pointing Cloud ML to the checkpointed model info that was saved as the final result of your training session.
The command to do that will look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;gcloud beta ml versions create v1 \
  --model hugs \
  --origin gs://your-gcs-bucket/path/to/model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It will take a few minutes to create the model version. Once that’s done, you can start to access it for prediction in your apps.  Each time you deploy a new version of your model, you will use a new version name.  If you have more than one version of a model, you can set one as the default.&lt;/p&gt;

&lt;h2 id=&quot;using-the-hugs-classifier-for-prediction-with-the-cloud-ml-api&quot;&gt;Using the “hugs classifier” for prediction with the Cloud ML API&lt;/h2&gt;

&lt;p&gt;Once your model is deployed, there are various ways to access it.  One easy way for initial testing is just to &lt;a href=&quot;https://cloud.google.com/ml/docs/quickstarts/prediction#use_the_online_prediction_service&quot;&gt;use the gcloud sdk from the command line&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But, for the web app, we’ll use the &lt;a href=&quot;https://developers.google.com/api-client-library/python/&quot;&gt;Google API client libs&lt;/a&gt;
instead, to call the &lt;a href=&quot;https://cloud.google.com/ml/reference/rest/v1beta1/projects/predict&quot;&gt;Cloud ML API web service&lt;/a&gt;.
So we just need to upload an image to the app; format the image data for input to our model; then just make the API
call&lt;sup&gt;&lt;a href=&quot;#footnote1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and display the response.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/motherboard.jpeg&quot; alt=&quot;a computer motherboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, if we upload this photo of a motherboard…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/motherboard_score.png&quot; width=&quot;600&quot; alt=&quot;a motherboard is scored as not huggable&quot; /&gt;&lt;/p&gt;

&lt;p&gt;… it’s judged as “not huggable”.&lt;/p&gt;

&lt;p&gt;Here is a Python code snippet showing a prediction API call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from googleapiclient import discovery
from oauth2client.client import GoogleCredentials

def create_client():

  credentials = GoogleCredentials.get_application_default()
  ml_service = discovery.build(
      'ml', 'v1beta1', credentials=credentials)
  return ml_service


def get_prediction(ml_service, project, model_name, input_image):
  request_dict = make_request_json(input_image)
  body = {'instances': [request_dict]}

  # This request will use the default model version.
  parent = 'projects/{}/models/{}'.format(project, model_name)
  request = ml_service.projects().predict(name=parent, body=body)
  result = request.execute()
  return result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see more detail on how the uploaded image was formatted for the API request, and how the response was 
parsed, &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/web_server/predict_server.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A quokka, in contrast, is (very rightly) judged as “huggable”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/quokka_score.png&quot; width=&quot;600&quot; alt=&quot;a quokka is scored as huggable&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bonus-hedgehogs-vs-dandelions&quot;&gt;Bonus: Hedgehogs vs Dandelions&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/hedgehog.jpg&quot; width=&quot;400&quot; alt=&quot;A hedgehog&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When I was experimenting with learning both the “hugs/no-hugs” and &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/flowers_preproc.sh&quot;&gt;“flowers”&lt;/a&gt; classification models, I learned something funny&lt;sup&gt;&lt;a href=&quot;#footnote2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.  I accidentally fed the “flowers” model an image of a hedgehog.  The hedgehog was meant for the “hugs/no-hugs” model, which will reasonably classify it as “don’t hug”.&lt;/p&gt;

&lt;p&gt;It turns out that if you ask the “flowers” model what kind of flower a hedgehog is, it will classify it pretty confidently as looking most like a dandelion. This seems pretty astute of the flowers model!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;$ python images_to_json.py hedgehog.jpg
$ gcloud beta ml predict --model flowers --json-instances request.json
KEY                             PREDICTION  SCORES
prediction_images/hedgehog.jpg  1           [0.006916556041687727, 0.9633635878562927, 0.0015918412245810032, 0.005548111163079739, 0.022190041840076447, 0.0003897929273080081]
$ gsutil cat gs://cloud-ml-data/img/flower_photos/dict.txt
daisy
dandelion
roses
sunflowers
tulips
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(We can see this is so because &lt;code&gt;dandelion&lt;/code&gt; is in the ‘1’ index position in the 
flower dataset’s &lt;code&gt;dict.txt&lt;/code&gt;, corresponding to the prediction of ‘1’.)&lt;/p&gt;

&lt;h2 id=&quot;what-next&quot;&gt;What Next?&lt;/h2&gt;

&lt;p&gt;If you’re interested to learn more about TensorFlow and CloudML, there are many examples and tutorials on the &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow site&lt;/a&gt;.  The &lt;a href=&quot;https://cloud.google.com/ml/docs/&quot;&gt;Cloud ML docs&lt;/a&gt; go into much more detail on how to train and serve a model, including Cloud ML’s support for distributed training and hyperparamter tuning.&lt;/p&gt;

&lt;p&gt;(You might also be interested in the upcoming &lt;a href=&quot;https://cloudnext.withgoogle.com/&quot;&gt;Google Cloud Next&lt;/a&gt; event, where you can hear much more about what Google is doing in the Big Data &amp;amp; Machine Learning area.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small&gt;
&lt;span id=&quot;footnote1&quot;&gt;1&lt;/span&gt;: While the prediction service is in alpha, you may sometimes see a &lt;code&gt;502&lt;/code&gt; error response if you make a request after not having used the service for a while.  If you see this, just resubmit. This will not be an issue once the service moves out of alpha.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;
&lt;span id=&quot;footnote2&quot;&gt;2&lt;/span&gt;: It’s possible that this amuses only me.&lt;/small&gt;&lt;/p&gt;

&lt;h4 id=&quot;image-credits&quot;&gt;Image credits:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://goo.gl/images/zdGnN9&quot;&gt;https://goo.gl/images/zdGnN9&lt;/a&gt; - yarn octopus&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://goo.gl/images/LmaVMu&quot;&gt;https://goo.gl/images/LmaVMu&lt;/a&gt; - motherboard&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://goo.gl/images/XTF5h0&quot;&gt;https://goo.gl/images/XTF5h0&lt;/a&gt; - quokka&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://goo.gl/images/XTF5h0&quot;&gt;https://goo.gl/images/yuFM1C&lt;/a&gt; - hedgehog&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 03 Feb 2017 00:00:00 -0800</pubDate>
        <link>http://amygdala.github.io/ml/2017/02/03/transfer_learning.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/ml/2017/02/03/transfer_learning.html</guid>
        
        <category>machine_learning</category>
        
        <category>Cloud_ML</category>
        
        
        <category>ML</category>
        
      </item>
    
      <item>
        <title>Building a Slackbot that uses the Google Cloud ML Natural Language API (and runs on Kubernetes)</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Many of us belong to some &lt;a href=&quot;https://slack.com/&quot;&gt;Slack&lt;/a&gt; communities.
Slack has an API that makes it easy to add ‘&lt;a href=&quot;https://slackhq.com/a-beginner-s-guide-to-your-first-bot-97e5b0b7843d#.iyzgmbaf0&quot;&gt;bots&lt;/a&gt;’ to a channel, that do interesting things with the posted content, and that can interact with the people on the channel.
Google’s &lt;a href=&quot;cloud.google.com/products/machine-learning/&quot;&gt;Cloud Machine Learning APIs&lt;/a&gt; make it easy to build bots with interesting and fun capabilities.&lt;/p&gt;

&lt;p&gt;Here, we’ll describe how to build such a bot, one that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;uses the &lt;a href=&quot;https://cloud.google.com/natural-language/&quot;&gt;Google Cloud ML Natural Language API&lt;/a&gt; to analyze channel content,&lt;/li&gt;
  &lt;li&gt;runs on &lt;a href=&quot;http://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; to make it easy to deploy, and&lt;/li&gt;
  &lt;li&gt;uses the &lt;a href=&quot;https://github.com/howdyai/botkit&quot;&gt;Botkit&lt;/a&gt; library to make it easy to interact with Slack.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code for this slackbot is &lt;a href=&quot;https://github.com/GoogleCloudPlatform/nodejs-docs-samples/tree/master/language/slackbot&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;using-the-cloud-ml-natural-language-api-in-a-bot&quot;&gt;Using the Cloud ML Natural Language API in a bot&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/natural-language/&quot;&gt;Google Cloud Natural Language API&lt;/a&gt;  helps reveal the structure and meaning of text by offering powerful machine learning models for multiple languages– currently, English, Spanish, and Japanese.&lt;/p&gt;

&lt;p&gt;You can use the NL API to do &lt;strong&gt;entity recognition&lt;/strong&gt; (identifying entities and label by types such as person, organization, location, events, products and media), &lt;strong&gt;sentiment analysis&lt;/strong&gt; (understanding the overall sentiment expressed in a block of text), and &lt;strong&gt;syntax analysis&lt;/strong&gt; (sentence extraction and tokenization, identifying parts of speech, creating parse trees for each sentence, and more).&lt;/p&gt;

&lt;p&gt;Our Natural Language (NL) slackbot uses the Google Cloud NL API in two different ways.&lt;/p&gt;

&lt;h3 id=&quot;entity-detection&quot;&gt;Entity detection&lt;/h3&gt;

&lt;p&gt;First, it uses the NL API’s  &lt;a href=&quot;https://cloud.google.com/natural-language/docs/basics&quot;&gt;&lt;strong&gt;entity detection&lt;/strong&gt;&lt;/a&gt; to track the most common topics that are being discussed over time in a channel.
It does this by detecting entities in each posted message, and recording them in a database.
Then, at any time the participants in the channel can query the NL slackbot to ask it for the top N entities/topics discussed in the channel (by default, over the past week).&lt;/p&gt;

&lt;h3 id=&quot;sentiment-analysis&quot;&gt;Sentiment analysis&lt;/h3&gt;

&lt;p&gt;Additionally, the NL slackbot uses the NL API to assess
the &lt;a href=&quot;https://cloud.google.com/natural-language/docs/basics&quot;&gt;&lt;strong&gt;sentiment&lt;/strong&gt;&lt;/a&gt; of any message posted to
the channel, and if the positive or negative magnitude of the statement is
sufficiently large, it sends a ‘thumbs up’ or ‘thumbs down’ to the channel in reaction.&lt;/p&gt;

&lt;h2 id=&quot;running-the-slackbot-as-a-kubernetes-app&quot;&gt;Running the Slackbot as a Kubernetes App&lt;/h2&gt;

&lt;p&gt;Our slackbot uses &lt;a href=&quot;https://cloud.google.com/container-engine/&quot;&gt;Google Container
Engine&lt;/a&gt;, a hosted version of
&lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;, to run the bot.  This is a convenient way to launch the bot in the cloud, so that there is no need to manage it locally, and to ensure that it stays running.
It also uses &lt;a href=&quot;https://cloud.google.com/container-registry/&quot;&gt;Google Container Registry&lt;/a&gt; to store a &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; image
for the bot.&lt;/p&gt;

&lt;p&gt;It’s useful to have the bot running in the cloud, and on a Kubernetes cluster.  While you could alternately just set it up on a VM somewhere, Kubernetes will ensure that it stays running.
If the &lt;a href=&quot;http://kubernetes.io/docs/user-guide/pods/&quot;&gt;&lt;em&gt;pod&lt;/em&gt;&lt;/a&gt; in the slackbot’s &lt;a href=&quot;http://kubernetes.io/docs/user-guide/deployments/&quot;&gt;&lt;em&gt;Deployment&lt;/em&gt;&lt;/a&gt; goes down for some reason, Kubernetes will restart it.&lt;/p&gt;

&lt;h2 id=&quot;starting-up-the-nl-slackbot&quot;&gt;Starting up the NL Slackbot&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/master/language/slackbot/README.md&quot;&gt;README&lt;/a&gt;  in the GitHub repo walks you through the process of starting up and running the slackbot.  As part of the process you’ll also
create a &lt;a href=&quot;https://api.slack.com/bot-users&quot;&gt;Slack bot user&lt;/a&gt; and get an authentication token.&lt;/p&gt;

&lt;p&gt;If you think you want to run your slackbot for a while, follow the instructions in &lt;a href=&quot;https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/master/language/slackbot/README.md#optional-create-a-slackbot-app-that-uses-persistent-storage&quot;&gt;the README section on setting up a Persistent Disk&lt;/a&gt; for the bot’s database.  That will allow the bot to be restarted without losing data.
The README also walks you through how you can test your bot locally before deploying to Kubernetes if you want.&lt;/p&gt;

&lt;p&gt;Once it’s running, invite the bot to a Slack channel.&lt;/p&gt;

&lt;h2 id=&quot;the-nl-slackbot-in-action&quot;&gt;The NL Slackbot in Action&lt;/h2&gt;

&lt;p&gt;Once your NL slackbot is running, and you’ve invited it to a channel, everyone in the channel can start to interact with it.
For the most part, the NL slackbot  will keep pretty quiet. Each time there is a post, the NL slackbot will analyze the &lt;strong&gt;entities&lt;/strong&gt; in that text. It will store those entities in a database.&lt;/p&gt;

&lt;p&gt;It will also analyze the &lt;strong&gt;sentiment&lt;/strong&gt; of the post (understanding the overall sentiment expressed in a block of text).  If the magnitude of the sentiment is greater than a certain threshold, either positive or negative, the bot will respond with a ‘thumbs up’ or ‘thumbs down’.  It doesn’t respond to all posts, only those for which the sentiment magnitude is above the threshold.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nl_coffee_bananas_sh.png&quot; alt=&quot;Analyzing sentiment&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(If you think that this aspect of the bot is a bit annoying, it is easy to disable, or change the threshold, by looking for where it is set &lt;a href=&quot;https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/master/language/slackbot/demo_bot.js&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;At any time, you can directly ask the bot for the top entities that it has detected in channel conversation (by default, the top 20 entities over the past week).  You do this by addressing the bot with the words &lt;code&gt;top entities&lt;/code&gt;.
For example, if your bot is called &lt;code&gt;@nlpbot&lt;/code&gt;, you would ask it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@nlpbot top entities
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, the result might look something like the following. (At least, if you have seeded your test channel with a combination of political news and Kubernetes posts :).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nl_slackbot_ents2_sh.png&quot; alt=&quot;Asking the bot for top entities&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-next&quot;&gt;What Next?&lt;/h2&gt;

&lt;p&gt;There are many ways that this bot could be developed further and be made more sophisticated.&lt;/p&gt;

&lt;p&gt;As just one example, you could integrate the other Cloud ML APIs as well.  You might add a capability that leverages the &lt;a href=&quot;https://cloud.google.com/vision/&quot;&gt;Cloud Vision API&lt;/a&gt; to analyze the images that people post to the channel.  Then, each time someone posted a meme image to the channel, you could use the Vision API to do OCR on the image, then pass that info to the NL API.&lt;/p&gt;

&lt;p&gt;You could also extend the NL slackbot to support more sophisticated queries on the entity database – e.g., “show me the top N PERSONS” or “top N LOCATIONS today”.
Or, you could include the wiki URLs in the results for any entities that have them. This information is currently being collected, but not displayed. That might look something like the following.  Note that “Trump” and “Donald Trump” are detected as referring to the same PERSON.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nl_slackbot_wiki_sh.png&quot; alt=&quot;Including wiki urls in entity information&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 14 Jan 2017 00:00:00 -0800</pubDate>
        <link>http://amygdala.github.io/ml/2017/01/14/nlapi.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/ml/2017/01/14/nlapi.html</guid>
        
        <category>machine_learning</category>
        
        <category>natural_language_processing</category>
        
        <category>natural_langage_API</category>
        
        <category>kubernetes</category>
        
        
        <category>ML</category>
        
      </item>
    
      <item>
        <title>Using the Cloud Vision API with Twilio Messaging on Kubernetes</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/vision/&quot;&gt;Google Cloud Vision API&lt;/a&gt; has just moved to GA (General Availability) status.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/cat_and_laptop.jpg&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Vision API lets you create applications that can classify images into thousands of categories (e.g., “sailboat”, “lion”… or &lt;strong&gt;cat&lt;/strong&gt; and &lt;strong&gt;laptop&lt;/strong&gt; as above); can detect faces and other objects in images (including predicting “sentiment”); can perform OCR (detection of text in images); can detect landmarks (like the Eiffel Tower); can detect logos; and can moderate for offensive content.
You can &lt;a href=&quot;https://www.youtube.com/watch?v=ud2Ipnq0pTU&quot;&gt;see Jeff Dean demoing the Vision API in this video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-vision&quot;&gt;This github repo&lt;/a&gt; has a number of Vision API examples, written in different languages, and showing off different aspects of the Cloud Vision API. Some of the examples are simple scripts, and others are a bit more complex.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python/twilio/twilio-k8s&quot;&gt;new example called ‘twilio-k8s’&lt;/a&gt; has just been added to the repo. It shows
how to run the
&lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python/twilio/twilio-labels&quot;&gt;“What’s That?” app&lt;/a&gt; (built by &lt;a href=&quot;http://www.blog.juliaferraioli.com/2016/02/exploring-world-using-vision-twilio.html&quot;&gt;Julia
Ferraioli&lt;/a&gt;), on
&lt;a href=&quot;http://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The app uses &lt;a href=&quot;https://www.twilio.com&quot;&gt;Twilio&lt;/a&gt; to allow images to be texted to a given number,
then uses the &lt;a href=&quot;https://cloud.google.com/vision/&quot;&gt;Cloud Vision API&lt;/a&gt; to find labels in the image
(classify what’s in the image) and return the detected labels as a reply text.
Because the app is running on Kubernetes, it’s easy to &lt;strong&gt;scale up the app&lt;/strong&gt; to support a large number
of requests.&lt;/p&gt;

&lt;p&gt;Once you’ve set up the app on your Google Container Engine (or Kubernetes) cluster, and set up your Twilio number, you can text images to get content labelings:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/yard.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/yard.jpg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It’s easy to modify your own version of the app to look for additional information in the image.  I’ve in fact modifed my version of the app to look for &lt;em&gt;logos&lt;/em&gt; too. It’s fun to see how well it can do with an incomplete view of a logo:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/cl_bar.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/cl_bar.png&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After you’ve set up your app, when you’re ready to share it, you can scale up the number of servers running on your Kubernetes cluster so that the app stays responsive.  The example’s &lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-vision/blob/master/python/twilio/twilio-k8s/README.md&quot;&gt;README&lt;/a&gt; goes into more detail about how to do all of this.&lt;/p&gt;

&lt;p&gt;For a relatively short time, you can try it out here: (785) 336-5113.&lt;br /&gt;
This number won’t work indefinitely, though :).&lt;/p&gt;

</description>
        <pubDate>Fri, 22 Apr 2016 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/ml/2016/04/22/vision-api.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/ml/2016/04/22/vision-api.html</guid>
        
        <category>Vision_API</category>
        
        <category>machine_learning</category>
        
        
        <category>ML</category>
        
      </item>
    
      <item>
        <title>Cloud Dataflow News</title>
        <description>&lt;p&gt;There’s been a lot happening with &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Google Cloud Dataflow&lt;/a&gt; lately.&lt;/p&gt;

&lt;p&gt;We are pleased to announce the recent induction of the &lt;a href=&quot;https://cloud.google.com/dataflow/what-is-google-cloud-dataflow#Sdks&quot;&gt;Google Cloud Dataflow SDK&lt;/a&gt; (and corresponding runners for &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Apache Flink&lt;/a&gt; and &lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;) into the new &lt;a href=&quot;http://beam.incubator.apache.org/&quot;&gt;Apache Beam incubator project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;http://oreilly.com/ideas/the-world-beyond-batch-streaming-102&quot;&gt;‘Streaming 102’&lt;/a&gt; article was published by O’Reilly , following 
&lt;a href=&quot;http://oreilly.com/ideas/the-world-beyond-batch-streaming-101&quot;&gt;‘Streaming 101’&lt;/a&gt;.  These articles provide a great overview of design and implementation considerations in stream data analysis.&lt;/p&gt;

&lt;p&gt;We’ve also recently written an article that &lt;a href=&quot;https://cloud.google.com/dataflow/blog/dataflow-beam-and-spark-comparison&quot;&gt;compares the programming models of Dataflow and Spark as they exist today&lt;/a&gt;, based on a mobile ‘gaming’ scenario, involving the evolution of a pipeline from a simple batch use case to more sophisticated streaming use cases, with side-by-side code snippets contrasting the two.
The article uses a suite of ‘gaming’ example pipelines that can be found in the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/DataflowJavaSDK/tree/master/examples/src/main/java8/com/google/cloud/dataflow/examples/complete/game&quot;&gt;Dataflow github repo&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Mon, 15 Feb 2016 00:00:00 -0800</pubDate>
        <link>http://amygdala.github.io/dataflow/2016/02/15/dataflow.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/dataflow/2016/02/15/dataflow.html</guid>
        
        <category>Apache_Beam</category>
        
        <category>big_data</category>
        
        <category>data_analysis</category>
        
        
        <category>Dataflow</category>
        
      </item>
    
      <item>
        <title>New sample repos for Symfony and Laravel with the Google App Engine PHP Runtime</title>
        <description>&lt;p&gt;After the recent &lt;a href=&quot;http://amygdala.github.io/gae/php/2015/03/09/gaephp.html&quot;&gt;updates to the Google App Engine PHP Runtime&lt;/a&gt;, we are creating GAE forks for some of the popular PHP framework starter apps.  These forks contain the modifications for running these apps on App Engine.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/symfony-standard&quot;&gt;This repository&lt;/a&gt; contains a modified Symfony Standard Edition Starter app for Google App Engine.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/laravel&quot;&gt;This repository&lt;/a&gt; contains a modified Laravel Starter app for Google App Engine.&lt;/p&gt;

&lt;p&gt;To get started with either one, see the README in the repo.&lt;/p&gt;

&lt;p&gt;You’ll want to first create a Google Cloud project if you don’t already have one. You can do that from &lt;a href=&quot;https://console.developers.google.com/start/appengine&quot;&gt;this page&lt;/a&gt; if you like.  (Note that there is a &lt;a href=&quot;https://console.developers.google.com/billing/freetrial&quot;&gt;free trial&lt;/a&gt; available).&lt;/p&gt;

&lt;p&gt;Then, if you’re not familiar with PHP on App Engine, you might want to follow the &lt;a href=&quot;https://cloud.google.com/appengine/docs/php/gettingstarted/introduction&quot;&gt;PHP Tutorial&lt;/a&gt; first.&lt;/p&gt;

&lt;p&gt;We’re looking forward to your feedback (and/or pull requests)!&lt;/p&gt;

</description>
        <pubDate>Wed, 20 May 2015 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/gae/php/2015/05/20/gaephp.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/gae/php/2015/05/20/gaephp.html</guid>
        
        <category>php</category>
        
        <category>gae</category>
        
        
        <category>gae</category>
        
        <category>php</category>
        
      </item>
    
      <item>
        <title>Real-time analysis of Twitter data using Kubernetes, PubSub and BigQuery</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/pubsub/overview&quot;&gt;Google Cloud &lt;strong&gt;PubSub&lt;/strong&gt;&lt;/a&gt; provides many-to-many, asynchronous messaging that decouples senders and receivers. It allows for secure and highly available communication between independently written applications and delivers low-latency, durable messaging.
It has just gone to Beta, and is available for anyone to try.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes-bigquery-python/tree/master/pubsub&quot;&gt;This example&lt;/a&gt; &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes&quot;&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/a&gt; app shows how to build a ‘pipeline’ to stream Twitter data into &lt;a href=&quot;https://cloud.google.com/bigquery/what-is-bigquery&quot;&gt;&lt;strong&gt;BigQuery&lt;/strong&gt;&lt;/a&gt; using &lt;a href=&quot;https://cloud.google.com/pubsub/docs&quot;&gt;PubSub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The app uses uses PubSub to buffer the data coming in from Twitter and to decouple ingestion from processing.
One of the Kubernetes app &lt;strong&gt;&lt;em&gt;pods&lt;/em&gt;&lt;/strong&gt; reads the data from Twitter and publishes it to a PubSub topic.  Other pods subscribe to the PubSub topic, grab data in small batches, and stream it into BigQuery.  The figure below suggests this flow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/k8s_pubsub_tw_bq.png&quot; width=&quot;600&quot; alt=&quot;Architecture of app&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This app can be thought of as a ‘workflow’ type of app– it doesn’t have a web front end (though Kubernetes is great for those types of apps as well).
Instead, it is designed to continously run a scalable data ingestion pipeline.&lt;/p&gt;

&lt;p&gt;Find the code and more detail &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes-bigquery-python/tree/master/pubsub&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Wed, 11 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/kubernetes/2015/03/11/pubsub.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/kubernetes/2015/03/11/pubsub.html</guid>
        
        <category>kubernetes</category>
        
        <category>pubsub</category>
        
        <category>bigquery</category>
        
        <category>twitter</category>
        
        
        <category>kubernetes</category>
        
      </item>
    
      <item>
        <title>Updates to the Google App Engine PHP Runtime</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://code.google.com/p/googleappengine/wiki/SdkReleaseNotes&quot;&gt;1.9.18 Google App Engine release&lt;/a&gt; has added new capabilities to the &lt;a href=&quot;https://cloud.google.com/appengine/docs/php&quot;&gt;App Engine PHP runtime&lt;/a&gt;. 
It’s now possible to &lt;a href=&quot;https://cloud.google.com/appengine/docs/php/#PHP_Selecting_the_PHP_runtime&quot;&gt;select PHP 5.5&lt;/a&gt; as your runtime, and a number of useful new features are supported if you do so.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;cURL&lt;/strong&gt; extension is now &lt;a href=&quot;https://cloud.google.com/appengine/docs/php/#PHP_Enabled_extensions&quot;&gt;supported&lt;/a&gt;.
We’ve also provided a &lt;a href=&quot;https://cloud.google.com/appengine/docs/php/config/php_ini#GAE_directives&quot;&gt;cURL implementation using the standard HTTP streams API&lt;/a&gt; for apps that do not need the complete cURL extension.
The &lt;strong&gt;ImageMagick&lt;/strong&gt; extension is now supported for PHP 5.5 apps as well.&lt;/p&gt;

&lt;p&gt;We’ve added an in memory virtual filesystem that makes it possible to create temporary files.
In the new PHP 5.5 runtime, you can now use the &lt;strong&gt;&lt;code&gt;sys_get_temp_dir()&lt;/code&gt;, &lt;code&gt;tmpfile()&lt;/code&gt; and &lt;code&gt;tempnam()&lt;/code&gt;&lt;/strong&gt; functions to &lt;a href=&quot;https://gae-php-tips.appspot.com/2015/03/03/file-system-changes-in-app-engine-1-9-18/&quot;&gt;create temporary files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The PHP 5.5. runtime also gives you the ability to &lt;a href=&quot;https://gae-php-tips.appspot.com/2015/03/09/direct-file-uploads-for-php-5-5/&quot;&gt;&lt;strong&gt;upload files directly to your application&lt;/strong&gt;&lt;/a&gt;, without the need to upload the files to &lt;a href=&quot;https://cloud.google.com/storage/docs&quot;&gt;Google Cloud Storage&lt;/a&gt; first. Direct uploads leverages the same in-memory virtual filesystem that is used to provide temporary filesystem support. Direct uploads are only available with the PHP 5.5 runtime, and are  limited to a maximum combined size of 32MB, which is the incoming request size limit.&lt;/p&gt;

&lt;p&gt;See the linked-to posts, from the unoffical “Tips and Tricks for PHP on Google App Engine” &lt;a href=&quot;https://gae-php-tips.appspot.com&quot;&gt;blog&lt;/a&gt;, for more detail, and the App Engine &lt;a href=&quot;https://code.google.com/p/googleappengine/wiki/SdkReleaseNotes&quot;&gt;release notes&lt;/a&gt; for more detail on what has changed.&lt;/p&gt;

</description>
        <pubDate>Mon, 09 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/gae/php/2015/03/09/gaephp.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/gae/php/2015/03/09/gaephp.html</guid>
        
        <category>php</category>
        
        <category>gae</category>
        
        
        <category>gae</category>
        
        <category>php</category>
        
      </item>
    
      <item>
        <title>Persistent Installation of MySQL and WordPress on Kubernetes</title>
        <description>&lt;p&gt;This post describes how to run a persistent installation of &lt;a href=&quot;https://wordpress.org/&quot;&gt;Wordpress&lt;/a&gt; on &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’ll use the &lt;a href=&quot;https://registry.hub.docker.com/_/mysql/&quot;&gt;mysql&lt;/a&gt; and &lt;a href=&quot;https://registry.hub.docker.com/_/wordpress/&quot;&gt;wordpress&lt;/a&gt; official &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; images for this installation. (The wordpress image includes an Apache server).&lt;/p&gt;

&lt;p&gt;We’ll create two Kubernetes &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/pods.md&quot;&gt;pods&lt;/a&gt; to run mysql and wordpress, both with associated &lt;a href=&quot;https://cloud.google.com/compute/docs/disks&quot;&gt;persistent disks&lt;/a&gt;, then set up a Kubernetes &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/services.md&quot;&gt;service&lt;/a&gt; to front each pod.&lt;/p&gt;

&lt;p&gt;This example demonstrates several useful things, including: how to set up and use persistent disks with Kubernetes pods; how to define Kubernetes services to leverage docker-links-compatible service environment variables; and use of an external load balancer to expose the wordpress service externally and make it transparent to the user if the wordpress pod moves to a different cluster node.&lt;/p&gt;

&lt;p&gt;Some of the post details, such as the Persistent Disk setup, require that Kubernetes is running on &lt;a href=&quot;https://cloud.google.com/compute/&quot;&gt;Google Compute Engine&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;install-gcloud-and-start-up-a-kubernetes-cluster&quot;&gt;Install gcloud and Start up a Kubernetes Cluster&lt;/h2&gt;

&lt;p&gt;First, if you have not already done so, &lt;a href=&quot;https://cloud.google.com/compute/docs/quickstart&quot;&gt;create&lt;/a&gt; a &lt;a href=&quot;https://cloud.google.com/&quot;&gt;Google Cloud Platform&lt;/a&gt; project, and install the &lt;a href=&quot;https://cloud.google.com/sdk/&quot;&gt;gcloud SDK&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then, set the gcloud default project name to point to the project you want to use for your Kubernetes cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gcloud config set project &amp;lt;project-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, grab the Kubernetes &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes/releases&quot;&gt;release binary&lt;/a&gt;.  (This example was tested with release 0.8.1).&lt;/p&gt;

&lt;p&gt;Then, start up a &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes&quot;&gt;Kubernetes&lt;/a&gt; &lt;a href=&quot;...&quot;&gt;cluster&lt;/a&gt; as &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/gce.md&quot;&gt;described here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kube-up.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;&amp;lt;kubernetes&amp;gt;&lt;/code&gt; is the path to your Kubernetes installation.&lt;/p&gt;

&lt;h2 id=&quot;create-and-format-two-persistent-disks&quot;&gt;Create and format two persistent disks&lt;/h2&gt;

&lt;p&gt;For this WordPress installation, we’re going to configure our Kubernetes &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/pods.md&quot;&gt;pods&lt;/a&gt; to use &lt;a href=&quot;https://cloud.google.com/compute/docs/disks&quot;&gt;persistent disks&lt;/a&gt;. This means that we can preserve installation state across pod shutdown and re-startup.&lt;/p&gt;

&lt;p&gt;Before doing anything else, we’ll create and format the persistent disks that we’ll use for the installation: one for the mysql pod, and one for the wordpress pod.
The general series of steps required is as described &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/volumes.md&quot;&gt;here&lt;/a&gt;, where $ZONE is the zone where your cluster is running, and $DISK_SIZE is specified as, e.g. ‘500GB’.  In future, this process will be more streamlined.&lt;/p&gt;

&lt;p&gt;So for the two disks used in this example, do the following.
First create and format the mysql disk, setting the disk size to meet your needs:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;gcloud compute disks create --size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DISK_SIZE&lt;/span&gt; --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ZONE&lt;/span&gt; mysql-disk
gcloud compute instances attach-disk --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ZONE&lt;/span&gt; --disk&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mysql-disk --device-name temp-data kubernetes-master
gcloud compute ssh --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ZONE&lt;/span&gt; kubernetes-master &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --command &lt;span class=&quot;s2&quot;&gt;&amp;quot;sudo mkdir /mnt/tmp &amp;amp;&amp;amp; sudo /usr/share/google/safe_format_and_mount /dev/disk/by-id/google-temp-data /mnt/tmp&amp;quot;&lt;/span&gt;
gcloud compute instances detach-disk --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ZONE&lt;/span&gt; --disk mysql-disk kubernetes-master&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then create and format the wordpress disk.  Note that you may not want as large a disk size for the wordpress code as for the mysql disk.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;gcloud compute disks create --size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DISK_SIZE&lt;/span&gt; --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ZONE&lt;/span&gt; wordpress-disk
gcloud compute instances attach-disk --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ZONE&lt;/span&gt; --disk&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$wordpress&lt;/span&gt;-disk --device-name temp-data kubernetes-master
gcloud compute ssh --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ZONE&lt;/span&gt; kubernetes-master &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --command &lt;span class=&quot;s2&quot;&gt;&amp;quot;sudo mkdir /mnt/tmp &amp;amp;&amp;amp; sudo /usr/share/google/safe_format_and_mount /dev/disk/by-id/google-temp-data /mnt/tmp&amp;quot;&lt;/span&gt;
gcloud compute instances detach-disk --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ZONE&lt;/span&gt; --disk wordpress-disk kubernetes-master&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;start-the-mysql-pod-and-service&quot;&gt;Start the Mysql Pod and Service&lt;/h2&gt;

&lt;p&gt;Now that the persistent disks are defined, the Kubernetes pods can be launched.  We’ll start with the mysql pod.&lt;/p&gt;

&lt;h3 id=&quot;start-the-mysql-pod&quot;&gt;Start the Mysql pod&lt;/h3&gt;

&lt;p&gt;Copy and then edit this &lt;a href=&quot;https://gist.github.com/amygdala/88a8740e3946ba55125b&quot;&gt;mysql.yaml&lt;/a&gt; pod definition to use the database password you specify.  &lt;code&gt;mysql.yaml&lt;/code&gt; looks like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;v1beta1&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;desiredState&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;manifest&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;v1beta1&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;
           &lt;span class=&quot;c1&quot;&gt;# change this&lt;/span&gt;
            &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;yourpassword&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;100&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;3306&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# name must match the volume name below&lt;/span&gt;
          &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql-persistent-storage&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# mount path within the container&lt;/span&gt;
            &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;/var/lib/mysql&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql-persistent-storage&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;persistentDisk&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# This GCE PD must already exist and be formatted ext4&lt;/span&gt;
            &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pdName&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql-disk&lt;/span&gt;
            &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;fsType&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ext4&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;Pod&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that we’ve defined a volume mount for &lt;code&gt;/var/lib/mysql&lt;/code&gt;, and specified a volume that uses the persistent disk (&lt;code&gt;mysql-disk&lt;/code&gt;) that you created.
Once you’ve edited the file to set your database password, create the pod as follows, where &lt;code&gt;&amp;lt;kubernetes&amp;gt;&lt;/code&gt; is the path to your Kubernetes installation:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh create -f mysql.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It may take a short period before the new pod reaches the &lt;code&gt;Running&lt;/code&gt; state.
List all pods to see the status of this new pod and the cluster node that it is running on:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh get pods&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;check-the-running-pod-on-the-compute-instance&quot;&gt;Check the running pod on the Compute instance&lt;/h4&gt;

&lt;p&gt;You can take a look at the logs for a pod by using &lt;code&gt;kubectl.sh log&lt;/code&gt;.  For example:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh log mysql&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you want to do deeper troubleshooting, e.g. if it seems a container is not staying up, you can also ssh in to the node that a pod is running on.  There, you can run &lt;code&gt;sudo -s&lt;/code&gt;, then &lt;code&gt;docker ps -a&lt;/code&gt; to see all the containers.  You can then inspect the logs of containers that have exited, via &lt;code&gt;docker logs &amp;lt;container_id&amp;gt;&lt;/code&gt;.  (You can also find some relevant logs under &lt;code&gt;/var/log&lt;/code&gt;, e.g. &lt;code&gt;docker.log&lt;/code&gt; and &lt;code&gt;kubelet.log&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&quot;start-the-myql-service&quot;&gt;Start the Myql service&lt;/h3&gt;

&lt;p&gt;We’ll define and start a &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/services.md&quot;&gt;service&lt;/a&gt; that lets other pods access the mysql database on a known port and host.
We will specifically name the service &lt;code&gt;mysql&lt;/code&gt;.  This will let us leverage the support for &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/services.md#how-do-they-work&quot;&gt;Docker-links-compatible&lt;/a&gt; serviceenvironment variables when we up the wordpress pod. The wordpress Docker image expects to be linked to a mysql container named &lt;code&gt;mysql&lt;/code&gt;, as you can see in the “How to use this image” section on the wordpress docker hub &lt;a href=&quot;https://registry.hub.docker.com/_/wordpress/&quot;&gt;page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So if we label our Kubernetes mysql service &lt;code&gt;mysql&lt;/code&gt;, the wordpress pod will be able to use the Docker-links-compatible environment variables, defined by Kubernetes, to connect to the database.&lt;/p&gt;

&lt;p&gt;Copy the &lt;a href=&quot;https://gist.github.com/amygdala/9f88e2ea9c37d26a8a68&quot;&gt;mysql-service.yaml&lt;/a&gt; file, which looks like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;v1beta1&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# the port that this service should serve on&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;3306&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# just like the selector in the replication controller,&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# but this time it identifies the set of pods to load balance&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# traffic to.&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# the container on each pod to connect to, can be a name&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# (e.g. &amp;#39;www&amp;#39;) or a number (e.g. 80)&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;3306&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mysql&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then, start the service like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh create -f mysql-service.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can see what services are running via:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh get services&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;start-wordpress-pod-and-service&quot;&gt;Start WordPress Pod and Service&lt;/h2&gt;

&lt;p&gt;Once the mysql service is up, start the wordpress pod.&lt;/p&gt;

&lt;p&gt;Copy this pod config file: &lt;a href=&quot;https://gist.github.com/amygdala/ccf107f940054ae5d740&quot;&gt;wordpress.yaml&lt;/a&gt; and edit the database password to be the same as you used in &lt;code&gt;mysql.yaml&lt;/code&gt;. Note that this config file also defines a volume, this one using the &lt;code&gt;wordpress-disk&lt;/code&gt; persistent disk that you created.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;v1beta1&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;wordpress&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;desiredState&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;manifest&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;v1beta1&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;frontendController&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;wordpress&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;wordpress&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;80&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# name must match the volume name below&lt;/span&gt;
          &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;wordpress-persistent-storage&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# mount path within the container&lt;/span&gt;
            &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;/var/www/html&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;WORDPRESS_DB_PASSWORD&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# change this&lt;/span&gt;
            &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;yourpassword&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;wordpress-persistent-storage&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;c1&quot;&gt;# emptyDir: {}&lt;/span&gt;
          &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;persistentDisk&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# This GCE PD must already exist and be formatted ext4&lt;/span&gt;
            &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pdName&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;wordpress-disk&lt;/span&gt;
            &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;fsType&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ext4&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;frontend&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;Pod&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Create the pod:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh create -f wordpress.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And list the pods to check that the status of the new pod changes to &lt;code&gt;Running&lt;/code&gt;.  As above, this might take a minute.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh get pods&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;start-the-wordpress-service&quot;&gt;Start the WordPress service&lt;/h3&gt;

&lt;p&gt;Once the wordpress pod is running, start its service.  Copy
&lt;a href=&quot;https://gist.github.com/amygdala/72128b4624a7c9317a45&quot;&gt;wordpress-service.yaml&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The service config file looks like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;v1beta1&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;frontend&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# the port that this service should serve on&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;3000&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# just like the selector in the replication controller,&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# but this time it identifies the set of pods to load balance&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# traffic to.&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;frontend&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# the container on each pod to connect to, can be a name&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# (e.g. &amp;#39;www&amp;#39;) or a number (e.g. 80)&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;80&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;frontend&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;createExternalLoadBalancer&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note the &lt;code&gt;createExternalLoadBalancer&lt;/code&gt; setting.  This will set up the wordpress service behind an external IP.
&lt;code&gt;createExternalLoadBalancer&lt;/code&gt; only works on GCE.&lt;/p&gt;

&lt;p&gt;Note also that we’ve set the service port to 3000.  We’ll return to that shortly.&lt;/p&gt;

&lt;p&gt;Start the service:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh create -f wordpress-service.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;and see it in the list of services:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh get services&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then, find the external IP for your WordPress service by listing the forwarding rules for your project:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gcloud compute forwarding-rules list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Look for the rule called &lt;code&gt;frontend&lt;/code&gt;, which is what we named the wordpress service, and note its IP address.&lt;/p&gt;

&lt;h2 id=&quot;visit-your-new-wordpress-blog&quot;&gt;Visit your new WordPress blog&lt;/h2&gt;

&lt;p&gt;To access your new installation, you’ll first need to open up port 3000 (the port specified in the wordpress service config) in the firewall. Do this via:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gcloud compute firewall-rules create wordpress --allow tcp:3000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will define a firewall rule called &lt;code&gt;wordpress&lt;/code&gt; that opens port 3000 in the default network for your project.&lt;/p&gt;

&lt;p&gt;Now, we can visit the running WordPress app.
Use the external IP that you obtained above, and visit it on port 3000:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;external_ip&amp;gt;:3000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see the familiar WordPress init page.&lt;/p&gt;

&lt;h2 id=&quot;take-down-and-restart-your-blog&quot;&gt;Take down and restart your blog&lt;/h2&gt;

&lt;p&gt;Set up your WordPress blog and play around with it a bit.  Then, take down its pods and bring them back up again. Because you used persistent disks, your blog state will be preserved.&lt;/p&gt;

&lt;p&gt;If you are just experimenting, you can take down and bring up only the pods:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span&gt;&lt;/span&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh delete -f wordpress.yaml
$ &amp;lt;kubernetes&amp;gt;/cluster/kubectl.sh delete -f mysql.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When you restart the pods again (using the &lt;code&gt;create&lt;/code&gt; operation as described above), their services will pick up the new pods based on their labels.&lt;/p&gt;

&lt;p&gt;If you want to shut down the entire app installation, you can delete the services as well.&lt;/p&gt;

&lt;p&gt;If you are ready to turn down your Kubernetes cluster altogether, run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ &amp;lt;kubernetes&amp;gt;/cluster/kube-down.sh
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Tue, 13 Jan 2015 00:00:00 -0800</pubDate>
        <link>http://amygdala.github.io/kubernetes/2015/01/13/k8s1.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/kubernetes/2015/01/13/k8s1.html</guid>
        
        
        <category>kubernetes</category>
        
      </item>
    
  </channel>
</rss>
