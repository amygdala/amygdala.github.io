<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Amy on GCP</title>
    <description></description>
    <link>http://amygdala.github.io/</link>
    <atom:link href="http://amygdala.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 02 Feb 2018 11:37:25 -0800</pubDate>
    <lastBuildDate>Fri, 02 Feb 2018 11:37:25 -0800</lastBuildDate>
    <generator>Jekyll v3.3.1</generator>
    
      <item>
        <title>Easy distributed training with TensorFlow using tf.estimator.train_and_evaluate and Cloud ML Engine</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;TensorFlow release 1.4 introduced the function &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate&quot;&gt;&lt;strong&gt;&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, which simplifies training, evaluation, and exporting of &lt;a href=&quot;https://www.tensorflow.org/get_started/estimator&quot;&gt;&lt;code&gt;Estimator&lt;/code&gt;&lt;/a&gt; models. It abstracts away the details of &lt;a href=&quot;https://www.google.com/url?q=https://www.tensorflow.org/deploy/distributed&quot;&gt;distributed execution&lt;/a&gt; for training and evaluation, while also supporting local execution, and provides consistent behavior across both local/non-distributed and distributed configurations.&lt;/p&gt;

&lt;p&gt;This means that with &lt;strong&gt;&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;&lt;/strong&gt;, you can run the same code on both locally and distributed in the cloud, on different devices and using different cluster configurations, and get consistent results, &lt;strong&gt;without making any code changes&lt;/strong&gt;. When you’re done training (or at intermediate stages), the trained model is automatically exported in a &lt;a href=&quot;https://www.tensorflow.org/programmers_guide/saved_model&quot;&gt;form suitable for serving&lt;/a&gt; (e.g. for &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/prediction-overview&quot;&gt;Cloud ML Engine online prediction&lt;/a&gt; or &lt;a href=&quot;https://www.tensorflow.org/serving/&quot;&gt;TensorFlow serving&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In this post, we’ll walk through how to use &lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt; with an &lt;code&gt;Estimator&lt;/code&gt; model, and then show how easy it is to do &lt;strong&gt;distributed training of the model on &lt;a href=&quot;https://cloud.google.com/ml-engine&quot;&gt;Cloud ML Engine&lt;/a&gt;&lt;/strong&gt;, moving between different cluster configurations with just a config tweak.
(The TensorFlow code itself supports distribution on any infrastructure (GCE, GKE, etc.) when properly configured, but we will focus on Cloud ML Engine, which makes the experience seamless).&lt;/p&gt;

&lt;p&gt;The primary steps necessary to do this are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;build your &lt;code&gt;Estimator&lt;/code&gt; model;&lt;/li&gt;
  &lt;li&gt;define how data is fed into the model for both training and test datasets (often these definitions are essentially the same); and&lt;/li&gt;
  &lt;li&gt;define training and eval specifications (&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec&quot;&gt;&lt;code&gt;TrainSpec&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/EvalSpec&quot;&gt;&lt;code&gt;EvalSpec&lt;/code&gt;&lt;/a&gt;) to be passed to &lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;.  The &lt;code&gt;EvalSpec&lt;/code&gt; can include information on how to export your trained model for prediction (serving), and we’ll look at how to do that as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we’ll look at how to &lt;strong&gt;use the trained model to make predictions&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The example also includes the use of &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/data/Dataset&quot;&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/a&gt; to manage our input data. This API is part of TensorFlow 1.4, and is an &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md&quot;&gt;easier and more performant way&lt;/a&gt; to create input pipelines to TensorFlow models; this is particularly important with large datasets and when using accelerators.
 (See &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md&quot;&gt;this article&lt;/a&gt; for more on why input pipelining is so important, particularly when using accelerators).&lt;/p&gt;

&lt;p&gt;For our example, we’ll use the The &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Census+Income&quot;&gt;Census Income Data
Set&lt;/a&gt; hosted by the &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/&quot;&gt;UC Irvine Machine Learning
Repository&lt;/a&gt;. We have hosted the data
on &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; (GCS) in a slightly cleaned form. We’ll use this dataset to predict income category based on various information about a person.&lt;/p&gt;

&lt;p&gt;This post omits some of the details of the example.
To see the specifics and work through the code yourself, visit the &lt;a href=&quot;http://jupyter.org/&quot;&gt;Jupyter&lt;/a&gt; notebook &lt;a href=&quot;https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb&quot;&gt;here&lt;/a&gt;.
(The example in the &lt;a href=&quot;https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb&quot;&gt;notebook&lt;/a&gt; is a slightly modified version of &lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator/trainer&quot;&gt;this example&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;step-1-create-an-estimator&quot;&gt;Step 1: Create an Estimator&lt;/h2&gt;

&lt;p&gt;The TensorFlow &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator&quot;&gt;Estimator&lt;/a&gt; class wraps a model, and provides built-in support for distributed training and evaluation. You should nearly always use Estimators to create your TensorFlow models. ‘Pre-made’ Estimator subclasses are an effective way to quickly create standard models, and you can build a &lt;a href=&quot;https://www.tensorflow.org/extend/estimators&quot;&gt;Custom Estimator&lt;/a&gt; if none of the pre-made Estimators suit your purpose.&lt;/p&gt;

&lt;p&gt;For this example, we’ll create an &lt;a href=&quot;https://www.tensorflow.org/get_started/estimator&quot;&gt;Estimator&lt;/a&gt; object using a pre-made subclass, &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier&quot;&gt;&lt;code&gt;DNNLinearCombinedClassifier&lt;/code&gt;&lt;/a&gt;, which implements a &lt;a href=&quot;https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html&quot;&gt;“wide and deep”&lt;/a&gt; model. Wide and deep models use a deep neural net (DNN) to learn high level abstractions about complex features or interactions between such features. These models then combine the outputs from the DNN with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;&gt;linear regression&lt;/a&gt; performed on simpler features. This provides a balance between power and speed that is effective on many structured data problems.&lt;/p&gt;

&lt;p&gt;See the accompanying &lt;a href=&quot;https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#First-step:-create-an-Estimator&quot;&gt;notebook&lt;/a&gt; for the details of defining our Estimator, including specification of the expected format of the input data.
The data is in csv format, and looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &amp;lt;=50K
50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, &amp;lt;=50K
38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, &amp;lt;=50K
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We’ll use the last field, which indicates income bracket, as our label, meaning that this is the value we’ll predict based on the values of the other fields.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#First-step:-create-an-Estimator&quot;&gt;notebook&lt;/a&gt;, we define a &lt;code&gt;build_estimator&lt;/code&gt; function, which takes as input config info, and returns a &lt;code&gt;tf.estimator.DNNLinearCombinedClassifier&lt;/code&gt; object.
We’ll call it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;run_config = tf.estimator.RunConfig()
run_config = run_config.replace(model_dir=output_dir)

FIRST_LAYER_SIZE = 100  # Number of nodes in the first layer of the DNN
NUM_LAYERS = 4  # Number of layers in the DNN
SCALE_FACTOR = 0.7  # How quickly should the size of the layers in the DNN decay
EMBEDDING_SIZE = 8  # Number of embedding dimensions for categorical columns

estimator = build_estimator(
    embedding_size=EMBEDDING_SIZE,
    # Construct layers sizes with exponential decay
    hidden_units=[
        max(2, int(FIRST_LAYER_SIZE *
                   SCALE_FACTOR**i))
        for i in range(NUM_LAYERS)
    ],
    config=run_config
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-2-define-input-functions-using-datasets&quot;&gt;Step 2: Define input functions using Datasets&lt;/h2&gt;

&lt;p&gt;Now that we have defined our model structure, the next step is to use it for training and evaluation.
As with any &lt;code&gt;Estimator&lt;/code&gt;, we’ll need to tell the &lt;code&gt;DNNLinearCombinedClassifier&lt;/code&gt; object how to get its training and eval data. We’ll define a function (&lt;code&gt;input_fn&lt;/code&gt;) that knows how to generate features and labels for training or evaluation, then use that definition to create the actual train and eval input functions.&lt;/p&gt;

&lt;p&gt;We’ll use &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/data/Dataset&quot;&gt;Datasets&lt;/a&gt; to access our data.
This API is a new way to create &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md&quot;&gt;input pipelines to TensorFlow models&lt;/a&gt;.
The &lt;code&gt;Dataset&lt;/code&gt; API is much more performant than using &lt;code&gt;feed_dict&lt;/code&gt; or the queue-based pipelines, and it’s &lt;a href=&quot;https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html&quot;&gt;cleaner and easier&lt;/a&gt; to use.&lt;/p&gt;

&lt;p&gt;In this simple example, our datasets are too small for the use of the Dataset API to make a large difference, but with larger datasets it becomes much more important.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;input_fn&lt;/code&gt; definition is below. It uses a couple of helper functions that are defined in the accompanying &lt;a href=&quot;https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Define-input-functions-(using-Datasets)&quot;&gt;notebook&lt;/a&gt;. One of these,
&lt;code&gt;parse_label_column&lt;/code&gt;, is used to convert the label strings (in our case, ‘ &amp;lt;=50K’ and ‘ &amp;gt;50K’) into &lt;a href=&quot;https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding&quot;&gt;one-hot&lt;/a&gt; encodings, which map categorical features into a format that works better with most ML classification models.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# This function returns a (features, indices) tuple, where features is a dictionary of
# Tensors, and indices is a single Tensor of label indices.
def input_fn(filenames,
                      num_epochs=None,
                      shuffle=True,
                      skip_header_lines=0,
                      batch_size=200):

  dataset = tf.data.TextLineDataset(filenames).skip(skip_header_lines).map(parse_csv)

  if shuffle:
    dataset = dataset.shuffle(buffer_size=batch_size * 10)
  dataset = dataset.repeat(num_epochs)
  dataset = dataset.batch(batch_size)
  iterator = dataset.make_one_shot_iterator()
  features = iterator.get_next()
  return features, parse_label_column(features.pop(LABEL_COLUMN))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we’ll use &lt;code&gt;input_fn&lt;/code&gt; to define both the &lt;code&gt;train_input&lt;/code&gt; and &lt;code&gt;eval_input&lt;/code&gt; functions.  We just need to pass &lt;code&gt;input_fn&lt;/code&gt; the different source files to use for training versus evaluation.
As we’ll see below, these two functions will be used to define a &lt;code&gt;TrainSpec&lt;/code&gt; and &lt;code&gt;EvalSpec&lt;/code&gt; used by &lt;code&gt;train_and_evaluate&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train_input = lambda: input_fn(
    TRAIN_FILES,
    batch_size=40
)

# Don't shuffle evaluation data
eval_input = lambda: input_fn(
    EVAL_FILES,
    batch_size=40,
    shuffle=False
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-3-define-training-and-eval-specs&quot;&gt;Step 3: Define training and eval specs&lt;/h2&gt;

&lt;p&gt;Now we’re nearly set.  We just need to define the the &lt;code&gt;TrainSpec&lt;/code&gt; and &lt;code&gt;EvalSpec&lt;/code&gt; used by &lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;. These specify not only the input functions, but how to export our trained model; that is, how to save it in the standard &lt;a href=&quot;https://www.tensorflow.org/programmers_guide/saved_model&quot;&gt;SavedModel&lt;/a&gt; format, so that we can later use it for serving.&lt;/p&gt;

&lt;p&gt;First, we’ll define the &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec&quot;&gt;&lt;code&gt;TrainSpec&lt;/code&gt;&lt;/a&gt;, which takes as an arg &lt;code&gt;train_input&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train_spec = tf.estimator.TrainSpec(train_input,
                                  max_steps=1000
                                  )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For our &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/EvalSpec&quot;&gt;&lt;code&gt;EvalSpec&lt;/code&gt;&lt;/a&gt;, we’ll instantiate it with something additional – a list of &lt;em&gt;exporters&lt;/em&gt;, that specify how to export (save) the trained model so that it can be used for serving with respect to a particular data input format. Here we’ll just define one such exporter.&lt;/p&gt;

&lt;p&gt;To specify our exporter, we must first define a
&lt;a href=&quot;https://www.tensorflow.org/programmers_guide/saved_model#preparing_serving_inputs&quot;&gt;&lt;em&gt;serving input function&lt;/em&gt;&lt;/a&gt;.
This is what determines the input format that the exporter will accept.
As we saw above, during training, an &lt;code&gt;input_fn()&lt;/code&gt; ingests data and prepares it for use by the model.
At serving time, similarly, a &lt;code&gt;serving_input_receiver_fn()&lt;/code&gt; accepts inference requests and prepares them for the model. This function has the following purposes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To add placeholders to the model graph that the serving system will feed with inference requests.&lt;/li&gt;
  &lt;li&gt;To add any additional ops needed to convert data from the input format into the feature Tensors expected by the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The serving input function should return a &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver&quot;&gt;&lt;code&gt;tf.estimator.export.ServingInputReceiver&lt;/code&gt;&lt;/a&gt; object, which packages the placeholders and the resulting feature &lt;code&gt;Tensors&lt;/code&gt; together.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;ServingInputReceiver&lt;/code&gt; is instantiated with two arguments — &lt;code&gt;features&lt;/code&gt; and &lt;code&gt;receiver_tensors&lt;/code&gt;. The &lt;code&gt;features&lt;/code&gt; represent the inputs to our Estimator when it is being served for prediction. The &lt;code&gt;receiver_tensors&lt;/code&gt; represent inputs to the server.&lt;/p&gt;

&lt;p&gt;These two arguments will not necessarily always be the same — in some cases we may want to perform some transformation(s) before feeding the data to the model. &lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/estimator/trainer/model.py#L197&quot;&gt;Here’s&lt;/a&gt; one example of that, where the inputs to the server (csv-formatted rows) include a field to be removed.&lt;/p&gt;

&lt;p&gt;However, in our case, the inputs to the server are the same as the features input to the model. Here’s what our serving input function looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def json_serving_input_fn():
  &quot;&quot;&quot;Build the serving inputs.&quot;&quot;&quot;
  inputs = {}
  for feat in INPUT_COLUMNS:
    inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

  return tf.estimator.export.ServingInputReceiver(inputs, inputs)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we define an &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/Exporter&quot;&gt;Exporter&lt;/a&gt; in terms of that serving input function. It will export the model in &lt;a href=&quot;https://www.tensorflow.org/programmers_guide/saved_model&quot;&gt;SavedModel&lt;/a&gt; format. We pass the &lt;code&gt;EvalSpec&lt;/code&gt; constructor a list of exporters (here, just one).&lt;/p&gt;

&lt;p&gt;Here, we’re using
the &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/FinalExporter&quot;&gt;&lt;code&gt;FinalExporter&lt;/code&gt;&lt;/a&gt; class.  This class performs a single export at the end of training. This is in contrast to
&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/estimator/LatestExporter&quot;&gt;&lt;code&gt;LatestExporter&lt;/code&gt;&lt;/a&gt;, which does regular exports and retains the last &lt;code&gt;N&lt;/code&gt;. (We’re just using one exporter here, but if you define multiple exporters, training will result in multiple saved models).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;exporter = tf.estimator.FinalExporter('census',
      json_serving_input_fn)
eval_spec = tf.estimator.EvalSpec(eval_input,
                                steps=100,
                                exporters=[exporter],
                                name='census-eval'
                                )
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-4-train-your-model-using-trainandevaluate&quot;&gt;Step 4: Train your model using &lt;code&gt;train_and_evaluate&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Now we have defined everything we need to train and evaluate our model, to and export the trained model for serving, via a call to &lt;strong&gt;&lt;code&gt;train_and_evaluate&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This call will train the model and export the result in a format that is easy to use for prediction!&lt;/p&gt;

&lt;p&gt;With &lt;code&gt;train_and_evaluate&lt;/code&gt;, the training behavior will be consistent whether you run this function in a local/non-distributed context or in a distributed configuration.&lt;/p&gt;

&lt;p&gt;The exported trained model can be served on many platforms. You may particularly want to consider ways to scalably serve your model, in order to handle many prediction requests at once— say if you’re using your model in an app you’re building, and you expect it to become popular. &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/prediction-overview&quot;&gt;Cloud ML Engine online prediction&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org/serving/&quot;&gt;TensorFlow serving&lt;/a&gt;) are two options for doing this.&lt;/p&gt;

&lt;p&gt;In this post, we’ll look at using &lt;strong&gt;Cloud ML Engine Online Prediction&lt;/strong&gt;. But first, let’s take a closer look at our exported model.&lt;/p&gt;

&lt;h3 id=&quot;examine-the-signature-of-the-exported-model&quot;&gt;Examine the signature of the exported model.&lt;/h3&gt;

&lt;p&gt;TensorFlow ships with a CLI that allows you to inspect the &lt;em&gt;signature&lt;/em&gt; of exported binary files. This can be useful as a sanity check.
It’s run as follows, by passing it the path to directory containing the &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md&quot;&gt;saved model&lt;/a&gt;, which will be called &lt;code&gt;saved_model.pb&lt;/code&gt;.
For our model, it will be found under &lt;code&gt;$output_dir/export/census&lt;/code&gt;.  This is because we passed the &lt;code&gt;census&lt;/code&gt; name to our &lt;code&gt;FinalExporter&lt;/code&gt; above.  (&lt;code&gt;$output_dir&lt;/code&gt; was specified when we constructed our Estimator).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;saved_model_cli show --dir $output_dir/export/census/&amp;lt;timestamp&amp;gt; --tag serve --signature_def predict
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;saved_model_cli&lt;/code&gt; command shows us this info (abbreviated for conciseness):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The given SavedModel SignatureDef contains the following input(s):
inputs['age'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder_8:0
inputs['capital_gain'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder_10:0
inputs['capital_loss'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder_11:0
inputs['education'] tensor_info:
    dtype: DT_STRING
    shape: (-1)
    name: Placeholder_2:0
&amp;lt;... more input fields here ...&amp;gt;
The given SavedModel SignatureDef contains the following output(s):
outputs['class_ids'] tensor_info:
    dtype: DT_INT64
    shape: (-1, 1)
    name: head/predictions/classes:0
outputs['classes'] tensor_info:
    dtype: DT_STRING
    shape: (-1, 1)
    name: head/predictions/str_classes:0
outputs['logistic'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1, 1)
    name: head/predictions/logistic:0
outputs['logits'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1, 1)
    name: head/predictions/logits:0
outputs['probabilities'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1, 2)
    name: head/predictions/probabilities:0
Method name is: tensorflow/serving/predict
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on our knowledge of &lt;code&gt;DNNLinearCombinedClassifier&lt;/code&gt;, and the input fields we defined, this looks as we expect. (Notice that the model generates multiple outputs).&lt;/p&gt;

&lt;h3 id=&quot;check-local-prediction-with-gcloud&quot;&gt;Check local prediction with gcloud&lt;/h3&gt;

&lt;p&gt;Another useful sanity check is running local prediction with your trained model. We’ll use the &lt;a href=&quot;https://cloud.google.com/sdk/downloads&quot;&gt;Google Cloud SDK (gcloud)&lt;/a&gt; command-line tool for that.&lt;/p&gt;

&lt;p&gt;We’ll use the example input in &lt;a href=&quot;https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/test.json&quot;&gt;&lt;code&gt;test.json&lt;/code&gt;&lt;/a&gt; to predict a person’s income bracket based on the features encoded in the &lt;code&gt;test.json&lt;/code&gt; instance. Again, we point to the directory containing the saved model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;gcloud ml-engine local predict --model-dir $output_dir/export/census/&amp;lt;timestamp&amp;gt; --json-instances test.json
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CLASS_IDS  CLASSES  LOGISTIC               LOGITS                 PROBABILITIES
[0]        [u'0']   [0.06585630029439926]  [-2.6521551609039307]  [0.9341437220573425, 0.06585630774497986]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see how the input fields in &lt;code&gt;test.json&lt;/code&gt; correspond to the inputs listed by the &lt;code&gt;saved_model_cli&lt;/code&gt; command above, and how the prediction outputs correspond to the outputs listed by &lt;code&gt;saved_model_cli&lt;/code&gt;.
In this model, Class 0 indicates income &amp;lt;= 50k and Class 1 indicates income &amp;gt;50k.&lt;/p&gt;

&lt;h2 id=&quot;using-cloud-ml-engine-for-easy-distributed-training-and-scalable-online-prediction&quot;&gt;Using Cloud ML Engine for easy distributed training and scalable online prediction&lt;/h2&gt;

&lt;p&gt;In the previous section, we looked at how to use &lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt; first to train and export a model, and then to make predictions using the trained model.&lt;/p&gt;

&lt;p&gt;In this section, we’ll see how easy it is to use the same code — without any changes — to do &lt;strong&gt;distributed training on &lt;a href=&quot;https://cloud.google.com/ml-engine/&quot;&gt;Cloud ML Engine&lt;/a&gt;&lt;/strong&gt;, thanks to the &lt;strong&gt;&lt;code&gt;Estimator&lt;/code&gt;&lt;/strong&gt; class and &lt;strong&gt;&lt;code&gt;train_and_evaluate&lt;/code&gt;&lt;/strong&gt;.  Then we’ll use &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/online-predict&quot;&gt;&lt;strong&gt;Cloud ML Engine Online Prediction&lt;/strong&gt;&lt;/a&gt; to scalably serve the trained model.&lt;/p&gt;

&lt;p&gt;One advantage of Cloud ML Engine is that there’s no lock-in. You could potentially train your TensorFlow model elsewhere, then deploy to Cloud ML Engine for serving (prediction); or alternately use Cloud ML Engine for distributed training and then serve elsewhere (e.g. with &lt;a href=&quot;https://github.com/tensorflow/serving&quot;&gt;TensorFlow serving&lt;/a&gt;).  Here, we’ll show how to use Cloud ML Engine for both stages.&lt;/p&gt;

&lt;p&gt;To launch a training job on Cloud ML Engine, we can again use &lt;code&gt;gcloud&lt;/code&gt;.  We’ll need to package our code so that it can be deployed, and specify the Python file to run to start the training (&lt;code&gt;--module-name&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;trainer&lt;/code&gt; module code is &lt;a href=&quot;https://github.com/amygdala/code-snippets/tree/master/ml/census_train_and_eval/trainer&quot;&gt;here&lt;/a&gt;.
&lt;code&gt;trainer.task&lt;/code&gt; is the entry point, and when that file is run, it calls &lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;.
(You can read more about how to package your code &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/packaging-trainer&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;If we want to, we could test (distributed) training via &lt;code&gt;gcloud&lt;/code&gt; locally first, to make sure that we have everything packaged up correctly. See the accompanying &lt;a href=&quot;https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb&quot;&gt;notebook&lt;/a&gt; for details.&lt;/p&gt;

&lt;p&gt;But here, we’ll jump right in to using Cloud ML Engine to do cloud-based distributed training.&lt;/p&gt;

&lt;p&gt;We’ll set the training job to use the &lt;code&gt;SCALE_TIER_STANDARD_1&lt;/code&gt; scale spec.  This &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/training-overview#job_configuration_parameters&quot;&gt;gives us&lt;/a&gt; one ‘master’ instance, plus four &lt;em&gt;workers&lt;/em&gt; and three &lt;em&gt;parameter servers&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;gcloud ml-engine jobs submit training $JOB_NAME --scale-tier `SCALE_TIER_STANDARD_1` \
    --runtime-version 1.4 --job-dir $GCS_JOB_DIR \
    --module-name trainer.task --package-path trainer/ \
    --region us-central1 \
    -- --train-steps 5000 --train-files $GCS_TRAIN_FILE --eval-files $GCS_EVAL_FILE --eval-steps 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The cool thing about this is that &lt;strong&gt;we don’t need to change our code at all to use this distributed config&lt;/strong&gt;.  Our use of the Estimator class in conjunction with the Cloud ML Engine scale specification makes the distributed training config transparent to us — it just works.
Further, we could swap in any of the other predefined scale tiers (say &lt;code&gt;BASIC_GPU&lt;/code&gt;), or define our own custom cluster, again without any code changes.
For example, we could alternatively &lt;a href=&quot;https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Extras:-Train-on-CMLE-using-a-custom-GPU-cluster&quot;&gt;configure our job&lt;/a&gt; to &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/using-gpus&quot;&gt;use a GPU cluster&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once our training job is running, we can stream its logs to the terminal, and/or monitor it in the &lt;a href=&quot;https://console.cloud.google.com/mlengine/jobs&quot;&gt;Cloud Console&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/census_train_eval/ml_jobs.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/census_train_eval/ml_jobs.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the logs, you’ll see output from the multiple worker replicas and parameter servers that we utilized by specifying a &lt;code&gt;SCALE_TIER_STANDARD_1 &lt;/code&gt; cluster.  In the logs viewers, you can filter on the output of a particular node (e.g. a given worker) if you like.&lt;/p&gt;

&lt;p&gt;Once your job is finished, you’ll find the exported model under the specified GCS directory, in addition to other data such as model checkpoints.
That exported model has exactly the same signature as the locally-generated model we looked at above, and can be used in just the same ways.&lt;/p&gt;

&lt;h3 id=&quot;scalably-serve-your-trained-model-with-cloud-ml-engine-online-prediction&quot;&gt;Scalably serve your trained model with Cloud ML Engine online prediction&lt;/h3&gt;

&lt;p&gt;You can deploy an exported model to Cloud ML Engine and scalably serve it for &lt;strong&gt;prediction&lt;/strong&gt;, using the &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/prediction-overview&quot;&gt;Cloud ML Engine prediction service&lt;/a&gt; to generate a prediction on new data with an easy-to-use REST API. Here we’ll look at Cloud ML Engine online prediction, which &lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/12/bringing-cloud-ml-engine-to-more-developers-with-online-prediction-features-and-reduced-prices&quot;&gt;recently moved to general availability (GA) status&lt;/a&gt;; but &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/batch-predict&quot;&gt;batch prediction&lt;/a&gt; is supported as well.&lt;/p&gt;

&lt;p&gt;The online prediction service scales the number of nodes it uses to maximize the number of requests it can handle without introducing too much latency. To do that, the service:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Allocates some nodes the first time you request predictions after a long pause in requests.&lt;/li&gt;
  &lt;li&gt;Scales the number of nodes in response to request traffic, adding nodes when traffic increases, and removing them when there are fewer requests.&lt;/li&gt;
  &lt;li&gt;Keeps at least one node ready to handle requests even when there are none to handle. It then scales down to zero by default when your model version goes several minutes without a prediction request (but if you like, you can specify a minimum number of nodes to keep ready for a given model).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the accompanying &lt;a href=&quot;https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Scalably-serve-your-trained-model-with-CMLE-Online-Prediction&quot;&gt;notebook&lt;/a&gt; for details on how to deploy your model so that you can use it to make predictions.&lt;/p&gt;

&lt;p&gt;Once your model is serving with Cloud ML Engine Online Prediction, you can access it via a REST API.  It’s &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/online-predict#requesting_predictions&quot;&gt;easy&lt;/a&gt; to do this programmatically via the Google &lt;a href=&quot;https://cloud.google.com/apis/docs/cloud-client-libraries&quot;&gt;Cloud Client libraries&lt;/a&gt; or via &lt;code&gt;gcloud&lt;/code&gt;.  &lt;br /&gt;
&lt;code&gt;gcloud&lt;/code&gt; is great for testing your deployed model, and the command looks almost the same as it did for the local version of the model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;gcloud ml-engine predict --model census --version v1 --json-instances test.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Cloud Console makes it easy to inspect the different versions of a model, as well as set the default version: &lt;a href=&quot;https://console.cloud.google.com/mlengine/models&quot;&gt;console.cloud.google.com/mlengine/models&lt;/a&gt;.
You can list your model information using &lt;code&gt;gcloud&lt;/code&gt; too.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/census_train_eval/ml_model_details.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/census_train_eval/ml_model_details.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary--and-whats-next&quot;&gt;Summary — and what’s next?&lt;/h2&gt;

&lt;p&gt;In this post, we’ve walked through how to configure and use the TensorFlow &lt;code&gt;Estimator&lt;/code&gt; class, and
&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;.  They enable distributed execution for training and evaluation, while also supporting local execution, and provides consistent behavior across both local/non-distributed and distributed configurations.&lt;/p&gt;

&lt;p&gt;For more, see the accompanying &lt;a href=&quot;https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb&quot;&gt;notebook&lt;/a&gt;.  The notebook includes examples of &lt;a href=&quot;https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Extras:-Train-on-CMLE-using-a-custom-GPU-cluster&quot;&gt;how to run your training job on a Cloud ML Engine GPU cluster&lt;/a&gt;, and &lt;a href=&quot;https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Extras:-Use-Hyperparameter-Tuning&quot;&gt;how to use Cloud ML Engine to do&lt;/a&gt; &lt;a href=&quot;https://cloud.google.com/ml-engine/docs/hyperparameter-tuning-overview&quot;&gt;hyperparameter tuning&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Fri, 26 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://amygdala.github.io/ml/tensorflow/cloud_ml_engine/2018/01/26/tf.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/ml/tensorflow/cloud_ml_engine/2018/01/26/tf.html</guid>
        
        <category>cmle</category>
        
        <category>cloud_ml</category>
        
        <category>tensorflow</category>
        
        <category>machine_learning</category>
        
        
        <category>ML</category>
        
        <category>TensorFlow</category>
        
        <category>Cloud_ML_Engine</category>
        
      </item>
    
      <item>
        <title>Using Cloud Dataflow pipeline templates from App Engine</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This post describes how to use &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt;
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/overview&quot;&gt;job templates&lt;/a&gt;
to easily launch &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Dataflow&lt;/a&gt; pipelines from a &lt;a href=&quot;https://cloud.google.com/appengine/&quot;&gt;Google App Engine (GAE)&lt;/a&gt; app,
in order to support &lt;a href=&quot;https://en.wikipedia.org/wiki/MapReduce&quot;&gt;MapReduce&lt;/a&gt;
jobs and many other data processing and analysis tasks.&lt;/p&gt;

&lt;p&gt;This post builds on a &lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;previous post&lt;/a&gt;, which
used a &lt;a href=&quot;https://cloud.google.com/appengine/docs/flexible/&quot;&gt;GAE Flexible&lt;/a&gt;
&lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/python/an-overview-of-app-engine#services_the_building_blocks_of_app_engine&quot;&gt;service&lt;/a&gt; 
to periodically launch a Python Dataflow pipeline.  The use of GAE Flex was necessary at the time, because we needed
to install the &lt;a href=&quot;https://cloud.google.com/sdk/&quot;&gt;&lt;code&gt;gcloud&lt;/code&gt; sdk&lt;/a&gt; in the instance container(s) in order to launch the pipelines.&lt;/p&gt;

&lt;p&gt;Since then, Cloud Dataflow &lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/overview&quot;&gt;templates&lt;/a&gt; have come into the
picture for the Python SDK. Dataflow templates allow you to stage your pipelines on
&lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; and execute them from a variety of environments.
This has a number of benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;With templates, you don’t have to recompile your code every time you execute a pipeline.&lt;/li&gt;
  &lt;li&gt;This means that you don’t need to launch your pipeline from a development environment or worry about dependencies.&lt;/li&gt;
  &lt;li&gt;It’s much easier for non-technical users to launch pipelines using templates.  You can launch via 
the &lt;a href=&quot;https://console.cloud.google.com&quot;&gt;Google Cloud Platform Console&lt;/a&gt;, the &lt;code&gt;gcloud&lt;/code&gt; command-line interface, or the REST API.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we’ll show how to use the Dataflow job template
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/reference/rest/#collection-v1b3projectslocationstemplates&quot;&gt;REST API&lt;/a&gt;
to periodically launch a Dataflow templated job from GAE.  Because we’re now simply calling an
API, and no longer relying on the &lt;code&gt;gcloud&lt;/code&gt; sdk to launch from App Engine, we can build a simpler &lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/&quot;&gt;App Engine
Standard&lt;/a&gt; app.&lt;/p&gt;

&lt;p&gt;With templates, you can use
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#modifying-your-code-to-use-runtime-parameters&quot;&gt;runtime parameters&lt;/a&gt;
to customize the execution.  We’ll use that feature in this example too.&lt;/p&gt;

&lt;p&gt;The pipeline used in this example is nearly the same as that described in the
&lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;earlier post&lt;/a&gt;; it analyzes data
stored in &lt;a href=&quot;https://cloud.google.com/datastore/&quot;&gt;Cloud Datastore&lt;/a&gt; — in
this case, stored tweets fetched periodically from Twitter.
The pipeline does several sorts of analysis on the tweet data; for example, it identifies important word co-occurrences in the tweets, based on a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;tf*idf&lt;/a&gt; metric.&lt;/p&gt;

&lt;figure&gt;
&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq2.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq2.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;i&gt;Detecting important word co-occurrences in tweets&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;defining-a-parameterized-dataflow-pipeline-and-creating-a-template&quot;&gt;Defining a parameterized Dataflow pipeline and creating a template&lt;/h2&gt;

&lt;p&gt;The first step in building our app is creating a Dataflow template. We do this by building a pipeline and then
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#creating-and-staging-templates&quot;&gt;deploying&lt;/a&gt;
it with the &lt;code&gt;--template_location&lt;/code&gt; flag, which causes the template to be compiled and stored at the given
&lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Google Cloud Storage (GCS)&lt;/a&gt; location.&lt;/p&gt;

&lt;p&gt;You can see the pipeline definition &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/job_template_launch/dfpipe/pipe.py&quot;&gt;here&lt;/a&gt;.
It reads recent tweets from the past N days from Cloud Datastore, then splits into three processing branches.
It finds the most popular words in terms of the percentage of tweets they were found in, calculates the most
popular URLs in terms of their count, and then derives relevant word co-occurrences using an approximation to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt; &lt;em&gt;tf*idf&lt;/em&gt;&lt;/a&gt;
ranking metric.  It writes the results to three BigQuery tables. (It would be equally straightforward to write results
to Datastore instead/as well).&lt;/p&gt;

&lt;figure&gt;
&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/df_template_pipe.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/df_template_pipe.png&quot; width=&quot;600&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;i&gt;The dataflow pipeline graph.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The &lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;previous post&lt;/a&gt; in this series
goes into a bit more detail about what some of the pipeline steps do, and how the pipeline accesses the Datastore.&lt;/p&gt;

&lt;p&gt;As part of our new template-ready pipeline definition, we’ll specify that the pipeline takes a
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#modifying-your-code-to-use-runtime-parameters&quot;&gt;runtime argument&lt;/a&gt;, 
named &lt;code&gt;timestamp&lt;/code&gt;. This value is used to filter out tweets N days older than the timestamp, so that the pipeline analyzes
only recent activity.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class UserOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
      parser.add_value_provider_argument('--timestamp', type=str)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, that argument can be accessed at runtime from a template-generated pipeline, as in this snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;  user_options = pipeline_options.view_as(UserOptions)
  ...
  wc_records = top_percents | 'format' &amp;gt;&amp;gt; beam.FlatMap(
      lambda x: [{'word': xx[0], 'percent': xx[1], 
                  'ts': user_options.timestamp.get()} for xx in x])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The example includes a template creation utility script called &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/job_template_launch/create_template.py&quot;&gt;&lt;code&gt;create_template.py&lt;/code&gt;&lt;/a&gt;, which
sets some pipeline options, including the &lt;code&gt;--template_location&lt;/code&gt; flag, defines the pipeline (via &lt;code&gt;pipe.process_datastore_tweets()&lt;/code&gt;), and calls &lt;code&gt;run()&lt;/code&gt; on it. The core of this script is shown below.
Note that the &lt;code&gt;pipeline_options&lt;/code&gt; dict doesn’t include &lt;code&gt;timestamp&lt;/code&gt;; we’ll define that
at runtime, not compile time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import dfpipe.pipe as pipe
...
pipeline_options = {
    'project': PROJECT,
    'staging_location': 'gs://' + BUCKET + '/staging',
    'runner': 'DataflowRunner',
    'setup_file': './setup.py',
    'job_name': PROJECT + '-twcount',
    'temp_location': 'gs://' + BUCKET + '/temp',
    'template_location': 'gs://' + BUCKET + '/templates/' + PROJECT + '-twproc_tmpl'
}
pipeline_options = PipelineOptions.from_dictionary(pipeline_options)
pipe.process_datastore_tweets(PROJECT, DATASET, pipeline_options)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because we used the &lt;code&gt;--template_location&lt;/code&gt; flag, a template for that pipeline
is compiled and saved to the indicated GCS location (rather than triggering a run of the pipeline).&lt;/p&gt;

&lt;p&gt;Now that the template is created, we can use it to launch Dataflow pipeline jobs from our GAE app.&lt;/p&gt;

&lt;h3 id=&quot;a-note-on-input-sources-and-template-runtime-arguments&quot;&gt;A note on input sources and template runtime arguments&lt;/h3&gt;

&lt;p&gt;As you can see from &lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#pipeline-io-and-runtime-parameters&quot;&gt;this table&lt;/a&gt;
in the documentation, the Dataflow Python SDK does not yet support the use of runtime parameters with Datastore input.&lt;/p&gt;

&lt;p&gt;For pipeline analysis, we want to consider only Datastore data from the last N days.
But, because of the above constraint, we can’t access the runtime &lt;code&gt;timestamp&lt;/code&gt; parameter when we’re constructing the
Datastore reader query. (If you try, you will see a compile-time error). Similarly, if you try the approach taken by
the non-template version of the pipeline &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/dfpipe/pipe.py&quot;&gt;here&lt;/a&gt;,
which uses &lt;code&gt;datetime.datetime.now()&lt;/code&gt; to construct its Datastore query, you’ll find that you’re always using the same
compile-time static timestamp each time you run the template.&lt;/p&gt;

&lt;p&gt;To work around this for the template version of this pipeline, we will include a filter step, that &lt;em&gt;can&lt;/em&gt; access runtime parameters, and which filters out all but the last N days of tweets post-query.
You can see this step as &lt;code&gt;FilterDate&lt;/code&gt; in the Dataflow pipeline graph figure above.&lt;/p&gt;

&lt;h3 id=&quot;launching-a-dataflow-templated-job-from-the-cloud-console&quot;&gt;Launching a Dataflow templated job from the Cloud Console&lt;/h3&gt;

&lt;p&gt;Before we actually deploy the GAE app, let’s check that we can launch a properly running Dataflow templated job
from our newly generated template. We can do that by launching a job based on that template from &lt;a href=&quot;https://console.cloud.google.com&quot;&gt;Cloud
Console&lt;/a&gt;.  (You could also do this via the &lt;code&gt;gcloud&lt;/code&gt; command-line tool). Note that the
pipeline won’t do anything interesting unless you already have Tweet data in the Datastore— which would be the case if
you tried the &lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;earlier example&lt;/a&gt; in this
series— but you can still confirm that it launches and runs successfully.&lt;/p&gt;

&lt;p&gt;Go to the &lt;a href=&quot;https://console.cloud.google.com/dataflow&quot;&gt;Dataflow pane&lt;/a&gt; of Cloud Console, and click “Create Job From
Template”.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/job_templates1.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/job_templates1.png&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;&lt;i&gt;Creating a Dataflow job from a template.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Select “Custom Template”, then browse to your new template’s location in GCS. This info was output when you ran
&lt;code&gt;create_template.py&lt;/code&gt;. (The pulldown menu also includes some predefined templates that you may want to explore later).&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/job_templates2.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/job_templates2.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;&lt;i&gt;Select &quot;Custom Template&quot;, and specify the path to the template file.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Finally, set your pipeline’s defined runtime parameter(s). In this case, we have one: &lt;code&gt;timestamp&lt;/code&gt;. The pipeline is
expecting a value in a format like this:&lt;br /&gt;
&lt;code&gt;2017-10-22 10:18:13.491543&lt;/code&gt; (you can generate such a string in the Python interpreter via &lt;code&gt;str(datetime.datetime.now())&lt;/code&gt;).&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/job_templates3.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/job_templates3.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;&lt;i&gt;Set your pipeline's runtime parameter(s) before running the job.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;While we don’t show it here, &lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/creating-templates#metadata&quot;&gt;you can extend your templates with additional
metadata&lt;/a&gt; so that custom parameters may be
validated when the template is executed.&lt;/p&gt;

&lt;p&gt;Once you click ‘Run Job’, you should be able to see your job running in Cloud Console.&lt;/p&gt;

&lt;h2 id=&quot;using-an-app-engine-app-to-periodically-launch-dataflow-jobs-and-fetch-tweets&quot;&gt;Using an App Engine app to periodically launch Dataflow jobs (and fetch Tweets)&lt;/h2&gt;

&lt;p&gt;Now that we’ve checked that we can successfully launch a Dataflow job using our template, we’ll define an App
Engine app handler to launch such jobs via the Dataflow job template
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/reference/rest/#collection-v1b3projectslocationstemplates&quot;&gt;REST API&lt;/a&gt;, and
run that handler periodically via a GAE cron.
We’ll use another handler of the same app to periodically fetch tweets and store them in the Datastore.&lt;/p&gt;

&lt;p&gt;You can see the GAE app script &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/job_template_launch/main.py&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
The &lt;code&gt;FetchTweets&lt;/code&gt; handler fetches tweets and stores them in the Datastore.
See the &lt;a href=&quot;http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html&quot;&gt;previous post&lt;/a&gt; in this series for a bit more info on that.  However, this part of the app is just
for example purposes; in your own apps, you probably already have some other means of collecting and storing data in Datastore.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;LaunchJob&lt;/code&gt; handler is the new piece of the puzzle: using the Dataflow REST API, it sets the &lt;code&gt;timestamp&lt;/code&gt; runtime parameter, and launches a Dataflow job using the template.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from googleapiclient.discovery import build
from oauth2client.client import GoogleCredentials
...
    credentials = GoogleCredentials.get_application_default()
    service = build('dataflow', 'v1b3', credentials=credentials)

    BODY = {
            &quot;jobName&quot;: &quot;{jobname}&quot;.format(jobname=JOBNAME),
            &quot;gcsPath&quot;: &quot;gs://{bucket}/templates/{template}&quot;.format(
                bucket=BUCKET, template=TEMPLATE),
            &quot;parameters&quot;: {&quot;timestamp&quot;: str(datetime.datetime.utcnow())},
             &quot;environment&quot;: {
                &quot;tempLocation&quot;: &quot;gs://{bucket}/temp&quot;.format(bucket=BUCKET),
                &quot;zone&quot;: &quot;us-central1-f&quot;
             }
        }

    dfrequest = service.projects().templates().create(
        projectId=PROJECT, body=BODY)
    dfresponse = dfrequest.execute()
    logging.info(dfresponse)
    self.response.write('Done')
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;launching-the-dataflow-pipeline-periodically-using-a-gae-cron&quot;&gt;Launching the Dataflow pipeline periodically using a GAE cron&lt;/h2&gt;

&lt;p&gt;For our GAE app, we want to launch a Dataflow templated job every few hours, where each job analyzes the tweets from the past few days, providing a ‘moving window’ of analysis. 
So, it makes sense to set things using a &lt;a href=&quot;https://cloud.google.com/appengine/docs/flexible/python/scheduling-jobs-with-cron-yaml&quot;&gt;cron.yaml&lt;/a&gt; file like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;cron:
- description: fetch tweets
  url: /timeline
  schedule: every 17 minutes
  target: default
- description: launch dataflow pipeline
  url: /launchtemplatejob
  schedule: every 5 hours
  target: default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A GAE app makes it easy to run such a cron, but note that now that we’re using templates, it becomes easier to to support this functionality in other ways too.  E.g., it would also be straightforward to use the &lt;code&gt;gcloud&lt;/code&gt; CLI to launch the template job, and set up a local cron job.&lt;/p&gt;

&lt;h2 id=&quot;a-look-at-the-example-results-in-bigquery&quot;&gt;A look at the example results in BigQuery&lt;/h2&gt;

&lt;p&gt;Once our example app is up and running, it periodically runs a Dataflow job that writes the results of its analysis to
BigQuery.  (It would also be straightforward to write results to the Datastore if that makes more sense for your
workflow – or to write to multiple sources).&lt;/p&gt;

&lt;p&gt;With BigQuery, it is easy to run some fun queries on the data. 
For example, we can find recent word co-occurrences that are ‘interesting’ by our metric:&lt;/p&gt;

&lt;figure&gt;
  &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/xScreenshot_2017-10-28_14_28_35%20copy%202.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/xScreenshot_2017-10-28_14_28_35%20copy%202.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;&lt;i&gt;&quot;Interesting&quot; word co-occurrences&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Or we can look for &lt;em&gt;emergent&lt;/em&gt; word pairs, that have become ‘interesting’ in the last day or so (compare April and Oct
2017 results):&lt;/p&gt;

&lt;figure&gt;
  &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/temp_queries.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/temp_queries.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;&lt;i&gt;Emergent (new) interesting word co-occurrences can reflect current news&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We can contrast the ‘interesting’ word pairs with the words that are simply the most popular within a given period (you
can see that most of these words are common, but not particularly newsworthy):&lt;/p&gt;

&lt;figure&gt;
  &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/pScreenshot_2017-10-28_14_27_05%202.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/gae_dataflow/pScreenshot_2017-10-28_14_27_05%202.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;&lt;i&gt;Popular, but not necessarily interesting words&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Or, find the most frequently tweeted URLs from the past few weeks (some URLs are truncated in the output):&lt;/p&gt;

&lt;figure&gt;
  &lt;a href=&quot;https://storage.googleapis.com/amy-jo/images/bq_popurls2b.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/amy-jo/images/bq_popurls2b.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;&lt;i&gt;The most frequently tweeted URLs from the past few weeks (filtering out some of the shortlinks)&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;summary-and-whats-next&quot;&gt;Summary… and what’s next?&lt;/h2&gt;

&lt;p&gt;In this post, we’ve looked at how you can programmatically launch Dataflow pipelines — that read from Datastore — using Cloud Dataflow &lt;a href=&quot;https://cloud.google.com/dataflow/docs/templates/overview&quot;&gt;job templates&lt;/a&gt;, and call the Dataflow
&lt;a href=&quot;https://cloud.google.com/dataflow/docs/reference/rest/#collection-v1b3projectslocationstemplates&quot;&gt;REST API&lt;/a&gt; from an App Engine app.
See the example app’s &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/job_template_launch/README.md&quot;&gt;README&lt;/a&gt; for more detail on how to configure and run the app yourself.&lt;/p&gt;

&lt;p&gt;Dataflow’s expressive programming model makes it easy to build and support a wide range of scalable processing and
analytics tasks. With templates, it becomes much easier to launch pipeline jobs — you don’t have to recompile every time
you execute, or worry about your environment and dependencies. And it’s more straightforward for less technical users to
launch template-based pipelines.&lt;/p&gt;

&lt;p&gt;We hope you find the example app useful as a starting point towards defining new pipeline templates and running
your own analytics — via App Engine apps or otherwise. We look forward to hearing more about what you build!&lt;/p&gt;
</description>
        <pubDate>Tue, 24 Oct 2017 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/dataflow/app_engine/2017/10/24/gae_dataflow.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/dataflow/app_engine/2017/10/24/gae_dataflow.html</guid>
        
        <category>dataflow</category>
        
        <category>gae</category>
        
        
        <category>Dataflow</category>
        
        <category>App_Engine</category>
        
      </item>
    
      <item>
        <title>Running Cloud Dataflow jobs from an App Engine app</title>
        <description>&lt;p&gt;This post looks at how you can launch &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt; pipelines from your &lt;a href=&quot;https://cloud.google.com/appengine/&quot;&gt;App Engine&lt;/a&gt; app, in order to support &lt;a href=&quot;https://en.wikipedia.org/wiki/MapReduce&quot;&gt;MapReduce&lt;/a&gt; jobs and other data processing and analysis tasks.&lt;/p&gt;

&lt;p&gt;Until recently, if you wanted to run MapReduce jobs from a Python App Engine app, you would use &lt;a href=&quot;https://github.com/GoogleCloudPlatform/appengine-mapreduce&quot;&gt;this MR library&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, &lt;a href=&quot;https://beam.apache.org/&quot;&gt;Apache Beam&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt; have entered the picture.  Apache Beam is a unified model for building data processing pipelines that handle bounded and unbounded data, as well as a collection of SDKs for building these pipelines. Google Cloud Dataflow is a managed service for executing parallelized data processing pipelines written using Apache Beam.&lt;/p&gt;

&lt;p&gt;Dataflow allows a wide range of data processing patterns, including ETL, batch computation, and continuous computation.
The Beam model &lt;a href=&quot;https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount.py&quot;&gt;supports&lt;/a&gt; and subsumes MapReduce.  So, you can map your MR jobs to equivalent Beam pipelines, and Beam’s programming model makes it straightforward to extend and modify your existing MR logic.&lt;/p&gt;

&lt;p&gt;The Beam Python SDK makes it easy to launch Dataflow pipeline jobs from a Python App Engine app. The SDK 
includes a &lt;a href=&quot;https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py#L868&quot;&gt;Cloud Datastore &lt;em&gt;source&lt;/em&gt; and &lt;em&gt;sink&lt;/em&gt;&lt;/a&gt;.  This makes it easy to write Dataflow pipelines that support the functionality of any existing MR jobs, as well as support additional analytics.&lt;/p&gt;

&lt;p&gt;In this blog post, we’ll look at an &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/tree/master/sdk_launch&quot;&gt;example app&lt;/a&gt; that shows how to periodically launch a Python Dataflow pipeline from GAE, to analyze data stored in &lt;a href=&quot;https://cloud.google.com/datastore/&quot;&gt;Cloud Datastore&lt;/a&gt;; in this case, stored tweets from Twitter.  The pipeline does several sorts of analysis on the data; for example, it identifies ‘interesting’ word co-occurrences (bigrams) in the tweets, as in this snippet below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq2.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq2.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The example is a GAE app with two &lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/python/an-overview-of-app-engine#services_the_building_blocks_of_app_engine&quot;&gt;services (previously, ‘modules’)&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a &lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/&quot;&gt;GAE Standard&lt;/a&gt; service that periodically pulls in timeline tweets from Twitter and stores them in Datastore; and&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a &lt;a href=&quot;https://cloud.google.com/appengine/docs/flexible/&quot;&gt;GAE Flexible&lt;/a&gt; service that periodically launches the Python Dataflow pipeline to analyze the tweet data in the Datastore.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Standard service– the one that gathers the tweets– is just for example purposes; in your own apps, you probably already have other means of collecting and storing data in Datastore.&lt;/p&gt;

&lt;h2 id=&quot;building-a-service-to-define-and-launch-a-dataflow-pipeline-from-app-engine&quot;&gt;Building a service to define and launch a Dataflow pipeline from App Engine&lt;/h2&gt;

&lt;p&gt;We’ll use a Flex custom runtime based on the &lt;code&gt;gcr.io/google_appengine/python&lt;/code&gt; image for the service that launches the dataflow pipeline, as we’ll install the &lt;code&gt;gcloud&lt;/code&gt; sdk in the instance container(s).  So, the example includes a &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/Dockerfile&quot;&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&lt;/a&gt; used to deploy the service.  As the last command in the &lt;code&gt;Dockerfile&lt;/code&gt;, we’ll start up a Gunicorn server to serve a Flask app script (&lt;code&gt;main_df.py&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;The Python code for this service consists of the small Flask app script (&lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/main_df.py&quot;&gt;&lt;code&gt;main_df.py&lt;/code&gt;&lt;/a&gt;), which accesses a module (&lt;code&gt;dfpipe&lt;/code&gt;) that does most of the heavy lifting in terms of defining and launching the example pipeline (in &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/dfpipe/pipe.py&quot;&gt;&lt;code&gt;dfpipe/pipe.py&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;setting-the-pipeline-options&quot;&gt;Setting the pipeline options&lt;/h3&gt;

&lt;p&gt;As part of the process of launching a Dataflow pipeline, various options may be set.
In order to make the &lt;code&gt;dfpipe&lt;/code&gt; module available to the Dataflow &lt;em&gt;workers&lt;/em&gt;, the pipeline options include a &lt;code&gt;setup_file&lt;/code&gt; flag.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;  pipeline_options = {
      'project': PROJECT,
      'staging_location': 'gs://' + BUCKET + '/staging',
      'runner': 'DataflowRunner',
      'setup_file': './setup.py',
      'job_name': PROJECT + '-twcount',
      'max_num_workers': 10,
      'temp_location': 'gs://' + BUCKET + '/temp'
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This points to a &lt;code&gt;setup.py&lt;/code&gt; file which specifies to package the &lt;code&gt;dfpipe&lt;/code&gt; module using &lt;code&gt;setuptools&lt;/code&gt;. If our pipeline also had dependencies on third-party libs, we could include those in setup.py as well. 
The indicated code is  gathered in a package that is built as a source distribution, staged in the staging area for the workflow being run, and then installed in the workers when they start running.&lt;/p&gt;

&lt;h2 id=&quot;a-look-at-the-dataflow-pipeline&quot;&gt;A look at the Dataflow pipeline&lt;/h2&gt;

&lt;p&gt;Now let’s take a quick look at &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/dfpipe/pipe.py&quot;&gt;&lt;code&gt;dfpipe/pipe.py&lt;/code&gt;&lt;/a&gt;, to see what the Python Dataflow pipeline does.&lt;/p&gt;

&lt;p&gt;It reads recent tweets from the past N days from Cloud Datastore, then
essentially splits into three processing branches. It finds the top N most popular words in terms of
the percentage of tweets they were found in, calculates the top N most popular URLs in terms of
their count, and then derives relevant word co-occurrences (bigrams) using an approximation to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt; &lt;em&gt;tf*idf&lt;/em&gt;&lt;/a&gt;
ranking metric.  It writes the results to three BigQuery tables. (It would be equally straightforward to write results to Datastore instead/as well).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_df_graph.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_df_graph.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;using-datastore-as-a-pipeline-source&quot;&gt;Using Datastore as a pipeline &lt;em&gt;source&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;This pipeline reads from Datastore, grabbing the tweets that the other GAE Standard service is periodically grabbing and writing to the Datastore.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/main.py&quot;&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt;, the app script for the GAE standard service, you can see the Tweet entity schema:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from google.appengine.ext import ndb
class Tweet(ndb.Model):
  user = ndb.StringProperty()
  text = ndb.StringProperty()
  created_at = ndb.DateTimeProperty()
  tid = ndb.IntegerProperty()
  urls = ndb.StringProperty(repeated=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/dfpipe/pipe.py&quot;&gt;&lt;code&gt;dfpipe/pipe.py&lt;/code&gt;&lt;/a&gt;, we can use the &lt;a href=&quot;https://cloud.google.com/datastore/docs/reference/rpc/google.datastore.v1&quot;&gt;&lt;code&gt;google.cloud.proto.datastore&lt;/code&gt;&lt;/a&gt; API to define a query for Tweet entities more recent than a given date— in this case, four days ago— by creating a property filter on the &lt;code&gt;created_at&lt;/code&gt; field.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from google.cloud.proto.datastore.v1 import query_pb2
def make_query(kind):
  &quot;&quot;&quot;Creates a Cloud Datastore query to retrieve all Tweet entities with a
  'created_at' date &amp;gt; N days ago.
  &quot;&quot;&quot;
  days = 4
  now = datetime.datetime.now()
  earlier = now - datetime.timedelta(days=days)

  query = query_pb2.Query()
  query.kind.add().name = kind
  datastore_helper.set_property_filter(query.filter, 'created_at',
                                       PropertyFilter.GREATER_THAN,
                                       earlier)
  return query
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we use that query to define an input source for the pipeline:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; p = beam.Pipeline(options=pipeline_options)
  # Create a query to read entities from datastore.
  query = make_query('Tweet')

  # Read entities from Cloud Datastore into a PCollection.
  lines = (p
      | 'read from datastore' &amp;gt;&amp;gt; ReadFromDatastore(project, query, None))
  ... 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use &lt;code&gt;properties.get()&lt;/code&gt; on an element of the resulting collection to extract the value of a given field of the entity, in this case the ‘text’ field:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class WordExtractingDoFn(beam.DoFn):
  &quot;&quot;&quot;Parse each tweet text into words, removing some 'stopwords'.&quot;&quot;&quot;

  def process(self, element):
    content_value = element.properties.get('text', None)
    text_line = ''
    if content_value:
      text_line = content_value.string_value
    words = set([x.lower() for x in re.findall(r'[A-Za-z\']+', text_line)])
    stopwords = [...]
    return list(words - set(stopwords))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, this snippet from the pipeline shows how  &lt;code&gt;WordExtractingDoFn&lt;/code&gt; can be used as part of the Datastore input processing:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;  # Count the occurrences of each word.
  percents = (lines
      | 'split' &amp;gt;&amp;gt; (beam.ParDo(WordExtractingDoFn())
                    .with_output_types(unicode))
      | 'pair_with_one' &amp;gt;&amp;gt; beam.Map(lambda x: (x, 1))
      | 'group' &amp;gt;&amp;gt; beam.GroupByKey()
      | 'count' &amp;gt;&amp;gt; beam.Map(lambda (word, ones): (word, sum(ones)))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;launching-the-dataflow-pipeline-periodically-using-a-cron-job&quot;&gt;Launching the Dataflow pipeline periodically using a cron job&lt;/h2&gt;

&lt;p&gt;In the example app, we want to launch a pipeline job every few hours, where each job analyzes the tweets from the past few days, providing a ‘moving window’ of analysis.
So, it makes sense to just set things up as an app &lt;a href=&quot;https://cloud.google.com/appengine/docs/flexible/python/scheduling-jobs-with-cron-yaml&quot;&gt;cron&lt;/a&gt; job, which looks like this (&lt;code&gt;backend&lt;/code&gt; is the name of the app service that handles this request, and the &lt;code&gt;url&lt;/code&gt; is the handler that launches the job):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;cron:
- description: launch dataflow pipeline
  url: /launchpipeline
  schedule: every 5 hours
  target: backend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A pipeline job could of course be triggered by other means as well– e.g. as part of handling a client request to the app, or perhaps via a &lt;a href=&quot;https://cloud.google.com/appengine/docs/standard/python/taskqueue/push/&quot;&gt;Task Queue task&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-look-at-the-example-results-in-bigquery&quot;&gt;A look at the example results in BigQuery&lt;/h2&gt;

&lt;p&gt;Once our example app is up and running, it periodically runs a Dataflow job that writes the results of its analysis to BigQuery.  (It would be just as easy to write results to the Datastore if that makes more sense for your workflow – or to write to multiple sources).&lt;/p&gt;

&lt;p&gt;With BigQuery, it is easy to run some fun queries on the data. 
For example, we can find recent word co-occurrences that are ‘interesting’ by our metric:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq3.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq3.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Or look for emerging word pairs, that have become ‘interesting’ in the last day or so (as of early April 2017):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq4.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_twitter_bq4.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can contrast the ‘interesting’ word pairs with the words that are simply the most popular within a given period (you can see that most of these words are common, but not particularly newsworthy):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_wc1.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_wc1.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Or, find the most often-tweeted URLs from the past few days (some URLs are truncated in the output):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_urls1.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/gae_dataflow/gae_dataflow_urls1.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-and-whats-next&quot;&gt;Summary… and what’s next?&lt;/h2&gt;

&lt;p&gt;In this post, we’ve looked at how you can programmatically launch Dataflow pipelines — that read from Datastore — directly from your App Engine app.
See the example app’s &lt;a href=&quot;https://github.com/amygdala/gae-dataflow/blob/master/sdk_launch/README.md&quot;&gt;README&lt;/a&gt; for more detail on how to configure and run the app yourself.&lt;/p&gt;

&lt;p&gt;Dataflow’s expressive programming model makes it easy to build and support a wide range of scalable processing and analytics tasks.
We hope you find the example app useful as a starting point towards defining new pipelines and running your own analytics from your App Engine apps.
We look forward to hearing more about what you build!&lt;/p&gt;

</description>
        <pubDate>Fri, 14 Apr 2017 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/dataflow/app_engine/2017/04/14/gae_dataflow.html</guid>
        
        <category>dataflow</category>
        
        <category>gae</category>
        
        
        <category>Dataflow</category>
        
        <category>App_Engine</category>
        
      </item>
    
      <item>
        <title>Learning and using your own image classifications</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transfer-learning-building-your-own-image-classifier&quot;&gt;Transfer learning: building your own image classifier&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#an-easy-way-to-use-your-trained-image-classifier&quot;&gt;An easy way to use your trained image classifier&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#an-example-app-would-you-hug-that&quot;&gt;An example app: “would you hug that?”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#exporting-your-trained-model-to-cloud-ml&quot;&gt;Exporting your trained model to Cloud ML&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#using-the-hugs-classifier-for-prediction-with-the-cloud-ml-api&quot;&gt;Using the “hugs classifier” for prediction with the Cloud ML API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bonus-hedgehogs-vs-dandelions&quot;&gt;Bonus: Hedgehogs vs Dandelions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-next&quot;&gt;What Next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/vision/&quot;&gt;Google Vision API&lt;/a&gt; is great for identifying labels, or categories, for a given
image. However, sometimes you want to further classify your own images, for more specialized categories that the Google
Vision API hasn’t been trained on.  E.g., maybe you’re a birdwatcher and want to recognize rare species of birds that the Vision API doesn’t do well at discriminating.  Maybe you’re a cell biologist, and want to try to automatically classify your slides.&lt;/p&gt;

&lt;p&gt;It turns out that it can be pretty straightforward to build your own neural net model to do this, via &lt;em&gt;transfer
learning&lt;/em&gt; –  bootstrapping an existing image classification model to reduce the effort needed to learn something new.&lt;/p&gt;

&lt;p&gt;In this post, we’ll take a look at an example that does that.&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning-building-your-own-image-classifier&quot;&gt;Transfer learning: building your own image classifier&lt;/h2&gt;

&lt;p&gt;One such deep neural net model is the &lt;a href=&quot;http://arxiv.org/abs/1512.00567&quot;&gt;Inception&lt;/a&gt; architecture, built using &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;, a machine learning framework open-sourced by Google.
Google has also open-sourced the &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/inception&quot;&gt;Inception v3&lt;/a&gt; model, trained to classify images against 1000 different &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt; categories.  We can use its penultimate “bottleneck” layer to train a new top layer that can recognize other classes of images: your own classes.
We’ll see that our new top layer does not need to be very complex, and that we typically don’t need much data or much training of this new model, to get good results for our new image classifications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image-classification-3-1.png&quot; alt=&quot;Transfer learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There have been some great posts showing how you can &lt;a href=&quot;https://petewarden.com/2016/02/28/tensorflow-for-poets/&quot;&gt;train this new ‘top layer’ model with TensorFlow&lt;/a&gt;, and how to do this training on &lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/12/how-to-train-and-classify-images-using-google-cloud-machine-learning-and-cloud-dataflow&quot;&gt;Google Cloud Machine Learning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;an-easy-way-to-use-your-trained-image-classifier&quot;&gt;An easy way to use your trained image classifier&lt;/h2&gt;

&lt;p&gt;In this post, we’ll focus more on the next step: After you’ve trained your model, and can classify your own images, you’ll want to be able to use the model for inference (prediction). That is, given a new image, you’d like to ask: which of your categories does it fall into?&lt;/p&gt;

&lt;p&gt;You’ll often need to have these prediction capabilities scale up. Perhaps you’ve built a mobile app to (say) recognize bird species, by using your trained ML model for online species prediction. If the app becomes wildly popular, you don’t want it to stop working because it’s getting too much traffic.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/ml/&quot;&gt;Google Cloud Machine Learning&lt;/a&gt; (Cloud ML) is an easy way to do this: in addition to supporting distributed model training, Cloud ML lets you scalably serve your online predictions after the model is trained.&lt;/p&gt;

&lt;p&gt;One way to access the Cloud ML online prediction service is to use the &lt;a href=&quot;https://developers.google.com/discovery/libraries&quot;&gt;Google API Client Libraries&lt;/a&gt; to access the &lt;a href=&quot;https://cloud.google.com/ml/reference/rest/&quot;&gt;Cloud ML API&lt;/a&gt;.
That means that it is quite straightforward to build an app that classifies images according to your categories, and scales up without your needing to do anything.&lt;/p&gt;

&lt;h2 id=&quot;an-example-app-would-you-hug-that&quot;&gt;An example app: “would you hug that?”&lt;/h2&gt;

&lt;p&gt;Let’s look at an example with a … kind of goofy data set.  The code used to train this model and make this app is
&lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/tree/master/workshop_sections/transfer_learning/cloudml&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(Note: This
post is not really a tutorial – the
&lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/README.md&quot;&gt;README&lt;/a&gt; in the repo walks you through the process of building the example in more detail.  If you want to give it a try, make sure you’ve done all the
&lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/INSTALL.md&quot;&gt;necessary setup&lt;/a&gt; first).&lt;/p&gt;

&lt;p&gt;Suppose we have a set of training images that have been labeled according to two categories: “things you might want to hug”, and “things you would not want to hug”. The ‘huggable’ dataset includes images of things like puppies and kittens.  The non-huggable dataset includes images of things with sharp edges, etc.
Now, we want to build a web app that we can upload images to, and have the app tell us whether or not the object is something “huggable”.&lt;/p&gt;

&lt;p&gt;We want our web app to work like this: If you upload an image to the app:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/yarn_octopus.png&quot; width=&quot;500&quot; alt=&quot;A yarn octopus&quot; /&gt;&lt;/p&gt;

&lt;p&gt;…the app uses the trained model to get the predicted category of the image (“hugs”? “no hugs”?).  Then the app will use the response to display the results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/yarn_octopus_score.png&quot; width=&quot;600&quot; alt=&quot;The yarn octopus is scored as huggable&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thanks to &lt;a href=&quot;https://twitter.com/juliaferraioli&quot;&gt;Julia Ferraioli&lt;/a&gt; for the idea, the “hugs/no-hugs” dataset and an earlier version of the prediction web app.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We’ll train our “hugs/no-hugs” model on Cloud ML, and use the Cloud ML API to make it easy to build the prediction web app.&lt;/p&gt;

&lt;p&gt;To do this, we first need to do some &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/hugs_preproc.sh&quot;&gt;image preprocessing&lt;/a&gt;, to extract the ‘bottleneck layer’ information (the &lt;em&gt;embeds&lt;/em&gt;) from the Inception v3 model, for each image in our dataset. These embeds will form the input data for the new ‘top layer’ model that we will train.
We will use &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt; (&lt;a href=&quot;https://beam.apache.org/&quot;&gt;Apache Beam&lt;/a&gt;)
to do this preprocessing – that is, the Beam pipeline uses the Inception v3 model to generate the inputs. We’ll save those preprocessing results in the cloud, in &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; (GCS), as &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/python_io/&quot;&gt;TFRecords&lt;/a&gt;, for easy consumption by the training process.&lt;/p&gt;

&lt;p&gt;Then, we’re ready to &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/hugs_train.sh&quot;&gt;train the model&lt;/a&gt;. Follow the code link to see the specifics.
(If you follow the blog post links above, you’ll also find other examples that train using a different “flowers” dataset.)&lt;/p&gt;

&lt;p&gt;Once the model is trained, we can use it to classify new images.  If we deploy the trained model to Cloud ML, we can make prediction requests using the Cloud ML API scalably, without any other setup.  This is great, since it makes our web app very easy to write.&lt;/p&gt;

&lt;h2 id=&quot;exporting-your-trained-model-to-cloud-ml&quot;&gt;Exporting your trained model to Cloud ML&lt;/h2&gt;

&lt;p&gt;To be able to &lt;a href=&quot;https://cloud.google.com/ml/docs/how-tos/getting-predictions&quot;&gt;use a trained model for prediction&lt;/a&gt;, you will need to &lt;a href=&quot;https://cloud.google.com/ml/docs/how-tos/preparing-models#adding_input_and_output_collections_to_the_graph&quot;&gt;add input and output collections&lt;/a&gt; to your model graph.  This gives Cloud ML the necessary ‘hooks’ into your deployed model graph for running predictions and returning the results.  See &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/trainer/model.py#L343&quot;&gt;this model prediction graph definition&lt;/a&gt; for an example. You’ll also need to make sure you export (save) your model as described &lt;a href=&quot;https://cloud.google.com/ml/docs/how-tos/preparing-models#exporting_saving_the_final_model&quot;&gt;here&lt;/a&gt;, including exporting the &lt;a href=&quot;https://www.tensorflow.org/versions/r0.11/how_tos/meta_graph/&quot;&gt;MetaGraph&lt;/a&gt;.  See &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/trainer/model.py#L365&quot;&gt;this method&lt;/a&gt; for example export code.&lt;/p&gt;

&lt;p&gt;Once you’ve done that, you can deploy a trained model to Cloud ML like this.  First, “create the model” in Cloud ML – this does not define the actual model, but creates a name to associate with the model once you upload it. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;gcloud beta ml models create hugs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/model.sh&quot;&gt;deploy a &lt;em&gt;version&lt;/em&gt; of that model&lt;/a&gt; by pointing Cloud ML to the checkpointed model info that was saved as the final result of your training session.
The command to do that will look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;gcloud beta ml versions create v1 \
  --model hugs \
  --origin gs://your-gcs-bucket/path/to/model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It will take a few minutes to create the model version. Once that’s done, you can start to access it for prediction in your apps.  Each time you deploy a new version of your model, you will use a new version name.  If you have more than one version of a model, you can set one as the default.&lt;/p&gt;

&lt;h2 id=&quot;using-the-hugs-classifier-for-prediction-with-the-cloud-ml-api&quot;&gt;Using the “hugs classifier” for prediction with the Cloud ML API&lt;/h2&gt;

&lt;p&gt;Once your model is deployed, there are various ways to access it.  One easy way for initial testing is just to &lt;a href=&quot;https://cloud.google.com/ml/docs/quickstarts/prediction#use_the_online_prediction_service&quot;&gt;use the gcloud sdk from the command line&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But, for the web app, we’ll use the &lt;a href=&quot;https://developers.google.com/api-client-library/python/&quot;&gt;Google API client libs&lt;/a&gt;
instead, to call the &lt;a href=&quot;https://cloud.google.com/ml/reference/rest/v1beta1/projects/predict&quot;&gt;Cloud ML API web service&lt;/a&gt;.
So we just need to upload an image to the app; format the image data for input to our model; then just make the API
call&lt;sup&gt;&lt;a href=&quot;#footnote1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and display the response.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/motherboard.jpeg&quot; alt=&quot;a computer motherboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, if we upload this photo of a motherboard…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/motherboard_score.png&quot; width=&quot;600&quot; alt=&quot;a motherboard is scored as not huggable&quot; /&gt;&lt;/p&gt;

&lt;p&gt;… it’s judged as “not huggable”.&lt;/p&gt;

&lt;p&gt;Here is a Python code snippet showing a prediction API call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from googleapiclient import discovery
from oauth2client.client import GoogleCredentials

def create_client():

  credentials = GoogleCredentials.get_application_default()
  ml_service = discovery.build(
      'ml', 'v1beta1', credentials=credentials)
  return ml_service


def get_prediction(ml_service, project, model_name, input_image):
  request_dict = make_request_json(input_image)
  body = {'instances': [request_dict]}

  # This request will use the default model version.
  parent = 'projects/{}/models/{}'.format(project, model_name)
  request = ml_service.projects().predict(name=parent, body=body)
  result = request.execute()
  return result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see more detail on how the uploaded image was formatted for the API request, and how the response was 
parsed, &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/web_server/predict_server.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A quokka, in contrast, is (very rightly) judged as “huggable”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/quokka_score.png&quot; width=&quot;600&quot; alt=&quot;a quokka is scored as huggable&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bonus-hedgehogs-vs-dandelions&quot;&gt;Bonus: Hedgehogs vs Dandelions&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/hedgehog.jpg&quot; width=&quot;400&quot; alt=&quot;A hedgehog&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When I was experimenting with learning both the “hugs/no-hugs” and &lt;a href=&quot;https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/transfer_learning/cloudml/flowers_preproc.sh&quot;&gt;“flowers”&lt;/a&gt; classification models, I learned something funny&lt;sup&gt;&lt;a href=&quot;#footnote2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.  I accidentally fed the “flowers” model an image of a hedgehog.  The hedgehog was meant for the “hugs/no-hugs” model, which will reasonably classify it as “don’t hug”.&lt;/p&gt;

&lt;p&gt;It turns out that if you ask the “flowers” model what kind of flower a hedgehog is, it will classify it pretty confidently as looking most like a dandelion. This seems pretty astute of the flowers model!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;$ python images_to_json.py hedgehog.jpg
$ gcloud beta ml predict --model flowers --json-instances request.json
KEY                             PREDICTION  SCORES
prediction_images/hedgehog.jpg  1           [0.006916556041687727, 0.9633635878562927, 0.0015918412245810032, 0.005548111163079739, 0.022190041840076447, 0.0003897929273080081]
$ gsutil cat gs://cloud-ml-data/img/flower_photos/dict.txt
daisy
dandelion
roses
sunflowers
tulips
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(We can see this is so because &lt;code&gt;dandelion&lt;/code&gt; is in the ‘1’ index position in the 
flower dataset’s &lt;code&gt;dict.txt&lt;/code&gt;, corresponding to the prediction of ‘1’.)&lt;/p&gt;

&lt;h2 id=&quot;what-next&quot;&gt;What Next?&lt;/h2&gt;

&lt;p&gt;If you’re interested to learn more about TensorFlow and CloudML, there are many examples and tutorials on the &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow site&lt;/a&gt;.  The &lt;a href=&quot;https://cloud.google.com/ml/docs/&quot;&gt;Cloud ML docs&lt;/a&gt; go into much more detail on how to train and serve a model, including Cloud ML’s support for distributed training and hyperparamter tuning.&lt;/p&gt;

&lt;p&gt;(You might also be interested in the upcoming &lt;a href=&quot;https://cloudnext.withgoogle.com/&quot;&gt;Google Cloud Next&lt;/a&gt; event, where you can hear much more about what Google is doing in the Big Data &amp;amp; Machine Learning area.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small&gt;
&lt;span id=&quot;footnote1&quot;&gt;1&lt;/span&gt;: While the prediction service is in alpha, you may sometimes see a &lt;code&gt;502&lt;/code&gt; error response if you make a request after not having used the service for a while.  If you see this, just resubmit. This will not be an issue once the service moves out of alpha.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;
&lt;span id=&quot;footnote2&quot;&gt;2&lt;/span&gt;: It’s possible that this amuses only me.&lt;/small&gt;&lt;/p&gt;

&lt;h4 id=&quot;image-credits&quot;&gt;Image credits:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://goo.gl/images/zdGnN9&quot;&gt;https://goo.gl/images/zdGnN9&lt;/a&gt; - yarn octopus&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://goo.gl/images/LmaVMu&quot;&gt;https://goo.gl/images/LmaVMu&lt;/a&gt; - motherboard&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://goo.gl/images/XTF5h0&quot;&gt;https://goo.gl/images/XTF5h0&lt;/a&gt; - quokka&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://goo.gl/images/XTF5h0&quot;&gt;https://goo.gl/images/yuFM1C&lt;/a&gt; - hedgehog&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 03 Feb 2017 00:00:00 -0800</pubDate>
        <link>http://amygdala.github.io/ml/2017/02/03/transfer_learning.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/ml/2017/02/03/transfer_learning.html</guid>
        
        <category>machine_learning</category>
        
        <category>cloud_ml</category>
        
        
        <category>ML</category>
        
      </item>
    
      <item>
        <title>Building a Slackbot that uses the Google Cloud ML Natural Language API (and runs on Kubernetes)</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Many of us belong to some &lt;a href=&quot;https://slack.com/&quot;&gt;Slack&lt;/a&gt; communities.
Slack has an API that makes it easy to add ‘&lt;a href=&quot;https://slackhq.com/a-beginner-s-guide-to-your-first-bot-97e5b0b7843d#.iyzgmbaf0&quot;&gt;bots&lt;/a&gt;’ to a channel, that do interesting things with the posted content, and that can interact with the people on the channel.
Google’s &lt;a href=&quot;cloud.google.com/products/machine-learning/&quot;&gt;Cloud Machine Learning APIs&lt;/a&gt; make it easy to build bots with interesting and fun capabilities.&lt;/p&gt;

&lt;p&gt;Here, we’ll describe how to build such a bot, one that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;uses the &lt;a href=&quot;https://cloud.google.com/natural-language/&quot;&gt;Google Cloud ML Natural Language API&lt;/a&gt; to analyze channel content,&lt;/li&gt;
  &lt;li&gt;runs on &lt;a href=&quot;http://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; to make it easy to deploy, and&lt;/li&gt;
  &lt;li&gt;uses the &lt;a href=&quot;https://github.com/howdyai/botkit&quot;&gt;Botkit&lt;/a&gt; library to make it easy to interact with Slack.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code for this slackbot is &lt;a href=&quot;https://github.com/GoogleCloudPlatform/nodejs-docs-samples/tree/master/language/slackbot&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;using-the-cloud-ml-natural-language-api-in-a-bot&quot;&gt;Using the Cloud ML Natural Language API in a bot&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/natural-language/&quot;&gt;Google Cloud Natural Language API&lt;/a&gt;  helps reveal the structure and meaning of text by offering powerful machine learning models for multiple languages– currently, English, Spanish, and Japanese.&lt;/p&gt;

&lt;p&gt;You can use the NL API to do &lt;strong&gt;entity recognition&lt;/strong&gt; (identifying entities and label by types such as person, organization, location, events, products and media), &lt;strong&gt;sentiment analysis&lt;/strong&gt; (understanding the overall sentiment expressed in a block of text), and &lt;strong&gt;syntax analysis&lt;/strong&gt; (sentence extraction and tokenization, identifying parts of speech, creating parse trees for each sentence, and more).&lt;/p&gt;

&lt;p&gt;Our Natural Language (NL) slackbot uses the Google Cloud NL API in two different ways.&lt;/p&gt;

&lt;h3 id=&quot;entity-detection&quot;&gt;Entity detection&lt;/h3&gt;

&lt;p&gt;First, it uses the NL API’s  &lt;a href=&quot;https://cloud.google.com/natural-language/docs/basics&quot;&gt;&lt;strong&gt;entity detection&lt;/strong&gt;&lt;/a&gt; to track the most common topics that are being discussed over time in a channel.
It does this by detecting entities in each posted message, and recording them in a database.
Then, at any time the participants in the channel can query the NL slackbot to ask it for the top N entities/topics discussed in the channel (by default, over the past week).&lt;/p&gt;

&lt;h3 id=&quot;sentiment-analysis&quot;&gt;Sentiment analysis&lt;/h3&gt;

&lt;p&gt;Additionally, the NL slackbot uses the NL API to assess
the &lt;a href=&quot;https://cloud.google.com/natural-language/docs/basics&quot;&gt;&lt;strong&gt;sentiment&lt;/strong&gt;&lt;/a&gt; of any message posted to
the channel, and if the positive or negative magnitude of the statement is
sufficiently large, it sends a ‘thumbs up’ or ‘thumbs down’ to the channel in reaction.&lt;/p&gt;

&lt;h2 id=&quot;running-the-slackbot-as-a-kubernetes-app&quot;&gt;Running the Slackbot as a Kubernetes App&lt;/h2&gt;

&lt;p&gt;Our slackbot uses &lt;a href=&quot;https://cloud.google.com/container-engine/&quot;&gt;Google Container
Engine&lt;/a&gt;, a hosted version of
&lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;, to run the bot.  This is a convenient way to launch the bot in the cloud, so that there is no need to manage it locally, and to ensure that it stays running.
It also uses &lt;a href=&quot;https://cloud.google.com/container-registry/&quot;&gt;Google Container Registry&lt;/a&gt; to store a &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; image
for the bot.&lt;/p&gt;

&lt;p&gt;It’s useful to have the bot running in the cloud, and on a Kubernetes cluster.  While you could alternately just set it up on a VM somewhere, Kubernetes will ensure that it stays running.
If the &lt;a href=&quot;http://kubernetes.io/docs/user-guide/pods/&quot;&gt;&lt;em&gt;pod&lt;/em&gt;&lt;/a&gt; in the slackbot’s &lt;a href=&quot;http://kubernetes.io/docs/user-guide/deployments/&quot;&gt;&lt;em&gt;Deployment&lt;/em&gt;&lt;/a&gt; goes down for some reason, Kubernetes will restart it.&lt;/p&gt;

&lt;h2 id=&quot;starting-up-the-nl-slackbot&quot;&gt;Starting up the NL Slackbot&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/master/language/slackbot/README.md&quot;&gt;README&lt;/a&gt;  in the GitHub repo walks you through the process of starting up and running the slackbot.  As part of the process you’ll also
create a &lt;a href=&quot;https://api.slack.com/bot-users&quot;&gt;Slack bot user&lt;/a&gt; and get an authentication token.&lt;/p&gt;

&lt;p&gt;If you think you want to run your slackbot for a while, follow the instructions in &lt;a href=&quot;https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/master/language/slackbot/README.md#optional-create-a-slackbot-app-that-uses-persistent-storage&quot;&gt;the README section on setting up a Persistent Disk&lt;/a&gt; for the bot’s database.  That will allow the bot to be restarted without losing data.
The README also walks you through how you can test your bot locally before deploying to Kubernetes if you want.&lt;/p&gt;

&lt;p&gt;Once it’s running, invite the bot to a Slack channel.&lt;/p&gt;

&lt;h2 id=&quot;the-nl-slackbot-in-action&quot;&gt;The NL Slackbot in Action&lt;/h2&gt;

&lt;p&gt;Once your NL slackbot is running, and you’ve invited it to a channel, everyone in the channel can start to interact with it.
For the most part, the NL slackbot  will keep pretty quiet. Each time there is a post, the NL slackbot will analyze the &lt;strong&gt;entities&lt;/strong&gt; in that text. It will store those entities in a database.&lt;/p&gt;

&lt;p&gt;It will also analyze the &lt;strong&gt;sentiment&lt;/strong&gt; of the post (understanding the overall sentiment expressed in a block of text).  If the magnitude of the sentiment is greater than a certain threshold, either positive or negative, the bot will respond with a ‘thumbs up’ or ‘thumbs down’.  It doesn’t respond to all posts, only those for which the sentiment magnitude is above the threshold.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nl_coffee_bananas_sh.png&quot; alt=&quot;Analyzing sentiment&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(If you think that this aspect of the bot is a bit annoying, it is easy to disable, or change the threshold, by looking for where it is set &lt;a href=&quot;https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/master/language/slackbot/demo_bot.js&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;At any time, you can directly ask the bot for the top entities that it has detected in channel conversation (by default, the top 20 entities over the past week).  You do this by addressing the bot with the words &lt;code&gt;top entities&lt;/code&gt;.
For example, if your bot is called &lt;code&gt;@nlpbot&lt;/code&gt;, you would ask it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@nlpbot top entities
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, the result might look something like the following. (At least, if you have seeded your test channel with a combination of political news and Kubernetes posts :).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nl_slackbot_ents2_sh.png&quot; alt=&quot;Asking the bot for top entities&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-next&quot;&gt;What Next?&lt;/h2&gt;

&lt;p&gt;There are many ways that this bot could be developed further and be made more sophisticated.&lt;/p&gt;

&lt;p&gt;As just one example, you could integrate the other Cloud ML APIs as well.  You might add a capability that leverages the &lt;a href=&quot;https://cloud.google.com/vision/&quot;&gt;Cloud Vision API&lt;/a&gt; to analyze the images that people post to the channel.  Then, each time someone posted a meme image to the channel, you could use the Vision API to do OCR on the image, then pass that info to the NL API.&lt;/p&gt;

&lt;p&gt;You could also extend the NL slackbot to support more sophisticated queries on the entity database – e.g., “show me the top N PERSONS” or “top N LOCATIONS today”.
Or, you could include the wiki URLs in the results for any entities that have them. This information is currently being collected, but not displayed. That might look something like the following.  Note that “Trump” and “Donald Trump” are detected as referring to the same PERSON.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nl_slackbot_wiki_sh.png&quot; alt=&quot;Including wiki urls in entity information&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 14 Jan 2017 00:00:00 -0800</pubDate>
        <link>http://amygdala.github.io/ml/2017/01/14/nlapi.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/ml/2017/01/14/nlapi.html</guid>
        
        <category>machine_learning</category>
        
        <category>natural_language_processing</category>
        
        <category>natural_langage_api</category>
        
        <category>kubernetes</category>
        
        
        <category>ML</category>
        
      </item>
    
      <item>
        <title>Using the Cloud Vision API with Twilio Messaging on Kubernetes</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/vision/&quot;&gt;Google Cloud Vision API&lt;/a&gt; has just moved to GA (General Availability) status.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/cat_and_laptop.jpg&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Vision API lets you create applications that can classify images into thousands of categories (e.g., “sailboat”, “lion”… or &lt;strong&gt;cat&lt;/strong&gt; and &lt;strong&gt;laptop&lt;/strong&gt; as above); can detect faces and other objects in images (including predicting “sentiment”); can perform OCR (detection of text in images); can detect landmarks (like the Eiffel Tower); can detect logos; and can moderate for offensive content.
You can &lt;a href=&quot;https://www.youtube.com/watch?v=ud2Ipnq0pTU&quot;&gt;see Jeff Dean demoing the Vision API in this video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-vision&quot;&gt;This github repo&lt;/a&gt; has a number of Vision API examples, written in different languages, and showing off different aspects of the Cloud Vision API. Some of the examples are simple scripts, and others are a bit more complex.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python/twilio/twilio-k8s&quot;&gt;new example called ‘twilio-k8s’&lt;/a&gt; has just been added to the repo. It shows
how to run the
&lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python/twilio/twilio-labels&quot;&gt;“What’s That?” app&lt;/a&gt; (built by &lt;a href=&quot;http://www.blog.juliaferraioli.com/2016/02/exploring-world-using-vision-twilio.html&quot;&gt;Julia
Ferraioli&lt;/a&gt;), on
&lt;a href=&quot;http://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The app uses &lt;a href=&quot;https://www.twilio.com&quot;&gt;Twilio&lt;/a&gt; to allow images to be texted to a given number,
then uses the &lt;a href=&quot;https://cloud.google.com/vision/&quot;&gt;Cloud Vision API&lt;/a&gt; to find labels in the image
(classify what’s in the image) and return the detected labels as a reply text.
Because the app is running on Kubernetes, it’s easy to &lt;strong&gt;scale up the app&lt;/strong&gt; to support a large number
of requests.&lt;/p&gt;

&lt;p&gt;Once you’ve set up the app on your Google Container Engine (or Kubernetes) cluster, and set up your Twilio number, you can text images to get content labelings:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/yard.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/yard.jpg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It’s easy to modify your own version of the app to look for additional information in the image.  I’ve in fact modifed my version of the app to look for &lt;em&gt;logos&lt;/em&gt; too. It’s fun to see how well it can do with an incomplete view of a logo:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amy-jo.storage.googleapis.com/images/cl_bar.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://amy-jo.storage.googleapis.com/images/cl_bar.png&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After you’ve set up your app, when you’re ready to share it, you can scale up the number of servers running on your Kubernetes cluster so that the app stays responsive.  The example’s &lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-vision/blob/master/python/twilio/twilio-k8s/README.md&quot;&gt;README&lt;/a&gt; goes into more detail about how to do all of this.&lt;/p&gt;

&lt;p&gt;For a relatively short time, you can try it out here: (785) 336-5113.&lt;br /&gt;
This number won’t work indefinitely, though :).&lt;/p&gt;

</description>
        <pubDate>Fri, 22 Apr 2016 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/ml/2016/04/22/vision-api.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/ml/2016/04/22/vision-api.html</guid>
        
        <category>vision_api</category>
        
        <category>machine_learning</category>
        
        
        <category>ML</category>
        
      </item>
    
      <item>
        <title>Cloud Dataflow News</title>
        <description>&lt;p&gt;There’s been a lot happening with &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Google Cloud Dataflow&lt;/a&gt; lately.&lt;/p&gt;

&lt;p&gt;We are pleased to announce the recent induction of the &lt;a href=&quot;https://cloud.google.com/dataflow/what-is-google-cloud-dataflow#Sdks&quot;&gt;Google Cloud Dataflow SDK&lt;/a&gt; (and corresponding runners for &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Apache Flink&lt;/a&gt; and &lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;) into the new &lt;a href=&quot;http://beam.incubator.apache.org/&quot;&gt;Apache Beam incubator project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;http://oreilly.com/ideas/the-world-beyond-batch-streaming-102&quot;&gt;‘Streaming 102’&lt;/a&gt; article was published by O’Reilly , following 
&lt;a href=&quot;http://oreilly.com/ideas/the-world-beyond-batch-streaming-101&quot;&gt;‘Streaming 101’&lt;/a&gt;.  These articles provide a great overview of design and implementation considerations in stream data analysis.&lt;/p&gt;

&lt;p&gt;We’ve also recently written an article that &lt;a href=&quot;https://cloud.google.com/dataflow/blog/dataflow-beam-and-spark-comparison&quot;&gt;compares the programming models of Dataflow and Spark as they exist today&lt;/a&gt;, based on a mobile ‘gaming’ scenario, involving the evolution of a pipeline from a simple batch use case to more sophisticated streaming use cases, with side-by-side code snippets contrasting the two.
The article uses a suite of ‘gaming’ example pipelines that can be found in the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/DataflowJavaSDK/tree/master/examples/src/main/java8/com/google/cloud/dataflow/examples/complete/game&quot;&gt;Dataflow github repo&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Mon, 15 Feb 2016 00:00:00 -0800</pubDate>
        <link>http://amygdala.github.io/dataflow/2016/02/15/dataflow.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/dataflow/2016/02/15/dataflow.html</guid>
        
        <category>apache_beam</category>
        
        <category>big_data</category>
        
        <category>data_analysis</category>
        
        
        <category>Dataflow</category>
        
      </item>
    
      <item>
        <title>New sample repos for Symfony and Laravel with the Google App Engine PHP Runtime</title>
        <description>&lt;p&gt;After the recent &lt;a href=&quot;http://amygdala.github.io/gae/php/2015/03/09/gaephp.html&quot;&gt;updates to the Google App Engine PHP Runtime&lt;/a&gt;, we are creating GAE forks for some of the popular PHP framework starter apps.  These forks contain the modifications for running these apps on App Engine.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/symfony-standard&quot;&gt;This repository&lt;/a&gt; contains a modified Symfony Standard Edition Starter app for Google App Engine.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/laravel&quot;&gt;This repository&lt;/a&gt; contains a modified Laravel Starter app for Google App Engine.&lt;/p&gt;

&lt;p&gt;To get started with either one, see the README in the repo.&lt;/p&gt;

&lt;p&gt;You’ll want to first create a Google Cloud project if you don’t already have one. You can do that from &lt;a href=&quot;https://console.developers.google.com/start/appengine&quot;&gt;this page&lt;/a&gt; if you like.  (Note that there is a &lt;a href=&quot;https://console.developers.google.com/billing/freetrial&quot;&gt;free trial&lt;/a&gt; available).&lt;/p&gt;

&lt;p&gt;Then, if you’re not familiar with PHP on App Engine, you might want to follow the &lt;a href=&quot;https://cloud.google.com/appengine/docs/php/gettingstarted/introduction&quot;&gt;PHP Tutorial&lt;/a&gt; first.&lt;/p&gt;

&lt;p&gt;We’re looking forward to your feedback (and/or pull requests)!&lt;/p&gt;

</description>
        <pubDate>Wed, 20 May 2015 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/gae/php/2015/05/20/gaephp.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/gae/php/2015/05/20/gaephp.html</guid>
        
        <category>php</category>
        
        <category>gae</category>
        
        
        <category>gae</category>
        
        <category>php</category>
        
      </item>
    
      <item>
        <title>Real-time analysis of Twitter data using Kubernetes, PubSub and BigQuery</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/pubsub/overview&quot;&gt;Google Cloud &lt;strong&gt;PubSub&lt;/strong&gt;&lt;/a&gt; provides many-to-many, asynchronous messaging that decouples senders and receivers. It allows for secure and highly available communication between independently written applications and delivers low-latency, durable messaging.
It has just gone to Beta, and is available for anyone to try.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes-bigquery-python/tree/master/pubsub&quot;&gt;This example&lt;/a&gt; &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes&quot;&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/a&gt; app shows how to build a ‘pipeline’ to stream Twitter data into &lt;a href=&quot;https://cloud.google.com/bigquery/what-is-bigquery&quot;&gt;&lt;strong&gt;BigQuery&lt;/strong&gt;&lt;/a&gt; using &lt;a href=&quot;https://cloud.google.com/pubsub/docs&quot;&gt;PubSub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The app uses uses PubSub to buffer the data coming in from Twitter and to decouple ingestion from processing.
One of the Kubernetes app &lt;strong&gt;&lt;em&gt;pods&lt;/em&gt;&lt;/strong&gt; reads the data from Twitter and publishes it to a PubSub topic.  Other pods subscribe to the PubSub topic, grab data in small batches, and stream it into BigQuery.  The figure below suggests this flow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/k8s_pubsub_tw_bq.png&quot; width=&quot;600&quot; alt=&quot;Architecture of app&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This app can be thought of as a ‘workflow’ type of app– it doesn’t have a web front end (though Kubernetes is great for those types of apps as well).
Instead, it is designed to continously run a scalable data ingestion pipeline.&lt;/p&gt;

&lt;p&gt;Find the code and more detail &lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes-bigquery-python/tree/master/pubsub&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Wed, 11 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/kubernetes/2015/03/11/pubsub.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/kubernetes/2015/03/11/pubsub.html</guid>
        
        <category>kubernetes</category>
        
        <category>pubsub</category>
        
        <category>bigquery</category>
        
        <category>twitter</category>
        
        
        <category>kubernetes</category>
        
      </item>
    
      <item>
        <title>Updates to the Google App Engine PHP Runtime</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://code.google.com/p/googleappengine/wiki/SdkReleaseNotes&quot;&gt;1.9.18 Google App Engine release&lt;/a&gt; has added new capabilities to the &lt;a href=&quot;https://cloud.google.com/appengine/docs/php&quot;&gt;App Engine PHP runtime&lt;/a&gt;. 
It’s now possible to &lt;a href=&quot;https://cloud.google.com/appengine/docs/php/#PHP_Selecting_the_PHP_runtime&quot;&gt;select PHP 5.5&lt;/a&gt; as your runtime, and a number of useful new features are supported if you do so.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;cURL&lt;/strong&gt; extension is now &lt;a href=&quot;https://cloud.google.com/appengine/docs/php/#PHP_Enabled_extensions&quot;&gt;supported&lt;/a&gt;.
We’ve also provided a &lt;a href=&quot;https://cloud.google.com/appengine/docs/php/config/php_ini#GAE_directives&quot;&gt;cURL implementation using the standard HTTP streams API&lt;/a&gt; for apps that do not need the complete cURL extension.
The &lt;strong&gt;ImageMagick&lt;/strong&gt; extension is now supported for PHP 5.5 apps as well.&lt;/p&gt;

&lt;p&gt;We’ve added an in memory virtual filesystem that makes it possible to create temporary files.
In the new PHP 5.5 runtime, you can now use the &lt;strong&gt;&lt;code&gt;sys_get_temp_dir()&lt;/code&gt;, &lt;code&gt;tmpfile()&lt;/code&gt; and &lt;code&gt;tempnam()&lt;/code&gt;&lt;/strong&gt; functions to &lt;a href=&quot;https://gae-php-tips.appspot.com/2015/03/03/file-system-changes-in-app-engine-1-9-18/&quot;&gt;create temporary files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The PHP 5.5. runtime also gives you the ability to &lt;a href=&quot;https://gae-php-tips.appspot.com/2015/03/09/direct-file-uploads-for-php-5-5/&quot;&gt;&lt;strong&gt;upload files directly to your application&lt;/strong&gt;&lt;/a&gt;, without the need to upload the files to &lt;a href=&quot;https://cloud.google.com/storage/docs&quot;&gt;Google Cloud Storage&lt;/a&gt; first. Direct uploads leverages the same in-memory virtual filesystem that is used to provide temporary filesystem support. Direct uploads are only available with the PHP 5.5 runtime, and are  limited to a maximum combined size of 32MB, which is the incoming request size limit.&lt;/p&gt;

&lt;p&gt;See the linked-to posts, from the unoffical “Tips and Tricks for PHP on Google App Engine” &lt;a href=&quot;https://gae-php-tips.appspot.com&quot;&gt;blog&lt;/a&gt;, for more detail, and the App Engine &lt;a href=&quot;https://code.google.com/p/googleappengine/wiki/SdkReleaseNotes&quot;&gt;release notes&lt;/a&gt; for more detail on what has changed.&lt;/p&gt;

</description>
        <pubDate>Mon, 09 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://amygdala.github.io/gae/php/2015/03/09/gaephp.html</link>
        <guid isPermaLink="true">http://amygdala.github.io/gae/php/2015/03/09/gaephp.html</guid>
        
        <category>php</category>
        
        <category>gae</category>
        
        
        <category>gae</category>
        
        <category>php</category>
        
      </item>
    
  </channel>
</rss>
