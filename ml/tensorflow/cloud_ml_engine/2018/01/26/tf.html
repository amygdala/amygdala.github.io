<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Easy distributed training with TensorFlow using tf.estimator.train_and_evaluate and Cloud ML Engine</title>
  <meta name="description" content="Introduction">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://amygdala.github.io/ml/tensorflow/cloud_ml_engine/2018/01/26/tf.html">
  <link rel="alternate" type="application/rss+xml" title="Amy on GCP" href="http://amygdala.github.io/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Amy on GCP</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Easy distributed training with TensorFlow using tf.estimator.train_and_evaluate and Cloud ML Engine</h1>
    <p class="post-meta">Jan 26, 2018</p>
  </header>

  <article class="post-content">
    <h2 id="introduction">Introduction</h2>

<p>TensorFlow release 1.4 introduced the function <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"><strong><code class="highlighter-rouge">tf.estimator.train_and_evaluate</code></strong></a>, which simplifies training, evaluation, and exporting of <a href="https://www.tensorflow.org/get_started/estimator"><code class="highlighter-rouge">Estimator</code></a> models. It abstracts away the details of <a href="https://www.google.com/url?q=https://www.tensorflow.org/deploy/distributed">distributed execution</a> for training and evaluation, while also supporting local execution, and provides consistent behavior across both local/non-distributed and distributed configurations.</p>

<p>This means that with <strong><code class="highlighter-rouge">tf.estimator.train_and_evaluate</code></strong>, you can run the same code on both locally and distributed in the cloud, on different devices and using different cluster configurations, and get consistent results, <strong>without making any code changes</strong>. When you’re done training (or at intermediate stages), the trained model is automatically exported in a <a href="https://www.tensorflow.org/programmers_guide/saved_model">form suitable for serving</a> (e.g. for <a href="https://cloud.google.com/ml-engine/docs/prediction-overview">Cloud ML Engine online prediction</a> or <a href="https://www.tensorflow.org/serving/">TensorFlow serving</a>).</p>

<p>In this post, we’ll walk through how to use <code class="highlighter-rouge">tf.estimator.train_and_evaluate</code> with an <code class="highlighter-rouge">Estimator</code> model, and then show how easy it is to do <strong>distributed training of the model on <a href="https://cloud.google.com/ml-engine">Cloud ML Engine</a></strong>, moving between different cluster configurations with just a config tweak.
(The TensorFlow code itself supports distribution on any infrastructure (GCE, GKE, etc.) when properly configured, but we will focus on Cloud ML Engine, which makes the experience seamless).</p>

<p>The primary steps necessary to do this are:</p>

<ul>
  <li>build your <code class="highlighter-rouge">Estimator</code> model;</li>
  <li>define how data is fed into the model for both training and test datasets (often these definitions are essentially the same); and</li>
  <li>define training and eval specifications (<a href="https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec"><code class="highlighter-rouge">TrainSpec</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/EvalSpec"><code class="highlighter-rouge">EvalSpec</code></a>) to be passed to <code class="highlighter-rouge">tf.estimator.train_and_evaluate</code>.  The <code class="highlighter-rouge">EvalSpec</code> can include information on how to export your trained model for prediction (serving), and we’ll look at how to do that as well.</li>
</ul>

<p>Then we’ll look at how to <strong>use the trained model to make predictions</strong>.</p>

<p>The example also includes the use of <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><strong>Datasets</strong></a> to manage our input data. This API is part of TensorFlow 1.4, and is an <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md">easier and more performant way</a> to create input pipelines to TensorFlow models; this is particularly important with large datasets and when using accelerators.
 (See <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md">this article</a> for more on why input pipelining is so important, particularly when using accelerators).</p>

<p>For our example, we’ll use the The <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">Census Income Data
Set</a> hosted by the <a href="https://archive.ics.uci.edu/ml/datasets/">UC Irvine Machine Learning
Repository</a>. We have hosted the data
on <a href="https://cloud.google.com/storage/">Google Cloud Storage</a> (GCS) in a slightly cleaned form. We’ll use this dataset to predict income category based on various information about a person.</p>

<p>This post omits some of the details of the example.
To see the specifics and work through the code yourself, visit the <a href="http://jupyter.org/">Jupyter</a> notebook <a href="https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb">here</a>.
(The example in the <a href="https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb">notebook</a> is a slightly modified version of <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator/trainer">this example</a>).</p>

<h2 id="step-1-create-an-estimator">Step 1: Create an Estimator</h2>

<p>The TensorFlow <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">Estimator</a> class wraps a model, and provides built-in support for distributed training and evaluation. You should nearly always use Estimators to create your TensorFlow models. ‘Pre-made’ Estimator subclasses are an effective way to quickly create standard models, and you can build a <a href="https://www.tensorflow.org/extend/estimators">Custom Estimator</a> if none of the pre-made Estimators suit your purpose.</p>

<p>For this example, we’ll create an <a href="https://www.tensorflow.org/get_started/estimator">Estimator</a> object using a pre-made subclass, <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier"><code class="highlighter-rouge">DNNLinearCombinedClassifier</code></a>, which implements a <a href="https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html">“wide and deep”</a> model. Wide and deep models use a deep neural net (DNN) to learn high level abstractions about complex features or interactions between such features. These models then combine the outputs from the DNN with a <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a> performed on simpler features. This provides a balance between power and speed that is effective on many structured data problems.</p>

<p>See the accompanying <a href="https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#First-step:-create-an-Estimator">notebook</a> for the details of defining our Estimator, including specification of the expected format of the input data.
The data is in CSV format, and looks like this:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K
50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, &lt;=50K
38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, &lt;=50K
...
</code></pre>
</div>

<p>We’ll use the last field, which indicates income bracket, as our label, meaning that this is the value we’ll predict based on the values of the other fields.</p>

<p>In the <a href="https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#First-step:-create-an-Estimator">notebook</a>, we define a <code class="highlighter-rouge">build_estimator</code> function, which takes as input config info, and returns a <code class="highlighter-rouge">tf.estimator.DNNLinearCombinedClassifier</code> object.
We’ll call it like this:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">run_config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">()</span>
<span class="n">run_config</span> <span class="o">=</span> <span class="n">run_config</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">model_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">)</span>

<span class="n">FIRST_LAYER_SIZE</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c"># Number of nodes in the first layer of the DNN</span>
<span class="n">NUM_LAYERS</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c"># Number of layers in the DNN</span>
<span class="n">SCALE_FACTOR</span> <span class="o">=</span> <span class="mf">0.7</span>  <span class="c"># How quickly should the size of the layers in the DNN decay</span>
<span class="n">EMBEDDING_SIZE</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c"># Number of embedding dimensions for categorical columns</span>

<span class="n">estimator</span> <span class="o">=</span> <span class="n">build_estimator</span><span class="p">(</span>
    <span class="n">embedding_size</span><span class="o">=</span><span class="n">EMBEDDING_SIZE</span><span class="p">,</span>
    <span class="c"># Construct layers sizes with exponential decay</span>
    <span class="n">hidden_units</span><span class="o">=</span><span class="p">[</span>
        <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">FIRST_LAYER_SIZE</span> <span class="o">*</span>
                   <span class="n">SCALE_FACTOR</span><span class="o">**</span><span class="n">i</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_LAYERS</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">config</span><span class="o">=</span><span class="n">run_config</span>
<span class="p">)</span>
</code></pre>
</div>

<h2 id="step-2-define-input-functions-using-datasets">Step 2: Define input functions using Datasets</h2>

<p>Now that we have defined our model structure, the next step is to use it for training and evaluation.
As with any <code class="highlighter-rouge">Estimator</code>, we’ll need to tell the <code class="highlighter-rouge">DNNLinearCombinedClassifier</code> object how to get its training and eval data. We’ll define a function (<code class="highlighter-rouge">input_fn</code>) that knows how to generate features and labels for training or evaluation, then use that definition to create the actual train and eval input functions.</p>

<p>We’ll use <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">Datasets</a> to access our data.
This API is a new way to create <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md">input pipelines to TensorFlow models</a>.
The <code class="highlighter-rouge">Dataset</code> API is much more performant than using <code class="highlighter-rouge">feed_dict</code> or the queue-based pipelines, and it’s <a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html">cleaner and easier</a> to use.</p>

<p>In this simple example, our datasets are too small for the use of the Dataset API to make a large difference, but with larger datasets it becomes much more important.</p>

<p>The <code class="highlighter-rouge">input_fn</code> definition is below. It uses a couple of helper functions that are defined in the accompanying <a href="https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Define-input-functions-(using-Datasets)">notebook</a>. One of these,
<code class="highlighter-rouge">parse_label_column</code>, is used to convert the label strings (in our case, ‘ &lt;=50K’ and ‘ &gt;50K’) into <a href="https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding">one-hot</a> encodings, which map categorical features into a format that works better with most ML classification models.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># This function returns a (features, indices) tuple, where features is a dictionary of</span>
<span class="c"># Tensors, and indices is a single Tensor of label indices.</span>
<span class="k">def</span> <span class="nf">input_fn</span><span class="p">(</span><span class="n">filenames</span><span class="p">,</span>
                      <span class="n">num_epochs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                      <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">skip_header_lines</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                      <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>

  <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TextLineDataset</span><span class="p">(</span><span class="n">filenames</span><span class="p">)</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="n">skip_header_lines</span><span class="p">)</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="n">parse_csv</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="n">iterator</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">make_one_shot_iterator</span><span class="p">()</span>
  <span class="n">features</span> <span class="o">=</span> <span class="n">iterator</span><span class="o">.</span><span class="n">get_next</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">features</span><span class="p">,</span> <span class="n">parse_label_column</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">LABEL_COLUMN</span><span class="p">))</span>
</code></pre>
</div>

<p>Then, we’ll use <code class="highlighter-rouge">input_fn</code> to define both the <code class="highlighter-rouge">train_input</code> and <code class="highlighter-rouge">eval_input</code> functions.  We just need to pass <code class="highlighter-rouge">input_fn</code> the different source files to use for training versus evaluation.
As we’ll see below, these two functions will be used to define a <code class="highlighter-rouge">TrainSpec</code> and <code class="highlighter-rouge">EvalSpec</code> used by <code class="highlighter-rouge">train_and_evaluate</code>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">train_input</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">input_fn</span><span class="p">(</span>
    <span class="n">TRAIN_FILES</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">40</span>
<span class="p">)</span>

<span class="c"># Don't shuffle evaluation data</span>
<span class="n">eval_input</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">input_fn</span><span class="p">(</span>
    <span class="n">EVAL_FILES</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</code></pre>
</div>

<h2 id="step-3-define-training-and-eval-specs">Step 3: Define training and eval specs</h2>

<p>Now we’re nearly set.  We just need to define the the <code class="highlighter-rouge">TrainSpec</code> and <code class="highlighter-rouge">EvalSpec</code> used by <code class="highlighter-rouge">tf.estimator.train_and_evaluate</code>. These specify not only the input functions, but how to export our trained model; that is, how to save it in the standard <a href="https://www.tensorflow.org/programmers_guide/saved_model">SavedModel</a> format, so that we can later use it for serving.</p>

<p>First, we’ll define the <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec"><code class="highlighter-rouge">TrainSpec</code></a>, which takes as an arg <code class="highlighter-rouge">train_input</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">train_spec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">TrainSpec</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span>
                                  <span class="n">max_steps</span><span class="o">=</span><span class="mi">1000</span>
                                  <span class="p">)</span>
</code></pre>
</div>

<p>For our <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/EvalSpec"><code class="highlighter-rouge">EvalSpec</code></a>, we’ll instantiate it with something additional – a list of <em>exporters</em>, that specify how to export (save) the trained model so that it can be used for serving with respect to a particular data input format. Here we’ll just define one such exporter.</p>

<p>To specify our exporter, we must first define a
<a href="https://www.tensorflow.org/programmers_guide/saved_model#preparing_serving_inputs"><em>serving input function</em></a>.
This is what determines the input format that the exporter will accept.
As we saw above, during training, an <code class="highlighter-rouge">input_fn()</code> ingests data and prepares it for use by the model.
At serving time, similarly, a <code class="highlighter-rouge">serving_input_receiver_fn()</code> accepts inference requests and prepares them for the model. This function has the following purposes:</p>

<ul>
  <li>To add placeholders to the model graph that the serving system will feed with inference requests.</li>
  <li>To add any additional ops needed to convert data from the input format into the feature Tensors expected by the model.</li>
</ul>

<p>The serving input function should return a <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver"><code class="highlighter-rouge">tf.estimator.export.ServingInputReceiver</code></a> object, which packages the placeholders and the resulting feature <code class="highlighter-rouge">Tensors</code> together.</p>

<p>A <code class="highlighter-rouge">ServingInputReceiver</code> is instantiated with two arguments — <code class="highlighter-rouge">features</code> and <code class="highlighter-rouge">receiver_tensors</code>. The <code class="highlighter-rouge">features</code> represent the inputs to our Estimator when it is being served for prediction. The <code class="highlighter-rouge">receiver_tensors</code> represent inputs to the server.</p>

<p>These two arguments will not necessarily always be the same — in some cases we may want to perform some transformation(s) before feeding the data to the model. <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/estimator/trainer/model.py#L197">Here’s</a> one example of that, where the inputs to the server (csv-formatted rows) include a field to be removed.</p>

<p>However, in our case, the inputs to the server are the same as the features input to the model. Here’s what our serving input function looks like:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">json_serving_input_fn</span><span class="p">():</span>
  <span class="s">"""Build the serving inputs."""</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">INPUT_COLUMNS</span><span class="p">:</span>
    <span class="n">inputs</span><span class="p">[</span><span class="n">feat</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">feat</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">ServingInputReceiver</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
</code></pre>
</div>

<p>Then, we define an <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Exporter">Exporter</a> in terms of that serving input function. It will export the model in <a href="https://www.tensorflow.org/programmers_guide/saved_model">SavedModel</a> format. We pass the <code class="highlighter-rouge">EvalSpec</code> constructor a list of exporters (here, just one).</p>

<p>Here, we’re using
the <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/FinalExporter"><code class="highlighter-rouge">FinalExporter</code></a> class.  This class performs a single export at the end of training. This is in contrast to
<a href="https://www.tensorflow.org/api_docs/python/tf/estimator/LatestExporter"><code class="highlighter-rouge">LatestExporter</code></a>, which does regular exports and retains the last <code class="highlighter-rouge">N</code>. (We’re just using one exporter here, but if you define multiple exporters, training will result in multiple saved models).</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">exporter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">FinalExporter</span><span class="p">(</span><span class="s">'census'</span><span class="p">,</span>
      <span class="n">json_serving_input_fn</span><span class="p">)</span>
<span class="n">eval_spec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">EvalSpec</span><span class="p">(</span><span class="n">eval_input</span><span class="p">,</span>
                                <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                <span class="n">exporters</span><span class="o">=</span><span class="p">[</span><span class="n">exporter</span><span class="p">],</span>
                                <span class="n">name</span><span class="o">=</span><span class="s">'census-eval'</span>
                                <span class="p">)</span>
</code></pre>
</div>

<h2 id="step-4-train-your-model-using-trainandevaluate">Step 4: Train your model using <code class="highlighter-rouge">train_and_evaluate</code></h2>

<p>Now we have defined everything we need to train and evaluate our model, to and export the trained model for serving, via a call to <strong><code class="highlighter-rouge">train_and_evaluate</code></strong>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">train_spec</span><span class="p">,</span> <span class="n">eval_spec</span><span class="p">)</span>
</code></pre>
</div>

<p>This call will train the model and export the result in a format that is easy to use for prediction!</p>

<p>With <code class="highlighter-rouge">train_and_evaluate</code>, the training behavior will be consistent whether you run this function in a local/non-distributed context or in a distributed configuration.</p>

<p>The exported trained model can be served on many platforms. You may particularly want to consider ways to scalably serve your model, in order to handle many prediction requests at once— say if you’re using your model in an app you’re building, and you expect it to become popular. <a href="https://cloud.google.com/ml-engine/docs/prediction-overview">Cloud ML Engine online prediction</a> and <a href="https://www.tensorflow.org/serving/">TensorFlow serving</a>) are two options for doing this.</p>

<p>In this post, we’ll look at using <strong>Cloud ML Engine Online Prediction</strong>. But first, let’s take a closer look at our exported model.</p>

<h3 id="examine-the-signature-of-the-exported-model">Examine the signature of the exported model.</h3>

<p>TensorFlow ships with a CLI that allows you to inspect the <em>signature</em> of exported binary files. This can be useful as a sanity check.
It’s run as follows, by passing it the path to directory containing the <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md">saved model</a>, which will be called <code class="highlighter-rouge">saved_model.pb</code>.
For our model, it will be found under <code class="highlighter-rouge">$output_dir/export/census</code>.  This is because we passed the <code class="highlighter-rouge">census</code> name to our <code class="highlighter-rouge">FinalExporter</code> above.  (<code class="highlighter-rouge">$output_dir</code> was specified when we constructed our Estimator).</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code>saved_model_cli show --dir <span class="nv">$output_dir</span>/export/census/&lt;timestamp&gt; --tag serve --signature_def predict
</code></pre>
</div>

<p>The <code class="highlighter-rouge">saved_model_cli</code> command shows us this info (abbreviated for conciseness):</p>

<div class="highlighter-rouge"><pre class="highlight"><code>The given SavedModel SignatureDef contains the following input(s):
inputs['age'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder_8:0
inputs['capital_gain'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder_10:0
inputs['capital_loss'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder_11:0
inputs['education'] tensor_info:
    dtype: DT_STRING
    shape: (-1)
    name: Placeholder_2:0
&lt;... more input fields here ...&gt;
The given SavedModel SignatureDef contains the following output(s):
outputs['class_ids'] tensor_info:
    dtype: DT_INT64
    shape: (-1, 1)
    name: head/predictions/classes:0
outputs['classes'] tensor_info:
    dtype: DT_STRING
    shape: (-1, 1)
    name: head/predictions/str_classes:0
outputs['logistic'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1, 1)
    name: head/predictions/logistic:0
outputs['logits'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1, 1)
    name: head/predictions/logits:0
outputs['probabilities'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1, 2)
    name: head/predictions/probabilities:0
Method name is: tensorflow/serving/predict
</code></pre>
</div>
<p>Based on our knowledge of <code class="highlighter-rouge">DNNLinearCombinedClassifier</code>, and the input fields we defined, this looks as we expect. (Notice that the model generates multiple outputs).</p>

<h3 id="check-local-prediction-with-gcloud">Check local prediction with gcloud</h3>

<p>Another useful sanity check is running local prediction with your trained model. We’ll use the <a href="https://cloud.google.com/sdk/downloads">Google Cloud SDK (gcloud)</a> command-line tool for that.</p>

<p>We’ll use the example input in <a href="https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/test.json"><code class="highlighter-rouge">test.json</code></a> to predict a person’s income bracket based on the features encoded in the <code class="highlighter-rouge">test.json</code> instance. Again, we point to the directory containing the saved model.</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code>gcloud ml-engine <span class="nb">local </span>predict --model-dir <span class="nv">$output_dir</span>/export/census/&lt;timestamp&gt; --json-instances test.json
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>CLASS_IDS  CLASSES  LOGISTIC               LOGITS                 PROBABILITIES
[0]        [u'0']   [0.06585630029439926]  [-2.6521551609039307]  [0.9341437220573425, 0.06585630774497986]
</code></pre>
</div>

<p>You can see how the input fields in <code class="highlighter-rouge">test.json</code> correspond to the inputs listed by the <code class="highlighter-rouge">saved_model_cli</code> command above, and how the prediction outputs correspond to the outputs listed by <code class="highlighter-rouge">saved_model_cli</code>.
In this model, Class 0 indicates income &lt;= 50k and Class 1 indicates income &gt;50k.</p>

<h2 id="using-cloud-ml-engine-for-easy-distributed-training-and-scalable-online-prediction">Using Cloud ML Engine for easy distributed training and scalable online prediction</h2>

<p>In the previous section, we looked at how to use <code class="highlighter-rouge">tf.estimator.train_and_evaluate</code> first to train and export a model, and then to make predictions using the trained model.</p>

<p>In this section, we’ll see how easy it is to use the same code — without any changes — to do <strong>distributed training on <a href="https://cloud.google.com/ml-engine/">Cloud ML Engine</a></strong>, thanks to the <strong><code class="highlighter-rouge">Estimator</code></strong> class and <strong><code class="highlighter-rouge">train_and_evaluate</code></strong>.  Then we’ll use <a href="https://cloud.google.com/ml-engine/docs/online-predict"><strong>Cloud ML Engine Online Prediction</strong></a> to scalably serve the trained model.</p>

<p>One advantage of Cloud ML Engine is that there’s no lock-in. You could potentially train your TensorFlow model elsewhere, then deploy to Cloud ML Engine for serving (prediction); or alternately use Cloud ML Engine for distributed training and then serve elsewhere (e.g. with <a href="https://github.com/tensorflow/serving">TensorFlow serving</a>).  Here, we’ll show how to use Cloud ML Engine for both stages.</p>

<p>To launch a training job on Cloud ML Engine, we can again use <code class="highlighter-rouge">gcloud</code>.  We’ll need to package our code so that it can be deployed, and specify the Python file to run to start the training (<code class="highlighter-rouge">--module-name</code>).</p>

<p>The <code class="highlighter-rouge">trainer</code> module code is <a href="https://github.com/amygdala/code-snippets/tree/master/ml/census_train_and_eval/trainer">here</a>.
<code class="highlighter-rouge">trainer.task</code> is the entry point, and when that file is run, it calls <code class="highlighter-rouge">tf.estimator.train_and_evaluate</code>.
(You can read more about how to package your code <a href="https://cloud.google.com/ml-engine/docs/packaging-trainer">here</a>).</p>

<p>If we want to, we could test (distributed) training via <code class="highlighter-rouge">gcloud</code> locally first, to make sure that we have everything packaged up correctly. See the accompanying <a href="https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb">notebook</a> for details.</p>

<p>But here, we’ll jump right in to using Cloud ML Engine to do cloud-based distributed training.</p>

<p>We’ll set the training job to use the <code class="highlighter-rouge">SCALE_TIER_STANDARD_1</code> scale spec.  This <a href="https://cloud.google.com/ml-engine/docs/training-overview#job_configuration_parameters">gives us</a> one ‘master’ instance, plus four <em>workers</em> and three <em>parameter servers</em>.</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code>gcloud ml-engine <span class="nb">jobs </span>submit training <span class="nv">$JOB_NAME</span> --scale-tier <span class="sb">`</span>SCALE_TIER_STANDARD_1<span class="sb">`</span> <span class="se">\</span>
    --runtime-version 1.4 --job-dir <span class="nv">$GCS_JOB_DIR</span> <span class="se">\</span>
    --module-name trainer.task --package-path trainer/ <span class="se">\</span>
    --region us-central1 <span class="se">\</span>
    -- --train-steps 5000 --train-files <span class="nv">$GCS_TRAIN_FILE</span> --eval-files <span class="nv">$GCS_EVAL_FILE</span> --eval-steps 100
</code></pre>
</div>

<p>The cool thing about this is that <strong>we don’t need to change our code at all to use this distributed config</strong>.  Our use of the Estimator class in conjunction with the Cloud ML Engine scale specification makes the distributed training config transparent to us — it just works.
Further, we could swap in any of the other predefined scale tiers (say <code class="highlighter-rouge">BASIC_GPU</code>), or define our own custom cluster, again without any code changes.
For example, we could alternatively <a href="https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Extras:-Train-on-CMLE-using-a-custom-GPU-cluster">configure our job</a> to <a href="https://cloud.google.com/ml-engine/docs/using-gpus">use a GPU cluster</a>.</p>

<p>Once our training job is running, we can stream its logs to the terminal, and/or monitor it in the <a href="https://console.cloud.google.com/mlengine/jobs">Cloud Console</a>.</p>

<p><a href="https://amy-jo.storage.googleapis.com/images/census_train_eval/ml_jobs.png" target="_blank"><img src="https://amy-jo.storage.googleapis.com/images/census_train_eval/ml_jobs.png" width="500" /></a></p>

<p>In the logs, you’ll see output from the multiple worker replicas and parameter servers that we utilized by specifying a <code class="highlighter-rouge">SCALE_TIER_STANDARD_1 </code> cluster.  In the logs viewers, you can filter on the output of a particular node (e.g. a given worker) if you like.</p>

<p>Once your job is finished, you’ll find the exported model under the specified GCS directory, in addition to other data such as model checkpoints.
That exported model has exactly the same signature as the locally-generated model we looked at above, and can be used in just the same ways.</p>

<h3 id="scalably-serve-your-trained-model-with-cloud-ml-engine-online-prediction">Scalably serve your trained model with Cloud ML Engine online prediction</h3>

<p>You can deploy an exported model to Cloud ML Engine and scalably serve it for <strong>prediction</strong>, using the <a href="https://cloud.google.com/ml-engine/docs/prediction-overview">Cloud ML Engine prediction service</a> to generate a prediction on new data with an easy-to-use REST API. Here we’ll look at Cloud ML Engine online prediction, which <a href="https://cloud.google.com/blog/big-data/2017/12/bringing-cloud-ml-engine-to-more-developers-with-online-prediction-features-and-reduced-prices">recently moved to general availability (GA) status</a>; but <a href="https://cloud.google.com/ml-engine/docs/batch-predict">batch prediction</a> is supported as well.</p>

<p>The online prediction service scales the number of nodes it uses to maximize the number of requests it can handle without introducing too much latency. To do that, the service:</p>

<ul>
  <li>Allocates some nodes the first time you request predictions after a long pause in requests.</li>
  <li>Scales the number of nodes in response to request traffic, adding nodes when traffic increases, and removing them when there are fewer requests.</li>
  <li>Keeps at least one node ready to handle requests even when there are none to handle. It then scales down to zero by default when your model version goes several minutes without a prediction request (but if you like, you can specify a minimum number of nodes to keep ready for a given model).</li>
</ul>

<p>See the accompanying <a href="https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Scalably-serve-your-trained-model-with-CMLE-Online-Prediction">notebook</a> for details on how to deploy your model so that you can use it to make predictions.</p>

<p>Once your model is serving with Cloud ML Engine Online Prediction, you can access it via a REST API.  It’s <a href="https://cloud.google.com/ml-engine/docs/online-predict#requesting_predictions">easy</a> to do this programmatically via the Google <a href="https://cloud.google.com/apis/docs/cloud-client-libraries">Cloud Client libraries</a> or via <code class="highlighter-rouge">gcloud</code>.  <br />
<code class="highlighter-rouge">gcloud</code> is great for testing your deployed model, and the command looks almost the same as it did for the local version of the model:</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code>gcloud ml-engine predict --model census --version v1 --json-instances test.json
</code></pre>
</div>

<p>The Cloud Console makes it easy to inspect the different versions of a model, as well as set the default version: <a href="https://console.cloud.google.com/mlengine/models">console.cloud.google.com/mlengine/models</a>.
You can list your model information using <code class="highlighter-rouge">gcloud</code> too.</p>

<p><a href="https://amy-jo.storage.googleapis.com/images/census_train_eval/ml_model_details.png" target="_blank"><img src="https://amy-jo.storage.googleapis.com/images/census_train_eval/ml_model_details.png" width="500" /></a></p>

<h2 id="summary--and-whats-next">Summary — and what’s next?</h2>

<p>In this post, we’ve walked through how to configure and use the TensorFlow <code class="highlighter-rouge">Estimator</code> class, and
<code class="highlighter-rouge">tf.estimator.train_and_evaluate</code>.  They enable distributed execution for training and evaluation, while also supporting local execution, and provides consistent behavior across both local/non-distributed and distributed configurations.</p>

<p>For more, see the accompanying <a href="https://github.com/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb">notebook</a>.  The notebook includes examples of <a href="https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Extras:-Train-on-CMLE-using-a-custom-GPU-cluster">how to run your training job on a Cloud ML Engine GPU cluster</a>, and <a href="https://nbviewer.jupyter.org/github/amygdala/code-snippets/blob/master/ml/census_train_and_eval/using_tf.estimator.train_and_evaluate.ipynb#Extras:-Use-Hyperparameter-Tuning">how to use Cloud ML Engine to do</a> <a href="https://cloud.google.com/ml-engine/docs/hyperparameter-tuning-overview">hyperparameter tuning</a>.</p>


  </article>

</div>

Tags:
  
    <a href="/tag/cmle">cmle</a>&nbsp
  
    <a href="/tag/cloud_ml">cloud_ml</a>&nbsp
  
    <a href="/tag/tensorflow">tensorflow</a>&nbsp
  
    <a href="/tag/machine_learning">machine_learning</a>&nbsp
  
</ul>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">Amy on GCP</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <!-- <li>Amy on GCP</li> -->
          <li><a href="mailto:"></a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/amygdala">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">amygdala</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/amygdala">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">amygdala</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text"></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
